<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 17]
- [math.AP](#math.AP) [Total: 36]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 8]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [nlin.SI](#nlin.SI) [Total: 1]
- [math.MG](#math.MG) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [gr-qc](#gr-qc) [Total: 4]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math.CV](#math.CV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [math-ph](#math-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Sparse Dictionary-Based Solution of Dynamic Inverse Problems](https://arxiv.org/abs/2602.18593)
*Aidan Mason-Mackay,Daniela Calvetti,Erkki Somersalo,Antti Aarnio,Mikko Kettunen,Ekaterina Paasonen Olli Gr√∂hn,Ville Kolehmainen*

Main category: math.NA

TL;DR: A Bayesian dictionary learning approach with hierarchical sparsity prior for dynamic inverse problems, using MAP estimation via IAS algorithm, showing competitive performance with ADMM but better hyperparameter robustness.


<details>
  <summary>Details</summary>
Motivation: Dynamic inverse problems often have limited data and high dimensionality. Spatial features and temporal correlations between frames can improve solution quality when properly leveraged through sparsity-promoting dictionary representations.

Method: Stochastic dictionary coding with random hierarchical sparsity prior, computing MAP estimate using Iterative Alternating Sequential (IAS) algorithm. Applied to dynamic CT and MRI datasets, compared against ADMM.

Result: Proposed methodology is competitive with ADMM for compressed sensing, with significantly lower sensitivity to hyper-parameter selection in real-world dynamic CT and MRI applications.

Conclusion: The Bayesian hierarchical sparsity approach with IAS optimization provides robust performance for dynamic inverse problems, offering competitive reconstruction quality with reduced hyperparameter tuning requirements compared to ADMM.

Abstract: In ill-posed dynamic inverse problems expected spatial features and temporal correlation between frames can be leveraged to improve the quality of the computed solution, in particular when the available data are limited and the dimensionality of the unknown is large. One way to take advantage of the spatial and temporal traits believed to characterize the solution is to encode them into the entries of a dictionary, and to seek the solution as a sparse linear combination of the dictionary atoms. To promote a vector of coefficients with mostly vanishing entries, we consider a stochastic extension of the dictionary coding problem model with a random hierarchical sparsity promoting prior. We compute the Maximum A Posteriori (MAP) estimate of the coefficient vector using the Iterative Alternating Sequential Algorithm (IAS), which has been demonstrated to efficiently solve inverse problems with minimal need for parameter tuning. The proposed methodology is tested on real-world dynamic Computed Tomography and MRI datasets, where it is compared to the popular Alternating Direction Method of Minimizers (ADMM). The computed examples show the that proposed methodology is competitive with the ADMM for compressed sensing, with a significantly lower sensitivity to hyper-parameter selection.

</details>


### [2] [Discretization in Multilayered Media with High Contrasts: Is It All About the Boundaries?](https://arxiv.org/abs/2602.18629)
*Camille Carvalho,St√©phanie Chaillat,Elsie Cortes,Chrysoula Tsogka*

Main category: math.NA

TL;DR: The paper proposes an adaptive boundary discretization approach for wave propagation in multilayered media with high material contrasts, showing that background wavenumber rather than maximum wavenumber determines optimal resolution.


<details>
  <summary>Details</summary>
Motivation: Wave propagation in multilayered media with high material contrasts poses numerical challenges due to large wavenumber variations causing strong reflections and complex transmission. Standard meshing strategies based on maximum wavenumber become inefficient when high wavenumbers are confined to localized subdomains.

Method: The authors use a boundary integral formulation to avoid volumetric discretization. Through systematic study of multilayer transmission problems, they analyze discretization requirements and propose an adaptive approach that achieves uniform accuracy and efficient computation across multiple layers.

Result: The study shows that no simple discretization rule based on maximum wavenumber or material contrasts emerges. Instead, the wavenumber of the background (exterior) medium plays a dominant role in determining optimal boundary resolution. Numerical experiments demonstrate the scalability and robustness of the proposed adaptive approach.

Conclusion: An adaptive boundary discretization approach based on background wavenumber rather than maximum wavenumber provides efficient and accurate computation for wave propagation in multilayered media with high material contrasts, overcoming limitations of standard meshing strategies.

Abstract: Wave propagation in multilayered media with high material contrasts poses significant numerical challenges, as large variations in wavenumbers lead to strong reflections and complex transmission of the incoming wave field. To address these difficulties, we employ a boundary integral formulation thereby avoiding volumetric discretization. In this framework, the accuracy of the numerical solution depends strongly on how the material interfaces are discretized. In this work, we demonstrate that standard meshing strategies based on resolving the maximum wavenumber across the domain become computationally inefficient in multilayered configurations, where high wavenumbers are confined to localized subdomains. Through a systematic study of multilayer transmission problems, we show that no simple discretization rule based on the maximum wavenumber or material contrasts emerges. Instead, the wavenumber of the background (exterior) medium plays a dominant role in determining the optimal boundary resolution. Building on these insights, we propose an adaptive approach that achieves uniform accuracy and efficient computation across multiple layers. Numerical experiments for a range of multilayer configurations demonstrate the scalability and robustness of the proposed approach.

</details>


### [3] [From an Elementary Proof of Error Representation for Hermite Quadrature to a Rediscovery of Legendre Polynomials and Rodrigues Formula](https://arxiv.org/abs/2602.18634)
*Tan Bui-Thanh,Giancarlo Villatoro,C. G. Krishnanunni*

Main category: math.NA

TL;DR: The paper generalizes Hermite quadrature to use (n-1) derivatives at endpoints, provides an elementary error derivation using reverse integration by parts, requires only nth-order derivative regularity (vs 2nth), connects Legendre polynomials to Hermite quadrature error kernels, and offers practical composite rules.


<details>
  <summary>Details</summary>
Motivation: To develop a more accessible and general error analysis for Hermite quadrature rules that requires less regularity than classical approaches and reveals fundamental connections between Hermite interpolation and Legendre polynomials.

Method: Generalizes two-point interpolatory Hermite quadrature to use function values and first (n-1) derivatives at endpoints. Uses reverse integration by parts for elementary error derivation. Provides alternative proof via Peano kernel theorem. Develops composite interpolatory Hermite quadrature for practical applications.

Result: Derives exact error representation requiring only nth-order derivative existence (vs classical 2nth). Discovers Legendre polynomials as error kernels for Hermite quadrature. Rediscovers Rodrigues formula for Legendre polynomials. Establishes elegant relation between Legendre polynomials and Hermite interpolation.

Conclusion: The approach offers significant advantages: simpler derivation using only integration by parts, applicability to less regular functions, reveals fundamental connections between Hermite quadrature and Legendre polynomials, and provides practical composite rules for implementation.

Abstract: We generalize two-point interpolatory Hermite quadrature to functions with available values and the first (n-1) derivatives at both end points. Armed with integration by parts in the reverse form we provide an elementary derivation of an exact error represenation of Hermite quadrature rule. This approach possesses several advantages over the classical approaches: i) Only integration by parts is needed for the derivation; ii) the error representation requires much milder regularity, namely the existence of nth-order derivative rather than a (2n)th-order derivative of the function under consideration. As a result, our error formula is valid for less regular functions for which the classical ones are not valid; iii) our approach rediscovers Legendre polynomials and more interestingly it provides a surprisingly elegant relation between Legendre polynomial and Hermite interpolation. In particular, Legendre polynomials are precisely the error kernels for interpolatory Hermite quadrature rules; and iv) We also rediscover the Rodrigues formula for Legendre polynomials as part of our findings. For those who are interested in a different proof of the exact error representation for Hermite quadrature rule, we provide an alternative proof using the Peano kernel theorem. We also provide a composite interpolatory Hermite quadrature rule for practical applications.

</details>


### [4] [Constructive discretization and approximation in reproducing kernel Hilbert spaces](https://arxiv.org/abs/2602.18719)
*Abdellah Chkifa,Matthieu Dolbeault,David Krieg,Mario Ullrich*

Main category: math.NA

TL;DR: Generalizes BSS sparsification algorithm with dimension-independent results, recovers discretization inequalities in L2 and sup norms, proves infinite-dimensional variant, improves constants and oversampling factors for least-squares approximation.


<details>
  <summary>Details</summary>
Motivation: To provide a more constructive version of recent approximation bounds that relied on stronger, less constructive results like MSS, and to make sparsification results dimension-independent with improved constants.

Method: Generalizes the Batson-Spielman-Srivastava (BSS) sparsification algorithm, making part of the result dimension-independent. Proves discretization inequalities in L2 and sup norms on finite-dimensional subspaces and extends to infinite-dimensional variant.

Result: Recovers discretization inequalities in L2- and sup-norms, proves suitable infinite-dimensional variant, discusses implications for least-squares approximation error, and improves constants and oversampling factors compared to previous results.

Conclusion: Provides a more constructive approach to approximation bounds than previous MSS-based results, with dimension-independent aspects and improved practical parameters for sampling-based least-squares approximation.

Abstract: We generalize the sparsification algorithm of Batson, Spielman and Srivastava, making one part of the result dimension-independent. In particular, we recover discretization inequalities in $L_2$- and sup-norms on general finite-dimensional subspaces, prove a suitable infinite-dimensional variant, and discuss the implications for the error of least-squares approximation based on samples. This gives a more constructive version of several recently established approximation bounds, some of which relied on the stronger and less constructive result of Marcus, Spielman and Srivastava. We also improve the constants and oversampling factors in these results.

</details>


### [5] [Finite element methods for isometric embedding of Riemannian manifolds](https://arxiv.org/abs/2602.18722)
*Guangwei Gao,Kaibo Hu,Buyang Li,Ganghui Zhang*

Main category: math.NA

TL;DR: First rigorous numerical framework for computing isometric embeddings of 2D Riemannian manifolds with positive curvature into R¬≥, with error estimates and extension to Ricci flow visualization.


<details>
  <summary>Details</summary>
Motivation: The isometric embedding problem connects intrinsic and extrinsic geometry but lacks rigorous numerical analysis due to its nonlinear and degenerate nature. This paper aims to bridge this gap by developing the first numerical framework for computing isometric embeddings.

Method: Develops a new weak formulation for Weyl's problem (isometric embedding of 2D manifolds with positive Gaussian curvature into R¬≥), leading to a numerical scheme suitable for high-order finite element discretization. The method naturally extends to isometric embeddings of Ricci flows.

Result: Proves well-posedness of the weak formulation, existence and uniqueness of numerical solutions, and convergence with error estimates. Numerical experiments demonstrate method convergence and effectiveness for computing isometric embeddings and visualizing Ricci flows.

Conclusion: Establishes a foundational numerical framework for computing isometric embeddings, providing the first rigorous numerical analysis in this area. The framework enables visualization of geometric evolutions in curvature flows and lays groundwork for broader applications.

Abstract: The isometric embedding problem for Riemannian manifolds, which connects intrinsic and extrinsic geometry, is a central question in differential geometry with deep theoretical significance and wide-ranging applications. Despite extensive analytical progress, the nonlinear and degenerate nature of this problem has hindered the development of rigorous numerical analysis in this area. As the first step toward addressing this gap, we study the numerical approximation of Weyl's problem, i.e., the isometric embedding of two-dimensional Riemannian manifolds with positive Gaussian curvature into $\mathbb{R}^3$, by establishing a new weak formulation that naturally leads to a numerical scheme well suited for high-order finite element discretization, and conducting a systematic analysis to prove the well-posedness of this weak formulation, the existence and uniqueness of its numerical solution, as well as its convergence with error estimates. This provides a foundational framework for computing isometric embeddings of Riemannian manifolds into Euclidean space, with the goal of extending it to a broader range of cases and applications in the future. Our framework also extends naturally to the isometric embedding of the Ricci flow, with rigorous error estimates, enabling the visualization of geometric evolutions in intrinsic curvature flows. Numerical experiments support the theoretical analysis by demonstrating the convergence of the method and its effectiveness in simulating isometric embeddings of given Riemannian manifolds as well as Ricci flows.

</details>


### [6] [Structure-preserving Krylov Subspace Approximations for the Matrix Exponential of Hamiltonian Matrices: A Comparative Study](https://arxiv.org/abs/2602.18937)
*Peter Benner,Heike Fa√übender,Michel-Niklas Senn*

Main category: math.NA

TL;DR: Structure-preserving Krylov methods for approximating f(H)b where H is Hamiltonian, f is exponential or phi-function, using J-orthogonal bases to maintain Hamiltonian structure in projections.


<details>
  <summary>Details</summary>
Motivation: Standard Krylov methods destroy Hamiltonian structure when approximating matrix-vector products f(H)b for Hamiltonian matrices H, which are central to exponential integrators for Hamiltonian systems. This motivates the development of structure-preserving methods.

Method: Uses Krylov bases with J-orthogonal columns to yield Hamiltonian projected matrices and symplectic reduced exponentials. Compares several such structure-preserving Krylov methods and discusses adaptive strategies for selecting Krylov subspace dimension.

Result: The paper compares several structure-preserving Krylov methods on representative Hamiltonian test problems, focusing on accuracy, efficiency, and structure preservation.

Conclusion: Structure-preserving Krylov methods using J-orthogonal bases effectively maintain Hamiltonian structure while approximating matrix-vector products f(H)b, offering advantages over standard Krylov methods for Hamiltonian systems.

Abstract: We study structure-preserving Krylov subspace methods for approximating the matrix-vector products f(H)b, where H is a large Hamiltonian matrix and f denotes either the matrix exponential or the related phi-function. Such computations are central to exponential integrators for Hamiltonian systems. Standard Krylov methods generally destroy the Hamiltonian structure under projection, motivating the use of Krylov bases with J-orthogonal columns that yield Hamiltonian projected matrices and symplectic reduced exponentials. We compare several such structure-preserving Krylov methods on representative Hamiltonian test problems, focusing on accuracy, efficiency, and structure preservation, and briefly discuss adaptive strategies for selecting the Krylov subspace dimension.

</details>


### [7] [Computing the SVD efficiently with photonic chips](https://arxiv.org/abs/2602.18950)
*Johannes Maly,Korbinian Neuner,Samarth Vadia*

Main category: math.NA

TL;DR: Photonic chips offer energy-efficient alternative to digital computers for SVD computation, with hybrid photonic-digital systems matching CPU/GPU performance while consuming less energy.


<details>
  <summary>Details</summary>
Motivation: Digital computers face fundamental performance limits due to physical constraints and energy consumption, creating need for alternative computing approaches. Analog systems like photonic chips offer promising alternatives for specific applications.

Method: Investigate linear photonic chips for accelerating singular value decomposition (SVD) computation using hybrid systems combining digital controller with photonic chip.

Result: Hybrid photonic-digital systems asymptotically perform on par with large-scale CPU/GPU systems in runtime while clearly outperforming digital systems in energy consumption.

Conclusion: Photonic computing offers viable alternative to digital processors for SVD computation, providing comparable performance with superior energy efficiency, making it promising for data processing applications.

Abstract: In light of today's massive data processing, digital computers are reaching fundamental performance limits due to physical limitations and energy consumption. For specific applications, tailored analog systems offer promising alternatives to digital processors. In this work, we investigate the potential of linear photonic chips for accelerating the computation of the singular value decomposition (SVD) of a matrix. The SVD is a key primitive in linear algebra and forms a crucial component of various modern data processing algorithms. Our main insights are twofold: first, hybrid systems of digital controller and photonic chip asymptotically perform on par with large-scale CPU/GPU systems in terms of runtime. Second, such hybrid systems clearly outperform digital systems in terms of energy consumption.

</details>


### [8] [On the convergence of explicit formulas for $L^2$ solutions to the Benjamin-Ono and continuum Calogero-Moser equations](https://arxiv.org/abs/2602.19046)
*Yvonne Alama Bronsard,Thierry Laurens*

Main category: math.NA

TL;DR: The paper develops fully-discrete numerical schemes for Benjamin-Ono and continuum Calogero-Moser equations using discretized explicit formulas, achieving exact time integration and structure preservation with rigorous convergence proofs for rough solutions.


<details>
  <summary>Details</summary>
Motivation: To create efficient and accurate numerical methods for nonlinear integrable PDEs (Benjamin-Ono and continuum Calogero-Moser equations) that preserve important structural properties and work for rough solutions at scaling-critical regularity.

Method: Discretize explicit formulas from nonlinear integrability theory to obtain schemes exact in time (only spatial discretization needed). Generalize previous fully-discrete schemes to include structure-preserving properties like mass and momentum conservation. Use Lax operator analysis to prove convergence.

Result: Proved convergence for rough solutions (L¬≤ for BO, L¬≤‚Çä for CCM - scaling-critical regularity). Error goes to zero as truncation parameters increase, uniformly on bounded time intervals. Demonstrated first numerical evidence of Talbot effect for BO with rigorous convergence support.

Conclusion: The discretized explicit formula approach yields efficient, structure-preserving numerical schemes with rigorous convergence proofs even for rough solutions, enabling reliable numerical study of phenomena like the Talbot effect in nonlinear integrable systems.

Abstract: By developing discrete counterparts to recent advances in nonlinear integrability, and in particular to the discovery of explicit formulas, we design and analyze fully-discrete approximations to the Benjamin-Ono (BO) and continuum Calogero-Moser (CCM) equations on the torus. We build on the key observation that discretizing such explicit formulas yields schemes that are exact in time (requiring only spatial discretization) and have a computational cost independent of the final time $T$. In this work, we first generalize the fully-discrete schemes of arXiv:2412.13480 to include numerical approximations with better structure preservation properties, including the conservation of mass and momentum in the case of the (BO) equation. Secondly, building on recent analyses of the corresponding Lax operators, we extend the convergence results to this class of schemes for rough solutions $u(t)$ merely belonging to $L^2(\mathbb{T})$ for (BO) and $L^2_{+}(\mathbb{T})$ for (CCM), the latter of which is precisely the scaling-critical regularity. Our main theorem states that the $L^2(\mathbb{T})$-norm of the error goes to zero as the truncation parameters go to infinity, uniformly on any bounded time interval $[-T,T]$. As an example, we apply our scheme to the (BO) equation with a square-wave initial profile, and obtain the first numerical evidence of the Talbot effect for (BO) supported by a rigorous convergence result.

</details>


### [9] [Forward Error-Oriented Iterative Refinement for Eigenvectors of a Real Symmetric Matrix](https://arxiv.org/abs/2602.19090)
*Takeshi Terao,Katsuhisa Ozaki*

Main category: math.NA

TL;DR: Proposed efficient eigenvalue decomposition algorithm that targets prescribed forward error instead of minimizing backward error, reducing computational cost for high-precision requirements.


<details>
  <summary>Details</summary>
Motivation: Existing methods for symmetric matrix eigenvalue decomposition focus on minimizing backward errors, but forward error magnitude remains unknown. This leads to excessive computational costs when high-precision solutions are needed, as backward errors must be reduced excessively.

Method: Two-part approach: 1) Efficient approximation algorithm that aims to achieve prescribed forward error (not focused on minimizing backward error), 2) High-accuracy numerical algorithm based on Ozaki scheme (emulation technique for matrix multiplication) adapted to eigenvalue decomposition problem.

Result: The proposed method significantly reduces computational cost compared to traditional backward-error-focused approaches, as demonstrated through numerical experiments evaluating efficiency.

Conclusion: By shifting focus from backward error minimization to achieving prescribed forward error, the proposed algorithm provides more efficient eigenvalue decomposition for high-precision requirements while maintaining accuracy.

Abstract: In this paper, we discuss numerical methods for the eigenvalue decomposition of real symmetric matrices. While many existing methods can compute approximate eigenpairs with sufficiently small backward errors, the magnitude of the resulting forward errors is often unknown. Consequently, when high-precision numerical solutions are required, the computational cost tends to increase significantly because backward errors must be reduced to an excessive degree. To address this issue, we propose an efficient approximation algorithm that aims to achieve a prescribed forward error, together with a high-accuracy numerical algorithm based on the Ozaki scheme -- an emulation technique for matrix multiplication -- adapted to this problem. Since the proposed method is not primarily focused on reducing backward errors, the computational cost can be significantly reduced. Finally, we present numerical experiments to evaluate the efficiency of the proposed method.

</details>


### [10] [A median-filter-based framework for interface optimal design problems](https://arxiv.org/abs/2602.19155)
*Sihao Cheng,Ziming Shao,Dong Wang*

Main category: math.NA

TL;DR: A continuous median filter framework for interface optimization problems that overcomes pinning effects, ensures energy stability, and converges to binary solutions without explicit penalization.


<details>
  <summary>Details</summary>
Motivation: Traditional binary iterative convolution-thresholding methods suffer from pinning effects caused by spatial discretization, limiting accurate interface evolution with small time steps. There's a need for a robust continuous framework that can handle various interface-related optimization problems while maintaining theoretical guarantees.

Method: Extends binary median filter scheme to continuous level-set scheme via weighted quantile interpretation. Uses convex relaxation that inherently enforces binary solutions without explicit penalization. Provides rigorous theoretical analysis proving unconditional energy stability of the iterative scheme.

Result: The continuous median filter scheme effectively eliminates pinning effects, guarantees unconditional energy stability, and accurately converges to binary solutions. Demonstrated effectiveness on Chan-Vese model, local intensity fitting model, and topology optimization in Stokes flow.

Conclusion: The proposed framework provides an efficient, robust solution for interface optimization problems with strong theoretical guarantees, overcoming limitations of traditional binary methods while maintaining practical applicability across diverse domains.

Abstract: We present a robust and efficient numerical framework based on a median filter scheme for solving a broad class of interface-related optimization problems, from image segmentation to topology optimization. A key innovation of our work is the extension of the binary scheme into a continuous level-set scheme via a weighted quantile interpretation. Unlike traditional binary iterative convolution-thresholding method (ICTM), this continuous median filter scheme effectively overcomes the pinning effect caused by spatial discretization, achieving accurate interface evolution even with small time steps. We also provide a rigorous theoretical analysis, proving the unconditional energy stability of the iterative scheme. Furthermore, we prove that for a wide class of data fidelity terms, the convex relaxation inherently enforces a binary solution, justifying the effectiveness of the method without explicit penalization. Numerical experiments on the Chan--Vese model, the local intensity fitting (LIF) model, and topology optimization in Stokes flow demonstrate that the proposed efficient continuous framework effectively eliminates the pinning effect, guarantees unconditional energy stability, and accurately converges to binary solutions.

</details>


### [11] [Parametric charge-conservative mixed finite element method for 3D incompressible inductionless MHD equations on curved domains](https://arxiv.org/abs/2602.19375)
*Xue Jiang,Lei Li,Lingxiao Li*

Main category: math.NA

TL;DR: A charge-conservative mixed finite element method with optimal convergence rates for 3D stationary incompressible inductionless MHD on curved domains.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method for inductionless MHD equations that preserves charge conservation and achieves optimal convergence on complex 3D curved geometries, addressing the challenge of maintaining divergence-free properties in discretizations.

Method: Uses isoparametric Taylor-Hood elements with grad-div stabilization for velocity-pressure, and parametric Brezzi-Douglas-Marini elements for current density. Employs Piola's transformation to ensure exact divergence-free discrete current density, with suitable extensions and projections for analysis.

Result: Derives optimal a priori error estimates in both energy norm and L¬≤-norm. Numerical experiments confirm theoretical convergence rates, demonstrating the method's effectiveness on three-dimensional curved domains.

Conclusion: The proposed charge-conservative mixed finite element method successfully achieves optimal convergence for inductionless MHD equations on curved 3D domains while maintaining exact divergence-free properties for current density through careful discretization choices.

Abstract: This paper develops a charge-conservative mixed finite element method with optimal convergence rates for the stationary incompressible inductionless MHD equations on three-dimensional curved domains. The discretization employs the isoparametric Taylor-Hood elements with grad-div stabilization for the velocity-pressure pair, and parametric Brezzi-Douglas-Marini elements for the current density. Utilizing the Piola's transformation, the discrete current density is exactly divergence-free. By employing suitable extensions and projections, optimal a priori error estimates are derived in both the energy norm and the $L^2$-norm. Numerical experiments are presented to confirm the theoretical results.

</details>


### [12] [Dekker's floating point number system and compensated summation algorithms](https://arxiv.org/abs/2602.19452)
*Longfei Gao,Frimpong Baidoo*

Main category: math.NA

TL;DR: Analysis of compensated summation techniques for improving floating-point accuracy in reduced precision computing environments, with detailed error analysis using Dekker's non-unique number system.


<details>
  <summary>Details</summary>
Motivation: The trend toward reduced precision computing hardware has created a need for numerical techniques that can enhance accuracy beyond native floating-point operations, particularly for summation operations when addends are not known in advance.

Method: The paper studies various compensated summation techniques, provides complete error behavior descriptions, analyzes the relationship between intermediate results at consecutive summing steps, and uses Dekker's non-unique floating-point number system for analysis. Numerical examples are designed to explain the techniques and verify analytical results.

Result: The analysis identifies operations that limit accuracy in compensated summation techniques, provides relationships between consecutive summing steps, and offers guidance for designing more nuanced techniques. The use of Dekker's number system enables succinct expression of general statements.

Conclusion: Compensated summation techniques are valuable for enhancing floating-point accuracy in reduced precision computing environments, with Dekker's non-unique number system providing a convenient analytical framework. The work provides both theoretical analysis and practical guidance for application scenarios where these techniques can be beneficial.

Abstract: The recent hardware trend towards reduced precision computing has reignited the interest in numerical techniques that can be used to enhance the accuracy of floating point operations beyond what is natively supported for basic arithmetic operations on the hardware. In this work, we study the behavior of various compensated summation techniques, which can be used to enhance the accuracy for the summation operation, particularly in situations when the addends are not known a priori. Complete descriptions of the error behavior are provided for these techniques. In particular, the relationship between the intermediate results at two consecutive summing steps is provided, which is used to identify the operation that limits accuracy and guide the design of more nuanced techniques. The analysis relies on the work of Dekker [Numerische Mathematik, 1971], which uses a special floating point number system that does not require uniqueness in the number representation. Despite this seemingly strange attribute, Dekker's system is very convenient for the analysis here and allows general statements to be expressed succinctly compared to a number system that requires uniqueness. To prepare the foundation for discussion, we start by giving a thorough exhibition of Dekker's number system and supply the details that were omitted in Dekker [Numerische Mathematik, 1971]. Numerical examples are designed to explain the inner workings of these compensated summation techniques, illustrate their efficacy, and empirically verify the analytical results derived. Discussions are also given on application scenarios where these techniques can be beneficial.

</details>


### [13] [Optimal Error Estimates of a new Multiphysic Finite Element Method for Nonlinear Poroelasticity model with Hencky-Mises Stress Tensor](https://arxiv.org/abs/2602.19457)
*Yanan He,Zhihao Ge*

Main category: math.NA

TL;DR: A new multiphysics finite element method for nonlinear poroelasticity with Hencky-Mises stress tensor, featuring stable discretization, existence/uniqueness proofs, and optimal error estimates including novel L¬≤-norm displacement estimates.


<details>
  <summary>Details</summary>
Motivation: To develop a stable numerical method for nonlinear poroelastic models that can handle large Lam√© parameter Œª while maintaining stability across various Lagrangian element pairs, and to establish error estimates including previously missing L¬≤-norm displacement estimates.

Method: Reformulate the poroelastic model as a fluid-fluid coupling problem (generalized nonlinear Stokes + reaction-diffusion), then develop a fully discrete multiphysics FEM using Lagrangian elements for space and backward Euler for time, with careful parameter handling to ensure stability.

Result: Proved existence/uniqueness of weak solutions, established stability analysis, derived optimal error estimates for displacement and pressure in both L¬≤ and H¬π norms (including novel L¬≤ displacement estimates), and confirmed optimal convergence rates through numerical tests.

Conclusion: The proposed method successfully handles nonlinear poroelasticity with Hencky-Mises stress, provides comprehensive error analysis including previously missing estimates, and maintains stability across a wide range of Lagrangian element pairs even for large Œª.

Abstract: In this paper, we develop a new multiphysics finite element method for a nonlinear poroelastic model with Hencky-Mises stress tensor. By introducing some new notations, we reformulate the original model into a fluid-fluid coupling problem, which is viewed as a generalized nonlinear Stokes sub-problem combined with a reaction-diffusion sub-problem. Then, we establish the existence and uniqueness of the weak solution for the reformulated problem, and propose a stable, fully discrete multiphysics finite element method which employs Lagrangian finite element pairs for spatial discretization and a backward Euler scheme for temporal discretization. By ensuring the parameters $Œ∫_1$ and $Œ∫_3$ remain bounded and non-zero even as $Œª$ tends to infinity, the proposed method maintains stability for a wide range of Lagrangian element pairs. Based on the continuity and monotonicity of the nonlinear term $\mathcal{N}(\varepsilon(\mathbf{u}_h^{n}))$, we give the stability analysis and derive optimal error estimates for the displacement vector $\mathbf{u}$ and the pressure $p$ in both $L^2$-norm and $H^1$-norm. In particular, the $L^2$-norm error estimate for the displacement $\mathbf{u}$, which was not present in previous literature, is established here through an auxiliary problem and a Poincar$\acute{e}$ inequality. Also, we present numerical tests to verify the theoretical analysis, and the results confirm the optimal convergence rates. Finally, we draw conclusions to summarize the work.

</details>


### [14] [Extreme $L_p$ discrepancy, numerical integration and the curse of dimensionality](https://arxiv.org/abs/2602.19760)
*Erich Novak,Friedrich Pillichshammer*

Main category: math.NA

TL;DR: Extreme L_p discrepancy suffers from curse of dimensionality for all p‚àà(1,‚àû), tractable for p=‚àû, open for p=1.


<details>
  <summary>Details</summary>
Motivation: To understand the computational complexity of extreme L_p discrepancy as a measure of irregularity of point distribution in high dimensions.

Method: Find a dual integration problem whose worst-case error equals extreme L_p discrepancy, then analyze this integration problem's complexity.

Result: Extreme L_p discrepancy suffers from curse of dimensionality for all p‚àà(1,‚àû), meaning complexity grows exponentially with dimension.

Conclusion: The problem is intractable for p‚àà(1,‚àû), tractable for p=‚àû, and remains open for p=1.

Abstract: The classical notion of extreme $L_p$ discrepancy is a quantitative measure for the irregularity of distribution of finite point sets in the $d$-dimensinal unit cube. In this paper we find a dual integration problem whose worst-case error is exactly the extreme $L_p$ discrepancy of the underlying integration nodes. Studying this integration problem we show that the extreme $L_p$ discrepancy suffers from the curse of dimensionality for all $p \in (1,\infty)$. It is known that the problem is tractable for $p=\infty$; the case $p=1$ stays open.

</details>


### [15] [Optimal $L^2$-norm error estimate of multiphysics finite element method for poroelasticity model and simulating brain edema](https://arxiv.org/abs/2602.19854)
*Zhihao Ge,Yanan He,Yajie Yang*

Main category: math.NA

TL;DR: Optimal L¬≤-norm error estimate for multiphysics FEM in poroelasticity with brain edema application showing permeability K has biggest impact on intracranial pressure and tissue deformation.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze a multiphysics finite element method for poroelasticity models, with application to understanding brain edema caused by abnormal cerebrospinal fluid accumulation in injured brain areas.

Method: Derived optimal L¬≤-norm error estimate using an auxiliary problem approach, implemented multiphysics finite element method, conducted numerical tests for verification, and applied to brain edema simulation to investigate effects of physical parameters.

Result: Theoretical error estimate validated by numerical tests. In brain edema simulations: permeability K has biggest influence on intracranial pressure and tissue deformation; Young's modulus E and Poisson ratio ŒΩ have little effect on maximum intracranial pressure but significantly affect tissue deformation and edema development speed.

Conclusion: Multiphysics FEM provides accurate simulation of poroelasticity with verified error estimates. For brain edema, permeability is the most critical parameter for intracranial pressure control, while elastic parameters primarily affect tissue deformation patterns and edema progression rate.

Abstract: In this paper, we derive an optimal $L^2$-norm error estimate of the multiphysics finite element method for the poroelasticity model by introducing an auxiliary problem. We show some numerical tests to verify the theoretical result and apply the multiphysics finite element method to simulate the brain edema which caused by abnormal accumulation of cerebrospinal fluid in injured areas. And we investigate the effects of the key physical parameters on brain edema and observed that the permeability $K$ has the biggest influence on intracranial pressure and tissue deformation, Young's modulus $E$ and Poisson ratio $ŒΩ$ have little effect on the maximum value of intracranial pressure, but have great effect on the tissue deformation and the developing speed of brain edema.

</details>


### [16] [An new polar factor retraction on the Stiefel manifold with closed-form inverse](https://arxiv.org/abs/2602.19923)
*Rasmus Jensen,Ralf Zimmermann*

Main category: math.NA

TL;DR: Introduces a new second-order accurate retraction on the compact Stiefel manifold with closed-form inverse, addressing computational efficiency needs in Riemannian computing.


<details>
  <summary>Details</summary>
Motivation: Existing retractions on the Stiefel manifold lack closed-form inverses, which are crucial for computational efficiency in Riemannian computing applications. Current retractions either lack closed-form inverses or are only first-order accurate.

Method: Develops a new retraction on the compact Stiefel manifold that is second-order accurate under the Euclidean metric and features a closed-form inverse that can be efficiently computed.

Result: Presents the first Stiefel retraction with both second-order accuracy and closed-form inverse, outperforming existing retractions like Riemannian exponential, polar factor, QR, Cayley, and quasi-geodesic retractions.

Conclusion: The new retraction provides a computationally efficient solution for Riemannian computing on the Stiefel manifold, combining second-order accuracy with closed-form inverse computation for the first time.

Abstract: Retractions are the workhorse in Riemannian computing applications, where computational efficiency is of the essence. This work introduces a new retraction on the compact Stiefel manifold of orthogonal frames. The retraction is second-order accurate under the Euclidean metric and features a closed-form inverse that can be efficiently computed. To the best of our knowledge, this is the first Stiefel retraction with both these properties. A variety of retractions is known on the Stiefel manifold, including the Riemannian exponential map, the polar factor retraction, the QR-retraction and the Cayley retraction, but none of them features a closed-form inverse. The only Stiefel retraction with closed-form inverse that we are aware of is based on quasi-geodesics, but this one is of first order.

</details>


### [17] [On the Spectral Properties of Van Leer and AUSM Flux-Vector Splitting Schemes](https://arxiv.org/abs/2602.19963)
*Zhengrong Xie,Zheng Li*

Main category: math.NA

TL;DR: Detailed mathematical proof confirms Van Leer flux-vector splitting's eigenvalue properties and analyzes AUSM variants' eigenvalue characteristics.


<details>
  <summary>Details</summary>
Motivation: The original proof of Van Leer's flux-vector splitting scheme was presented in condensed form, requiring a more detailed and rigorous analysis of the eigenvalue sign conditions for the Jacobian matrices.

Method: Constructed Sturm sequence of the discriminant to analyze eigenvalues of Jacobian matrices for Van Leer splitting and extended analysis to two AUSM variants (linear and second-order pressure splittings).

Result: Proved that for Van Leer splitting (1‚â§Œ≥‚â§3, |M|<1, a>0), ‚àÇF‚Å∫/‚àÇU has one zero eigenvalue and two positive real eigenvalues. For AUSM variants: linear pressure splitting has eigenvalues not all same sign, while second-order pressure splitting has all characteristic equation coefficients positive with numerical confirmation of non-negative discriminant.

Conclusion: Provides rigorous mathematical foundation for Van Leer's original claims and clarifies eigenvalue properties of AUSM variants, with numerical validation supporting theoretical results for second-order pressure splitting.

Abstract: The flux-vector splitting scheme of Van Leer is a cornerstone of computational fluid dynamics, yet its original proof of the eigenvalue sign condition was presented in a condensed form. In this work, we provide a detailed and rigorous analysis of the eigenvalues of the Jacobian matrices associated with the Van Leer splitting for the one-dimensional Euler equations. By constructing the Sturm sequence of the discriminant, we prove that for the admissible parameter range $1 \le Œ≥\le 3$, $|M|<1$, and $a>0$, the Jacobian $\partial F^+/\partial U$ has one zero eigenvalue and two positive real eigenvalues, confirming Van Leer's original claim. Furthermore, we extend our analysis to two variants of the original AUSM scheme (Advection Upstream Splitting Method) proposed by Liou and Steffen, considering both linear and second-order pressure splittings. For the linear pressure splitting we show that the eigenvalues are not all of the same sign, while for the second-order pressure splitting we prove that all coefficients of the characteristic equation are positive. Numerical experiments reported in the appendix confirm the non-negativity of the discriminant for the AUSM with the second-order pressure splitting, implying that its eigenvalues are real and positive.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [18] [A new family of solitons for nonlinear Schr√∂dinger equations with non-vanishing boundary conditions in high dimension](https://arxiv.org/abs/2602.18691)
*Xiuqing Duan*

Main category: math.AP

TL;DR: New minimization procedure constructs traveling wave solutions for nonlinear Schr√∂dinger equations with non-vanishing boundary conditions in N‚â•4 dimensions, showing relationships between different soliton families.


<details>
  <summary>Details</summary>
Motivation: To develop a new construction method for traveling wave solutions to nonlinear Schr√∂dinger equations with non-vanishing boundary conditions at spatial infinity, and to establish relationships between different families of solitons obtained through various minimization approaches.

Method: Introduces a new minimization procedure to construct traveling wave solutions. Compares this new construction with existing approaches: Mari≈ü's action minimization with Pohozaev constraint (family ùìü) and Chiron & Mari≈ü's energy minimization at fixed momentum (family ùì†).

Result: Shows that under certain conditions: ùì† ‚äÇ ùìô ‚äÇ ùìü (the new family contains the fixed-momentum family and is contained within the Pohozaev-constrained family). Also shows that ùìü ‚äÇ ùìô under specific conditions, establishing relationships between these soliton families.

Conclusion: The new minimization procedure provides an alternative construction method for traveling wave solutions, and establishes inclusion relationships between different families of solitons obtained through various variational approaches in nonlinear Schr√∂dinger equations with non-vanishing boundary conditions.

Abstract: In space dimensions $N \geq 4$, we introduce a new minimization procedure to construct traveling wave solutions to nonlinear Schr√∂dinger equations with non-vanishing boundary conditions at spatial infinity. We denote the family of solitons obtained using this construction by $\mathscr{J}$. Mari≈ü (Ann. of Math. 178:107-182, 2013) obtained a family of solitons by minimizing the action functional subject to a Pohozaev constraint; we use $\mathscr{P}$ to denote this family of solitons. Chiron and Mari≈ü (Arch. Rational Mech. Anal. 226:143-242, 2017) used minimizing energy at fixed momentum to obtain a family of solitons; we denote this family of solitons by $\mathscr{Q}$. We show that, under some conditions, we have $\mathscr{Q} \subset \mathscr{J} \subset \mathscr{P}$. In addition, we show that $\mathscr{P} \subset \mathscr{J}$ under specific conditions.

</details>


### [19] [Green representations and global H√∂lder continuity for solutions of elliptic equations](https://arxiv.org/abs/2602.18737)
*Duc Duong*

Main category: math.AP

TL;DR: The paper constructs Green's function G and its derivatives H_l for a second-order elliptic PDE with measurable coefficients, enabling representation of solutions and their gradients via integral formulas.


<details>
  <summary>Details</summary>
Motivation: To establish explicit integral representation formulas for weak solutions of second-order elliptic equations with measurable coefficients, allowing direct computation of solutions and their gradients from source terms.

Method: Construct functions G(x,y) and H_l(x,y) on Œ©√óŒ© that serve as Green's function and its partial derivatives, enabling integral representations of solutions u and their gradients ‚àÇu/‚àÇx_l in terms of the source f.

Result: For f ‚àà L¬π(Œ©), the constructed functions satisfy ‚à´_Œ© G(x,y)f(x)dx = u(y) and ‚à´_Œ© H_l(x,y)f(x)dx = -‚àÇu/‚àÇx_l(y) almost everywhere in Œ© for all l=1,...,N.

Conclusion: The paper successfully constructs Green's function and its derivatives for elliptic PDEs with measurable coefficients, providing explicit integral representation formulas for solutions and their gradients.

Abstract: Let $N\in\mathbb{N}$ and $u$ be a weak solution of equation $\displaystyle Lu\equiv - \sum_{i,j=1}^{N}\frac{\partial}{\partial x_{j}}(\frac{\partial u}{\partial x_{i}}b^{ij})= f$ in $Œ©\subset \mathbb{R}^{N}$. We obtain functions $G$ and $H_{l}$ on $Œ©\times Œ©$ for every $l\in\{1,\cdots,N\}$ having following properties: if $f$ is in $L^{1}(Œ©)$, then $\int_Œ©G(x,y)f(x)dx = u(y)$, $\int_Œ©H_{l}(x,y)f(x)dx = -\frac{\ \partial u}{\partial x_{l}}(y)\quad a.e~y\in Œ©, \forall~l\in\{1,\cdots,N\}.$

</details>


### [20] [Statistical Error Bounds for Generative Solvers of Chaotic PDEs: Wasserstein Stability, Generalization, and Turbulence](https://arxiv.org/abs/2602.18794)
*Victor Armegioiu*

Main category: math.AP

TL;DR: Generative PDE samplers (flow matching, rectified flows, diffusion) are analyzed as law-level operators for statistical solutions of incompressible Euler equations, with convergence in time-averaged Wasserstein distance and structure-function controls.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous connections between modern generative samplers for PDE forecasting and the mathematical framework of statistical solutions for turbulent dynamics, bridging machine learning methods with theoretical fluid dynamics.

Method: Law-level analysis using correlation-measure framework (Lanthaler-Mishra-Par√©s-Pulido), convergence in time-averaged Wasserstein distance, structure-function bounds for compactness, discrete Gr√∂nwall recursion with average-strain amplification, and hierarchy residual analysis.

Result: Proved W‚ÇÇ stability estimate with distance-weighted average strain growth, error decomposition into resolved mismatch and high-frequency tail, precompactness in d_T, convergence of LM-admissible observables, and residual bounds by training errors.

Conclusion: Generative samplers induce valid approximations of statistical solutions when hierarchy residuals vanish, with finite-grid diagnostics interpretable as resolved observables within the statistical-solution framework.

Abstract: Statistical solutions of incompressible Euler describe turbulent dynamics as time-parameterized laws on $L^2$ whose multi-point correlations satisfy an infinite hierarchy of weak identities. Modern generative samplers for PDE forecasting (flow matching, rectified flows, diffusion via probability-flow ODEs) are measure-transport mechanisms and therefore induce Markov operators on laws. We develop a law-level analysis compatible with the correlation-measure framework of Lanthaler--Mishra--Par√©s-Pulido (LM): convergence in $d_T(Œº,ŒΩ)=\int_{0}^{T} W_{1}\!\bigl(Œº_t,ŒΩ_t\bigr)\,\mathrm{d}t$, compactness controlled by structure functions, and identification of limits through hierarchy identities.
  Quantitatively, we prove a $W_2$ stability estimate whose growth rate is a distance-weighted average strain under optimal couplings, and a one-step error decomposition into a resolved mismatch term and an unavoidable high-frequency coverage tail controlled by structure-function (spectral) bounds. These inputs propagate through multi-step rollouts via a discrete Gr√∂nwall recursion with amplification governed by the average-strain exponent rather than a worst-case Lipschitz constant. On the qualitative side, sampler-native path controls yield LM time regularity; together with uniform energy and structure-function bounds this gives precompactness in $d_T$ and strong convergence of LM-admissible observables. If hierarchy residuals vanish along a sequence, every limit is an LM statistical solution, with residuals bounded by training-native drift/score regression errors. Finally, we show how common finite-grid diagnostics--proper distributional scores and likelihood-style certificates--admit principled interpretations as resolved observables within the same statistical-solution framework.

</details>


### [21] [Heisenberg Uncertainty Principle on half spaces and Orthants: Best constants, Optimizers and Stability](https://arxiv.org/abs/2602.18810)
*Nguyen Lam,Yukta Lodha,Guozhen Lu,Ambar N. Sengupta*

Main category: math.AP

TL;DR: The paper establishes sharp Heisenberg Uncertainty Principle bounds for orthants/half-spaces, computes optimal constants, identifies extremal functions, and proves stability estimates.


<details>
  <summary>Details</summary>
Motivation: While the Heisenberg Uncertainty Principle has been well-studied in full Euclidean spaces, there's a gap in understanding its behavior on restricted domains like half-spaces and orthants, which are important for many applications.

Method: The authors investigate the sharp form of the Heisenberg Uncertainty Principle on orthants by explicitly computing optimal constants and determining all extremal functions that achieve these bounds.

Result: The paper provides explicit optimal constants for the Heisenberg Uncertainty Principle on orthants, characterizes all extremal functions, and establishes several stability estimates for half-spaces and orthants.

Conclusion: This work fills a significant gap in the literature by extending the sharp Heisenberg Uncertainty Principle analysis from full Euclidean spaces to restricted domains like orthants and half-spaces, with complete characterization of optimal bounds and extremals.

Abstract: Though the sharp Heisenberg Uncertainty Principle has been extensively studied in the entire Euclidean spaces, the counterpart on the half spaces or more general orthants has been missing in the literature. We investigate the sharp Heisenberg Uncertainty Principle on orthants by computing explicitly the optimal constant and determining all possible extremal functions. Moreover, we establish several stability estimates of the Heisenberg Uncertainty Principle on the half spaces and orthants.

</details>


### [22] [Steady states and dynamics of a higher dimensional thin film equation](https://arxiv.org/abs/2602.18827)
*Shen Bian*

Main category: math.AP

TL;DR: Analysis of a higher-dimensional thin film equation with competitive aggregation-repulsion effects, revealing critical thresholds for steady-state structure, variational properties, and dynamic behavior including blow-up thresholds.


<details>
  <summary>Details</summary>
Motivation: To understand how competitive effects between aggregation (backward second-order degenerate diffusion) and repulsion (fourth-order diffusion) influence steady-state solutions, variational structure, and dynamic evolution in higher-dimensional thin film equations with degenerate diffusion exponent m>0.

Method: Systematic analysis of existence and geometric properties of steady-state solutions for all m>0, identification of critical threshold m*=(d+2)/(d-2), proving that radially decreasing steady states coincide with extremals of Gagliardo-Nirenberg-Sobolev inequality and global minimizers of free energy for 0<m<m*, establishing uniqueness for m‚â†1+2/d, and identifying sharp L^{m+1} norm threshold in supercritical regime.

Result: Revealed critical threshold m* for variational compactness and solution structure; proved equivalence between radially decreasing steady states, inequality extremals, and energy minimizers for 0<m<m*; established uniqueness for m‚â†1+2/d; identified sharp threshold in supercritical regime distinguishing global existence from finite-time blow-up based on initial data relative to unique radial steady-state L^{m+1} norm.

Conclusion: Steady-state solutions serve as theoretical pivot to construct unified analytical framework connecting parameter classification, variational structure, and dynamic behavior, elucidating how regularity barrier prevents infinite energy descent and selects stable equilibrium states, providing unified variational principle for understanding steady-state selection and dynamic bifurcations in higher-order degenerate diffusion equations.

Abstract: We study a higher-dimensional thin film equation that incorporates competitive effects between aggregation and repulsion, where repulsion is modeled by fourth-order diffusion and aggregation by backward second-order degenerate diffusion, with a degenerate diffusion exponent $m>0$. We first conduct a systematic analysis of the existence and geometric properties of steady-state solutions for all $m>0$, revealing a critical threshold $m^*=(d+2)/(d-2)$ for variational compactness and solution structure. For $0 < m < m^*$, we then prove that, under natural regularity constraints, radially decreasing steady states coincide with both the extremals of the Gagliardo-Nirenberg-Sobolev inequality and the global minimizers of the free energy. Moreover, we establish the uniqueness of such steady states for $m \neq 1 + 2/d$. Furthermore, in the supercritical regime $1 + 2/d < m < m^*$, we identify a sharp threshold given by the $L^{m+1}$ norm of the unique radial steady-state solution, which distinguishes between global existence for initial data below the threshold and finite-time blow-up for initial data above the threshold. The main contribution of this work is to use steady-state solutions as a theoretical pivot to construct a unified analytical framework that connects parameter classification, variational structure and dynamic behavior. This framework elucidates how the regularity barrier prevents infinite energy descent and selects stable equilibrium states, and thus predicts the global evolution of the system, thereby providing a unified variational principle for understanding steady-state selection and dynamic bifurcations in such higher-order degenerate diffusion equations.

</details>


### [23] [Slow-fast system in Rosales-Majda combustion model with fractional order kinetics](https://arxiv.org/abs/2602.18841)
*Claude-Michel Brauner,Jinlong Jing,Robert Roussarie*

Main category: math.AP

TL;DR: Study of traveling wave solutions for a 1D detonation model with reaction order Œ±‚àà[0,1), analyzing bifurcation diagrams in (Œ≤,c) parameter space and identifying viscous shock wave trajectories.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of traveling wave solutions in detonation models with simplified chemical kinetics, particularly focusing on how viscosity (Œ≤) and wave velocity (c) parameters affect the system dynamics and shock wave formation.

Method: The paper uses a slow-fast dynamical system approach for temperature and mass fraction variables. Analytical methods include Poincar√©-Bendixson theorem and Fenichel theory (geometric singular perturbation theory), complemented by numerical computations to verify theoretical results.

Result: The study produces bifurcation diagrams in the (Œ≤,c) parameter space and characterizes the nature of trajectories corresponding to viscous shock waves, revealing how different parameter regimes affect detonation wave behavior.

Conclusion: The combination of analytical techniques (Poincar√©-Bendixson and Fenichel theory) with numerical validation provides comprehensive understanding of traveling wave solutions in detonation models, particularly for reaction orders Œ±‚àà[0,1) with simplified chemical kinetics.

Abstract: We consider traveling wave solutions of a one-dimensional model for detonation waves derived by Rosales and Majda, when the reaction order $Œ±$ belongs to $[0,1)$. The chemical kinetics is a simplified Arrhenius law or a Heaviside function. The model in the reaction zone is a slow-fast dynamical system for a vector representing temperature and mass fraction, which depends on the velocity $c$ and small viscosity $Œ≤$. Our goal in this paper is to study the bifurcation diagram in the $(Œ≤,c)$ parameter space and identify the nature of the trajectories corresponding to viscous shock waves. The demonstrations are based on a variety of techniques including the Poincar√©-Bendixson theorem and the Fenichel theory. Theoretical results are confirmed by numerical computations.

</details>


### [24] [Partial regularity in nonlocal systems II](https://arxiv.org/abs/2602.18848)
*Cristiana De Filippis,Giuseppe Mingione,Simon Nowak*

Main category: math.AP

TL;DR: Nonlinear nonlocal systems of order 2s>1 in ‚Ñù‚Åø have C¬π,Œ± regularity (Œ±<2s-1) outside a closed singular set with Hausdorff dimension < n-2, which is empty when n=2.


<details>
  <summary>Details</summary>
Motivation: To establish regularity properties for solutions to nonlinear nonlocal systems of order greater than 1, understanding where solutions are smooth and characterizing the singular set where regularity breaks down.

Method: The paper likely uses techniques from regularity theory for nonlocal operators, potentially involving blow-up analysis, fractional Sobolev spaces, and geometric measure theory to study the singular set.

Result: Solutions are C¬π,Œ± for every Œ±<2s-1 outside a closed singular set with Hausdorff dimension less than n-2. When n=2, the singular set is empty, meaning solutions are globally C¬π,Œ±.

Conclusion: Nonlinear nonlocal systems of order 2s>1 exhibit good regularity properties: solutions are smooth outside a small singular set, and in dimension 2, they are completely smooth everywhere.

Abstract: Solutions to nonlinear nonlocal systems of order $2s>1$ in $\mathbb{R}^n$ are $C^{1,Œ±}$, for every $Œ±<2s-1$, outside a closed singular set whose Hausdorff dimension is less than $n-2$, and which is empty when $n=2$.

</details>


### [25] [Multiple positive bound states for NLS equations on noncompact metric graphs with an attractive potential](https://arxiv.org/abs/2602.18894)
*Q. Liu*

Main category: math.AP

TL;DR: Existence of multiple bound states for subcritical NLS with attractive potential on metric graphs when mass is large enough.


<details>
  <summary>Details</summary>
Motivation: To establish existence of bounded states and geometrically distinct solutions for NLS equations on metric graphs, addressing limitations where ground states may fail to exist for all mass values.

Method: Analysis of subcritical nonlinear Schr√∂dinger (NLS) equation with attractive potential on metric graphs, using topological and variational methods to count solutions based on graph structure.

Result: At least as many bound states exist as the number of bounded edges when mass Œº is sufficiently large, under suitable assumptions on the attractive potential.

Conclusion: Metric graphs support multiple geometrically distinct bound states for large mass, contrasting with ground state existence issues, providing insight into solution multiplicity based on graph topology.

Abstract: In this paper, we establish the existence of bounded states and geometrically distinct solutions for the subcritical NLS equation with attractive potential on metric graphs $\mathcal{G}$ when the mass $Œº$ is large enough.We show that the NLS equation exists at least as many bound states of mass $Œº$ as the number of bounded edges of $\mathcal{G}$ if the attractive potential satisfies some suitable assumptions.It is worth noting that this is different from the case of ground state, which on some graphs may fail to exist for every value of $Œº$.

</details>


### [26] [A unified duality framework for barotropic, quantum and Korteweg fluids](https://arxiv.org/abs/2602.18917)
*Dmitry Vorotnikov*

Main category: math.AP

TL;DR: The paper develops a dual variational formulation for compressible fluid models (barotropic Euler, quantum Euler, Euler-Korteweg) using Brenier's approach, establishes consistency with time-adaptive weights, proves existence of variational dual solutions, and formulates a Dafermos principle for entropy dissipation.


<details>
  <summary>Details</summary>
Motivation: To develop a unified variational framework for analyzing compressible fluid models, extending Brenier's duality approach from incompressible to compressible settings, and to establish rigorous mathematical foundations for these systems.

Method: Dual variational formulation inspired by Brenier, with time-adaptive weights to handle large time intervals; abstract framework unifying three compressible fluid systems; measure-theoretic approach for continuous, vacuum-free initial data in finite Radon measure spaces.

Result: Proved consistency of duality scheme on large time intervals; existence of variational dual solutions for Cauchy problems; absence of duality gap; formulation and proof of Dafermos principle for entropy dissipation; connections to Brenier's shock-free substitutes for Burgers' equation.

Conclusion: The paper successfully extends Brenier's variational approach to compressible fluid models, providing a unified framework that yields existence results and fundamental principles like the Dafermos principle, connecting to broader mathematical fluid dynamics theory.

Abstract: We investigate a dual variational formulation, in the spirit of Brenier, for several compressible fluid models: the compressible barotropic Euler system, the quantum Euler system, and the Euler-Korteweg system. We identify a unified abstract framework encompassing all three systems, which enables a simultaneous analysis. By introducing time-adaptive weights, we establish the consistency of the duality scheme on large time intervals. We prove the existence of variational dual solutions to the corresponding Cauchy problems for continuous, vacuum-free initial data in spaces of finite Radon measures, and establish the absence of a duality gap. As an application, we formulate and prove a 'Dafermos principle' for these models: no subsolution can dissipate the total entropy earlier or at a faster rate than the corresponding strong solution on its interval of existence. We also discuss connections between our abstract consistency result and Brenier's shock-free substitutes for entropy solutions of Burgers' equation.

</details>


### [27] [A generalized Helmholtz-type decomposition of symmetric tensor fields and applications to ray transforms](https://arxiv.org/abs/2602.18983)
*Antti Kykk√§nen,Rohit Kumar Mishra,Suman Kumar Sahoo*

Main category: math.AP

TL;DR: The paper extends solenoidal-potential decomposition to 2D symmetric m-tensor fields with mean-zero condition, and uses this to prove injectivity of momentum and elastic ray transforms for these tensors.


<details>
  <summary>Details</summary>
Motivation: Previous work established general decomposition results for symmetric tensor fields with restrictions on dimension and order. The authors aim to extend these decomposition results to dimension 2 and apply them to prove injectivity properties for important integral transforms in tomography.

Method: The authors extend solenoidal-potential type decomposition to symmetric m-tensor fields in R^2 under a mean-zero assumption. They then use this decomposition to analyze momentum and elastic ray transforms, proving injectivity results and establishing connections between the two transforms for 2-tensors.

Result: Successfully extended decomposition to dimension 2 with mean-zero condition. Proved injectivity of momentum and elastic ray transforms using this decomposition. Established a connection between the two integral transforms specifically for 2-tensors.

Conclusion: The solenoidal-potential decomposition in 2D with mean-zero condition provides a powerful tool for proving injectivity of momentum and elastic ray transforms, revealing important connections between these transforms in tensor tomography.

Abstract: We study a solenoidal-potential type decomposition of a symmetric $m$-tensor field in $\Rb^2$, and its implications to injectivity questions for the momentum and elastic ray transforms. For symmetric tensor fields, a general decomposition with a restriction on the dimension and order of the decomposition was proved in~\cite{Rohit_Suman}. We extend the result to dimension $2$ under a mean-zero assumption. We use the decomposition in $2$ dimensions to prove the injectivity of the momentum and elastic ray transforms. We also prove a connection between the two integral transforms for $2$-tensors.

</details>


### [28] [Mathematical analysis for a doubly degenerate parabolic equation: Application to the Richards equation](https://arxiv.org/abs/2602.19037)
*Abderrahmane Benfanich,Yves Bourgault,Abdelaziz Beljadid*

Main category: math.AP

TL;DR: Mathematical analysis of doubly degenerate parabolic equations applied to Richards equation using bounded auxiliary variable, establishing existence of weak solutions via semi-implicit time discretization and maximal monotone operator theory in weighted Sobolev spaces.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical analysis for doubly degenerate parabolic equations, specifically the Richards equation, which models water flow in porous media. The challenge lies in handling strict degeneracy and strong nonlinearities without imposing unrealistic positivity constraints on diffusivity or requiring high solution regularity.

Method: Uses bounded auxiliary variable approach with semi-implicit time discretization combined with maximal monotone operator theory. Analysis conducted within weighted Sobolev spaces to handle degeneracy and nonlinearities. Employs L-scheme linearization for numerical approximation.

Result: Establishes existence of weak solutions without requiring strictly positive lower bounds on diffusivity or high regularity. Proves Richards equation preserves physical bounds of saturation with the auxiliary variable. Demonstrates unconditional linear convergence of L-scheme linearization to semi-discrete solution.

Conclusion: The bounded auxiliary variable approach provides rigorous mathematical framework for analyzing doubly degenerate parabolic equations like Richards equation, enabling robust solution existence proofs and convergent numerical schemes without restrictive assumptions.

Abstract: This paper presents a mathematical analysis of a doubly degenerate parabolic equation and its application to the Richards equation using a bounded auxiliary variable. We establish the existence of weak solutions using semi-implicit time discretization combined with maximal monotone operator theory. The analysis is conducted within weighted Sobolev spaces, allowing for a rigorous treatment of the equation's strict degeneracy and strong nonlinearities. A key feature of this study is the derivation of convergence results without imposing strictly positive lower bounds on the diffusivity or requiring high regularity of the solution. Furthermore, we prove that the Richards equation using the introduced auxiliary variable preserves the physical bounds of the saturation and demonstrate the unconditional linear convergence of the L-scheme linearization to the semi-discrete solution.

</details>


### [29] [Symmetry and Approximate Symmetry for Solutions of Mixed Local-Nonlocal Singular Equations](https://arxiv.org/abs/2602.19054)
*Sanjit Biswas*

Main category: math.AP

TL;DR: Establishes radial symmetry for positive weak solutions of mixed local-nonlocal equations with singular nonlinearities using moving plane method, plus quantitative Gidas-Ni-Nirenberg type theorem.


<details>
  <summary>Details</summary>
Motivation: To study quantitative properties of solutions to mixed local-nonlocal equations, which combine local and nonlocal operators, particularly focusing on symmetry properties and establishing new analytical tools for this class of equations.

Method: Uses moving plane method to prove radial symmetry; establishes weak Harnack-type inequality and Alexandroff-Bakelman-Pucci inequality analogue for mixed nonhomogeneous setting with lower order term.

Result: Proves radial symmetry for positive weak solutions of mixed local-nonlocal equations with possibly singular nonlinearity; provides quantitative version of Gidas-Ni-Nirenberg type theorem; establishes new inequalities for mixed problems.

Conclusion: This paper initiates quantitative study of mixed local-nonlocal equations, providing symmetry results and new analytical tools that appear to be novel contributions to the field.

Abstract: In this article, we establish radial symmetry for positive weak solutions of a class of mixed local-nonlocal equations with possibly singular nonlinearity via the moving plane method. Furthermore, we provide a quantitative version of Gidas-Ni-Nirenberg type theorem for mixed local-nonlocal equations. To this regard, we establish a weak Harnack-type inequality and an analogue of the Alexandroff-Bakelman-Pucci inequality in the mixed nonhomogeneous setting with a lower order term, which appear to be new. To the best of our knowledge, this paper initiates the study of the quantitative properties of solutions to mixed problems.

</details>


### [30] [Non-uniqueness of smooth solutions to the Navier-Stokes equations on torus $\TTT^2$](https://arxiv.org/abs/2602.19074)
*Changxing Miao,Yao Nie,Weikui Ye*

Main category: math.AP

TL;DR: The paper solves the open problem of local well-posedness for 2D incompressible Navier-Stokes equations in BMO‚Åª¬π space, overcoming geometric intersection difficulties of 2D Mikado flows.


<details>
  <summary>Details</summary>
Motivation: While the 3D case was recently settled by Coiculescu and Palasek (2025), the 2D case remained open despite two decades of attention. The 2D setting presents unique difficulties due to geometric intersections of two-dimensional Mikado flows.

Method: Develops a heat-dominated Fourier mode flow built upon steady two-dimensional Euler flows, and presents the proof using a new iterative scheme to overcome the geometric intersection challenges.

Result: Solves the two-dimensional problem of local well-posedness for incompressible Navier-Stokes equations in BMO‚Åª¬π space, completing the picture after the recent 3D breakthrough.

Conclusion: The paper successfully resolves the long-standing open problem in 2D, providing a complete understanding of Navier-Stokes well-posedness in BMO‚Åª¬π spaces across both two and three dimensions.

Abstract: The local well-posedness theory for the incompressible Navier-Stokes equations in $\BMO^{-1}$ has attracted considerable attention over the past two decades. In a recent breakthrough, Coiculescu and Palasek (Invent. Math., 2025) settled the three-dimensional case by demonstrating the existence of two distinct global solutions, both smooth for $t>0$, evolving from a common initial datum in ${\rm BMO}^{-1}(\mathbb{T}^3)$. However, the two-dimensional case remains open. In this paper, we solve the two-dimensional problem. Unlike its three-dimensional counterpart, the two-dimensional setting presents additional difficulties stemming from the geometric intersections of two-dimensional Mikado flows. To overcome these difficulties, we develop a heat-dominated Fourier mode flow built upon steady two-dimensional Euler flows, and present the proof using a new iterative scheme.

</details>


### [31] [A unified theory for diffusion with memory and delay via measure-valued kernels](https://arxiv.org/abs/2602.19099)
*Hiroki Ishizaka*

Main category: math.AP

TL;DR: The paper analyzes a diffusion equation with historical effects modeled by a Borel measure, establishing well-posedness and stability for general finite measures, and identifying conditions for dissipative memory terms.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive mathematical framework for diffusion equations with memory/historical effects that can handle various types of delay mechanisms (distributed memory, discrete delays, mixed kernels) within a unified measure-theoretic approach.

Method: The authors use a measure-theoretic approach where memory-delay is represented by a nonnegative Borel measure Œº. The diffusion component uses a coercive bilinear form a‚ÇÄ, while historical effects are modeled as a measure-induced convolution operator K_Œº derived from a bounded non-negative form a‚ÇÅ. They analyze well-posedness, stability bounds, and identify positive-type conditions for dissipative memory terms.

Result: Established well-posedness over finite time horizons for general finite measures Œº, derived stability bounds applicable with delays, identified positive-type condition for dissipative memory terms leading to refined energy inequality, and provided verifiable sufficient conditions for absolutely continuous kernels including complete monotonicity and internal-variable representation.

Conclusion: The measure-theoretic framework successfully unifies various memory models (memory-free, distributed-memory Volterra, discrete-delay, mixed kernels) and provides rigorous mathematical foundations for analyzing diffusion equations with historical effects, including stability conditions and decay mechanisms.

Abstract: We investigate a diffusion equation that incorporates historical effects, in which the memory-delay mechanism is represented by a nonnegative Borel measure $Œº$ on the interval $(0, \mathfrak T]$. The diffusion component is characterised by a coercive bilinear form $a_0$, while the historical term is modelled as a measure-induced convolution operator $\mathcal K_Œº$, derived from a bounded non-negative form $a_1$. This framework includes the memory-free parabolic problem ($Œº=0$), distributed-memory Volterra models ($dŒº(s)=k(s)\,ds$), discrete-delay models with atomic measures, and mixed kernels. For general finite measures $Œº$, we establish well-posedness over finite time horizons and derive stability bounds that remain applicable in the presence of delays. Subsequently, we identify a positive-type condition under which the memory term is dissipative, leading to a refined energy inequality. For absolutely continuous kernels, we provide verifiable sufficient conditions, including complete monotonicity, and an internal-variable representation that elucidates the decay mechanism.

</details>


### [32] [Local well-posedness for the Boltzmann equation with hard potentials](https://arxiv.org/abs/2602.19148)
*Hao-Guang Li,Wei-Xi Li,Chao-Jiang Xu*

Main category: math.AP

TL;DR: Local existence and uniqueness of weak solutions for non-cutoff Boltzmann equation with hard potentials, requiring pointwise bounds on hydrodynamic quantities.


<details>
  <summary>Details</summary>
Motivation: To establish well-posedness for the spatially inhomogeneous non-cutoff Boltzmann equation with hard potentials in the non-perturbative regime, addressing the more severe velocity moment loss compared to soft potentials.

Method: Combines hypoelliptic estimates with interpolation inequalities to handle the moment-loss terms, working with initial data having polynomial decay in velocity.

Result: Proves local-in-time existence and uniqueness of weak solutions, conditional on pointwise bounds for mass, energy, and entropy (hydrodynamic quantities).

Conclusion: The paper successfully extends existence theory to hard potentials despite more severe moment loss, using a conditional approach that requires control of hydrodynamic quantities.

Abstract: We consider the spatially inhomogeneous non-cutoff Boltzmann equation with hard potentials in the non-perturbative setting. For initial data with polynomial decay in the velocity variable, we establish the local-in-time existence and uniqueness of weak solutions, conditional to pointwise bounds on the hydrodynamic quantities (mass, energy, and entropy). Compared to the soft potential case, the key challenge for full-range hard potentials lies in the more severe loss of velocity moments. The proof combines a hypoelliptic estimate with interpolation inequalities to handle the moment-loss terms.

</details>


### [33] [Regularity of Second-Order Elliptic PDEs in Spectral Barron Spaces](https://arxiv.org/abs/2602.19381)
*Ziang Chen,Liqiang Huang,Mengxuan Yang,Shengxuan Zhou*

Main category: math.AP

TL;DR: The paper establishes regularity results for elliptic PDEs in spectral Barron spaces, showing solutions gain two orders of Barron regularity under mild conditions. This enables dimension-independent neural network approximations.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of solutions to elliptic PDEs in high dimensions and enable efficient approximation by neural networks without the curse of dimensionality.

Method: The authors use spectral Barron spaces to analyze second-order elliptic PDEs on ‚Ñù^d. They prove regularity theorems under mild ellipticity and smallness assumptions, showing solutions gain two additional orders of Barron regularity.

Result: The main result establishes that solutions to certain elliptic PDEs gain two orders of Barron regularity. As a corollary, they identify a class of PDEs whose solutions can be approximated by two-layer neural networks with cosine activations, with width independent of spatial dimension.

Conclusion: The regularity theory in spectral Barron spaces provides a mathematical foundation for dimension-independent neural network approximations of PDE solutions, overcoming the curse of dimensionality for a class of elliptic problems.

Abstract: We establish a regularity theorem for second-order elliptic PDEs on $\mathbb{R}^{d}$ in spectral Barron spaces. Under mild ellipticity and smallness assumptions, the solution gains two additional orders of Barron regularity. As a corollary, we identify a class of PDEs whose solutions can be approximated by two-layer neural networks with cosine activation functions, where the width of the neural network is independent of the spatial dimension.

</details>


### [34] [An attractive analytic solution of the Maxwell's equation](https://arxiv.org/abs/2602.19191)
*Xiaorong Zou*

Main category: math.AP

TL;DR: Analytic solution for Maxwell's equations with smooth periodic initial conditions, complexity comparable to Fourier expansions of initial functions.


<details>
  <summary>Details</summary>
Motivation: To provide an attractive analytic solution for Maxwell's equations when given smooth periodic functions as initial conditions, addressing the need for explicit solutions in such scenarios.

Method: Develops an analytic approach to solve Maxwell's equations using smooth periodic initial functions, with complexity scaling similarly to Fourier expansions of those initial functions.

Result: Successfully obtains analytic solutions for Maxwell's equations with demonstrative examples, showing the solution complexity is comparable to Fourier expansions of initial functions.

Conclusion: The paper presents an efficient analytic solution method for Maxwell's equations with periodic initial conditions, offering practical utility with manageable computational complexity.

Abstract: In this paper, we provide an attractive analytic solution for Maxwell's equation for a given set of smooth periodic functions as initial condition with demonstrative examples. The complexity of the solution is comparable to the Fourier expansions of the initial functions.

</details>


### [35] [Finiteness of Nonscattering Wavenumbers for Herglotz Incident Waves](https://arxiv.org/abs/2602.19302)
*Jingni Xiao*

Main category: math.AP

TL;DR: Finiteness of nonscattering wavenumbers for star-shaped domains in 2D with inhomogeneous media, generalizing previous results and removing geometric restrictions.


<details>
  <summary>Details</summary>
Motivation: To study nonscattering phenomena for inhomogeneous media, continuing previous work [30], and understand when infinite sequences of nonscattering wavenumbers can occur versus when only finitely many exist.

Method: Analyze star-shaped domains in ‚Ñù¬≤ with Herglotz incident waves. For ellipses with constant medium coefficient q‚àà(0,1)‚à™(1,‚àû), prove finiteness of nonscattering wavenumbers. For admissible C¬≤ star-shaped domains with q‚àà(0,1), establish analogous results under broader geometric assumptions on radius function.

Result: 1) For ellipses with constant q‚àà(0,1)‚à™(1,‚àû), there exist at most finitely many nonscattering wavenumbers. 2) For admissible C¬≤ star-shaped domains with q‚àà(0,1), analogous finiteness results hold under broader geometric assumptions. Infinite sequences of nonscattering wavenumbers require exact radial symmetry and cannot persist under admissible geometric perturbations.

Conclusion: Nonscattering wavenumbers are generically finite for star-shaped domains with inhomogeneous media, except in cases of exact radial symmetry. The results generalize and strengthen previous work by removing geometric restrictions and broadening applicability.

Abstract: This paper continues the study initiated in [30] on nonscattering phenomena for inhomogeneous media. We investigate star-shaped domains in $\mathbb{R}^2$ and establish finiteness results for nonscattering wavenumbers associated with Herglotz incident waves of fixed density. First, for ellipses with constant medium coefficient $q\in(0,1)\cup(1,\infty)$, we prove that there exist at most finitely many nonscattering wavenumbers. This generalizes and strengthens the corresponding results in [30], in particular removing additional geometric restrictions in the case $q>1$. Second, for admissible $C^2$ star-shaped domains with $q\in(0,1)$, we establish analogous finiteness results under broader geometric assumptions on the radius function. Our results reveal that infinite sequences of nonscattering wavenumbers are tied to exact radial symmetry and cannot persist under admissible geometric perturbations.

</details>


### [36] [Discretization and regularization for the reconstruction of inhomogeneities by scattering measurements](https://arxiv.org/abs/2602.19970)
*Daniela Di Donato,Luca Rondi*

Main category: math.AP

TL;DR: Finite measurement acoustic scattering inverse problem solved via discrete variational regularization with convergence analysis for parameter selection


<details>
  <summary>Details</summary>
Motivation: To reconstruct inhomogeneities from a finite number of acoustic scattering measurements in time-harmonic setting, addressing the practical limitation of finite measurements

Method: Formulate reconstruction as fully discrete variational problem with regularization, involving parameters for number of measurements, regularization parameter, and mesh discretization size for Helmholtz equation coefficients

Result: Convergence analysis shows careful parameter selection enables discrete regularized solution to approximate true inverse problem solution

Conclusion: Discrete variational regularization with properly chosen parameters provides effective approach for acoustic scattering inverse problems with finite measurements

Abstract: We consider the inverse problem of reconstructing inhomogeneities by performing a finite number of scattering measurements of acoustic type in the time-harmonic setting. We set up the reconstruction as a fully discrete variational problem with regularization. Such a problem depends on a variety of parameters, that is, the number of measurements, the regularization parameter and the discretization parameter, namely the size of the mesh on which we discretize the unknown coefficients of the Helmholtz type equation modelling our physical system. We show, through a convergence analysis, that one can carefully choose these parameters in such a way that the solution to this discrete regularized minimum problem is a good approximation of the looked-for solution to the inverse problem.

</details>


### [37] [High-order long-time asymptotics for small solutions to the one-dimensional nonlinear Schr√∂dinger equation](https://arxiv.org/abs/2602.19374)
*Jacek Jendrej,Tony Salvi*

Main category: math.AP

TL;DR: Global well-posedness and modified scattering for 1D Schr√∂dinger equation with gauge-invariant polynomial nonlinearity for small localized initial data.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time behavior of solutions to the one-dimensional Schr√∂dinger equation with gauge-invariant polynomial nonlinearity, particularly addressing global existence and asymptotic behavior for small initial data.

Method: Space-time resonance method is used to analyze the equation. The approach handles small localized initial data of finite energy in low-regularity classes.

Result: Established global existence of solutions with persistence of localization for the associated profile. Derived rigorous asymptotic expansion at arbitrary order, accounting for long-range effects from cubic nonlinearity.

Conclusion: The space-time resonance method successfully proves global well-posedness and provides detailed asymptotic description for 1D Schr√∂dinger equations with gauge-invariant polynomial nonlinearities, including handling of long-range scattering effects.

Abstract: We investigate the global well-posedness and modified scattering for the one-dimensional Schr√∂dinger equation with gauge-invariant polynomial nonlinearity. For small localized initial data of finite energy in a low-regularity class, we establish global existence of solution together with persistence of the localization of the associated profile. We further provide a rigorous derivation of the asymptotic expansion at arbitrary order of such solutions, taking into account long-range effects induced by the cubic component of the nonlinearity. Our analysis relies on the space-time resonance method.

</details>


### [38] [An isoperimetric inequality for the second Robin eigenvalue of the Weighted Laplacian](https://arxiv.org/abs/2602.19456)
*Yi Gao,Kui Wang,Anqiang Zhu*

Main category: math.AP

TL;DR: Ball centered at origin maximizes second Robin eigenvalue for weighted Laplacian among symmetric Lipschitz domains with prescribed weighted measure, for negative Robin parameters.


<details>
  <summary>Details</summary>
Motivation: Study shape optimization for second Robin eigenvalue of weighted Laplacian on symmetric domains, seeking domains that maximize this eigenvalue under constraints.

Method: Investigate shape optimization problem on bounded Lipschitz domains symmetric about origin, using weighted Laplacian with Robin boundary conditions and prescribed weighted measure constraint.

Result: Main theorem proves ball centered at origin maximizes second Robin eigenvalue among all Lipschitz bounded domains of prescribed weighted measure symmetric about origin, for range of negative Robin parameters.

Conclusion: For symmetric domains with prescribed weighted measure and negative Robin parameters, the ball centered at origin is optimal shape for maximizing second Robin eigenvalue of weighted Laplacian.

Abstract: In this paper, we investigate a shape optimization problem for the second Robin eigenvalue of the weighted Laplacian on bounded Lipschitz domains symmetric about the origin. Our main theorem states that the ball centered at the origin maximizes the second Robin eigenvalue among all Lipschitz bounded domains of prescribed weighted measure and symmetric about the origin for a range of negative Robin parameters.

</details>


### [39] [The homogeneous and inhomogeneous Dirichlet problem](https://arxiv.org/abs/2602.19551)
*David S. Jerison,Carlos E. Kenig*

Main category: math.AP

TL;DR: The paper addresses controversies in Dirichlet problems for Laplacian on Lipschitz domains, refuting recent contradictory claims by Amrouche and Moussaoui through multiple proofs supporting Dahlberg's original results.


<details>
  <summary>Details</summary>
Motivation: To address recent postings by Amrouche and Moussaoui that purport to contradict established results by Dahlberg and Jerison-Kenig regarding area integral estimates and Sobolev space estimates for Dirichlet problems on Lipschitz domains.

Method: The authors identify the fatal gap in Amrouche and Moussaoui's reasoning, provide a self-contained proof of a special case of Dahlberg's results, and present two additional proofs adapted from work by Kenig and Dahlberg-Kenig-Pipher-Verchota.

Result: The paper successfully disproves the central conclusions of Amrouche and Moussaoui's postings by demonstrating that Dahlberg's original results remain valid, supported by multiple independent proofs.

Conclusion: The established results of Dahlberg and Jerison-Kenig regarding Dirichlet problems on Lipschitz domains remain correct, and the contradictory claims by Amrouche and Moussaoui are invalid due to fundamental gaps in their reasoning.

Abstract: We revisit the homogeneous and inhomogeneous Dirichlet problem for the Laplacian on Lipschitz domains. This is motivated by the recent postings by Amrouche and Moussaoui which purport to contradict known area integral estimates of Dahlberg and known Sobolev space estimates of Jerison and Kenig. We explain the fatal gap in the reasoning in these postings and give a self-contained proof of a special case of the results of Dahlberg. We then show that this is sufficient to disprove the central conclusions of these postings. We also provide two further proofs of the results of Dahlberg, adapted from work of Kenig and of Dahlberg-Kenig-Pipher-Verchota. Other proofs are in the original paper by Dahlberg and in work by Fabes-Mendez-Mitrea.

</details>


### [40] [An inverse random source problem for the fractional Helmholtz equation](https://arxiv.org/abs/2602.19559)
*Peijun Li,Zhenqian Li*

Main category: math.AP

TL;DR: The paper studies an inverse random source problem for stochastic fractional Helmholtz equations, showing that principal symbols of covariance and relation operators can be uniquely recovered from far-field patterns of a single random source realization.


<details>
  <summary>Details</summary>
Motivation: To develop mathematical theory for inverse problems involving stochastic fractional Helmholtz equations, which have applications in wave propagation through random media and uncertainty quantification in inverse scattering problems.

Method: Combines Born linearization, asymptotic expansions of fractional Helmholtz Green kernel at high wavenumbers, microlocal analysis of Fourier integral operators, and analysis of Lippmann-Schwinger integral equations.

Result: Establishes well-posedness of direct problem in distributional sense; proves unique determination (with probability one) of principal symbols of covariance and relation operators from far-field patterns of single random source realization.

Conclusion: The inverse random source problem for stochastic fractional Helmholtz equations is solvable using microlocal analysis techniques, enabling recovery of statistical properties of random sources from far-field measurements.

Abstract: This paper investigates an inverse random source problem for the stochastic fractional Helmholtz equation. The source is modeled as a centered, complex-valued, microlocally isotropic generalized Gaussian random field whose covariance and relation operators are described by classical pseudo-differential operators. For sufficiently large wavenumbers, we first establish the well-posedness of the direct problem in the distributional sense by analyzing the corresponding Lippmann--Schwinger integral equation. For the inverse problem, we show that the principal symbols of both the covariance and relation operators can be uniquely determined, with probability one, from the far-field patterns generated by a single realization of the random source. The approach employs a combination of the Born linearization, asymptotic expansions of the fractional Helmholtz Green kernel at high wavenumbers, and microlocal analysis of associated Fourier integral operators.

</details>


### [41] [Homogenization for the Poisson equation in domains perforated by random closed sets](https://arxiv.org/abs/2602.19579)
*Naoto Sato*

Main category: math.AP

TL;DR: Homogenization of Poisson equation in randomly perforated domains yields strange term effect; perforations modeled by rescaled germ-grain processes with stationary hole capacities; homogenized potential is constant despite nonstationary hole distribution.


<details>
  <summary>Details</summary>
Motivation: To understand homogenization effects in randomly perforated domains, particularly the emergence of strange terms in the homogenized equation, and to analyze how stationary capacities of holes affect homogenization despite potentially nonstationary spatial distributions.

Method: Use rescaled germ-grain processes to model random perforations; assume stationary capacities of holes; study homogenization of Poisson equation in these domains; establish corrector results.

Result: Obtain strange term effect in homogenized equation; show homogenized potential is constant regardless of potentially nonstationary spatial distribution of holes; establish corrector results for convergence.

Conclusion: Homogenization in randomly perforated domains with stationary hole capacities leads to constant potential in homogenized equation, demonstrating that capacity stationarity rather than spatial distribution determines homogenized behavior; corrector results validate the homogenization approach.

Abstract: We study the homogenization of the Poisson equation in randomly perforated domains and obtain the strange term effect in the homogenized equation. The perforations are modeled by rescaled germ-grain processes, and the main assumption is stationarity of the capacities of the holes. We emphasize that the potential in the homogenized equation is constant, despite the possibly nonstationary spatial distribution of the holes. We also establish corrector results.

</details>


### [42] [Gradual smoothing: strong hypercontractivity and logarithmic Sobolev inequalities](https://arxiv.org/abs/2602.19650)
*Arturo de Pablo,David Lee,Fernando Quir√≥s,Jorge Ruiz-Cases*

Main category: math.AP

TL;DR: The paper studies regularity improvement over time for solutions to parabolic evolution problems driven by non-translation-invariant L√©vy operators, establishes equivalence between smoothing effects and logarithmic Sobolev inequalities, introduces strong hypercontractivity, and analyzes specific L√©vy operators.


<details>
  <summary>Details</summary>
Motivation: To understand how the regularity of solutions to parabolic evolution problems driven by L√©vy operators improves over time, particularly for non-translation-invariant operators, and to explore the relationship between smoothing effects and logarithmic Sobolev inequalities.

Method: Establishes equivalence between general smoothing effects and families of logarithmic Sobolev inequalities; introduces and characterizes strong hypercontractivity; analyzes purely nonlocal L√©vy operators with kernels comparable to log(I-Œî); studies translation-invariant cases.

Result: Identifies strong hypercontractivity as a new regularization type; shows purely nonlocal L√©vy operators comparable to log(I-Œî) are strongly hypercontractive but not supercontractive or ultracontractive; proves eventual boundedness of solutions in translation-invariant cases.

Conclusion: The paper establishes connections between smoothing effects and logarithmic Sobolev inequalities, introduces strong hypercontractivity as a distinct regularization property, and provides specific results about L√©vy operators comparable to log(I-Œî), showing they exhibit strong hypercontractivity but not stronger contractivity properties.

Abstract: We study the possibility of a gradual improvement as time progresses of the regularity of solutions to evolution problems of parabolic type driven by L√©vy operators, not necessarily translation invariant. In the course of our analysis we study the equivalence between general smoothing effects and a family of logarithmic Sobolev inequalities. This equivalence allows us to identify a new type of regularization, strong hypercontractivity, characterized by the existence of a time at which solutions belong to every $L^p$ space with $p$ finite. It can also be used to prove logarithmic Sobolev inequalities in a context not previously seen in the literature. We then show that any purely nonlocal L√©vy operator whose kernel is comparable to that of $\log(I-Œî)$ is strongly hypercontractive, but fails to be supercontractive and, consequently, also fails to be ultracontractive. Finally, in the translation-invariant case, we also prove that solutions get bounded eventually.

</details>


### [43] [A Lagrangian approach for prescribed mass solutions of cubic-quintic Schr√∂dinger equations and $L^2$-supercritical problems](https://arxiv.org/abs/2602.19751)
*Silvia Cingolani,Marco Gallo,Kazunaga Tanaka*

Main category: math.AP

TL;DR: The paper studies existence of radially symmetric normalized solutions for nonlinear scalar field equations in ‚Ñù·¥∫ with L¬≤ constraint, using Lagrangian formulation and mountain pass techniques to analyze critical points of the functional b‚Çò(Œº).


<details>
  <summary>Details</summary>
Motivation: To establish new existence results for normalized solutions (with fixed L¬≤ norm) of nonlinear scalar field equations without requiring global Ambrosetti-Rabinowitz type conditions, improving previous results by Jeanjean and collaborators.

Method: Uses Lagrangian formulation J‚Çò(Œº,u) with unknown multiplier Œº, defines b‚Çò(Œº) as mountain pass minimax value, and studies existence through local minima/maxima of b‚Çò(Œº). Applies to cubic-quintic type equations and L¬≤-supercritical problems.

Result: Shows existence of solutions related to local minima and maxima of b‚Çò(Œº). For N=2,3, obtains new existence results for normalized solutions without global Ambrosetti-Rabinowitz conditions, partially improving Jeanjean's and Jeanjean-Lu's results.

Conclusion: The Lagrangian approach with mountain pass techniques provides a framework to establish existence of normalized solutions for nonlinear scalar field equations under weaker conditions than previously required, with applications to specific nonlinearities.

Abstract: We study the existence of radially symmetric solutions of the following nonlinear scalar field equations in $\mathbb R^N$ ($N \geq 2$):
  $$ (*)_m \quad - Œîu + Œºu = g(u) \quad \text{in}\ {\mathbb R}^N, \quad {1\over 2} \int_{{\mathbb R}^N} u^2\, dx = m,$$
  where $g(s) \in C({\mathbb R},{\mathbb R})$, $m > 0$ and $Œº\in {\mathbb R}$ is an unknown Lagrangian multiplier. We take an approach using a Lagrangian formulation of $(*)_m$:
  $$J_m(Œº,u)={1\over 2}\int_{{\mathbb R}^N} |\nabla u|^2\,dx -\int_{{\mathbb R}^N} G(u)\,dx +Œº\left({1\over 2}\int_{{\mathbb R}^N} u^2\, dx-m\right) \in C^1((0,\infty)\times H_r^1({\mathbb R}^N), {\mathbb R})$$
  and we give new general existence results through the function:
  $$ b_m:\, (0,\infty) \to {\mathbb R};\ Œº\mapsto \text{Mountain Pass minimax value for}\ (u\mapsto J_m(Œº,u)).$$
  We will show the existence of solutions of $(*)_m$ related to local minima and local maxima of $b_m(Œº)$. As applications, we study cubic-quintic type equations and $L^2$-supercritical problems. In particular, when $N=2,3$, we show new existence results of normalized solutions without assuming global Ambrosetti-Rabinowitz type conditions, which partially improve the preceding results due to Jeanjean [24] and Jeanjean-Lu [26, 28].

</details>


### [44] [Local well-posedness of strong solutions to the non-isentropic compressible primitive equations with vertical diffusion](https://arxiv.org/abs/2602.19801)
*Rupert Klein,Jinkai Li,Xin Liu,Edriss S. Titi*

Main category: math.AP

TL;DR: Local existence and uniqueness of strong solutions for non-isentropic compressible primitive equations with vertical temperature diffusion but without gravity, under specific regularity conditions.


<details>
  <summary>Details</summary>
Motivation: The primitive equations of atmospheric dynamics present greater mathematical challenges than Navier-Stokes equations due to the absence of dynamical equation for vertical momentum, requiring recovery of vertical velocity from other quantities via hydrostatic balance, leading to spatial derivative loss and stronger nonlinearity.

Method: Analyze the initial-boundary value problem for non-isentropic compressible primitive equations with only vertical diffusion for temperature (no gravity). Assume initial velocity and pressure have one order higher regularity than initial density.

Result: Established local existence and uniqueness of strong solutions, as well as continuous dependence on initial data, for any suitably regular initial data meeting the specified regularity conditions.

Conclusion: Successfully proved well-posedness results for this challenging system of compressible primitive equations, addressing the mathematical difficulties arising from the hydrostatic balance constraint and derivative loss.

Abstract: Due to the absence of dynamical equation in the vertical momentum component of the primitive equations (PEs) of atmospheric dynamics, the vertical component of the velocity can be recovered only from the information on the other physical quantities, while utilizing the hydrostatic balance. This causes one spatial derivative loss while leads to a stronger nonlinearity, comparing to the classic compressible Navier-Stokes equations. As a result, the mathematical analysis on the compressible primitive equations is mathematically more challenging than that on the compressible Navier-Stokes equations. In this paper, we consider the initial-boundary value problem to the non-isentropic compressible primitive equations with only vertical diffusion for the temperature, but without gravity. Local existence and uniqueness as well as the continuous dependence on the initial data of strong solutions are established for any suitably regular initial data. The initial velocity and pressure are assumed here to be of one order derivative higher regularity than that of the initial density.

</details>


### [45] [Sharp non-uniqueness for the Navier-Stokes equations in scaling critical spaces](https://arxiv.org/abs/2602.19846)
*Mikihiro Fujii*

Main category: math.AP

TL;DR: The paper shows that uniqueness of mild solutions to Navier-Stokes equations in critical Besov spaces is sharp - uniqueness fails for slightly larger scaling-critical spaces, and provides a complete classification of uniqueness for all Besov space pairs (p,q).


<details>
  <summary>Details</summary>
Motivation: While uniqueness is known for mild solutions in the critical class C([0,T);L^n(R^n)), it's unclear whether this result is sharp or if uniqueness holds in other scaling-critical spaces. The paper aims to determine the precise boundary of uniqueness in critical Besov spaces.

Method: The authors provide a complete classification for every pair (p,q) of whether uniqueness holds in critical Besov spaces C([0,T);\dot{B}_{p,q}^{n/p-1}(R^n)). They develop a non-uniqueness mechanism that produces infinitely many global solutions even from zero initial state, with large-time asymptotics governed by non-trivial stationary flow.

Result: The paper proves that uniqueness in L^n(R^n) is sharp - uniqueness fails if L^n(R^n) is replaced by some scaling critical spaces that are even slightly larger. They provide the first examples of non-dissipative unforced Navier-Stokes flow with critical regularity.

Conclusion: The uniqueness result for mild solutions in C([0,T);L^n(R^n)) is optimal, and the paper establishes a complete classification of uniqueness in critical Besov spaces, revealing that non-uniqueness can occur in slightly larger spaces, with solutions exhibiting non-trivial stationary flow asymptotics.

Abstract: It is known that uniqueness of mild solutions to the incompressible Navier-Stokes equations holds in the critical class $C([0,T);L^n(\mathbb{R}^n))$ for $n \geqslant 3$. In this paper, we prove that this result is sharp in the sense that uniqueness fails if $L^n(\mathbb{R}^n)$ is replaced by some scaling critical spaces that are even slightly larger. We achieve this through a complete classification for every pair $(p,q)$ of whether uniqueness of mild solutions in the critical Besov class $C([0,T);\dot{B}_{p,q}^{n/p-1}(\mathbb{R}^n))$ holds or not. Our non-uniqueness mechanism produces infinitely many global solutions emanating even from zero initial state, whose large-time asymptotics are governed by non-trivial stationary flow. To the best of our knowledge, such non-unique solutions provide the first examples of non-dissipative unforced Navier-Stokes flow with critical regularity.

</details>


### [46] [Stability and Finite-Time Blow-Up for a Fractionally Damped Nonlinear Plate Equation: Numerical and Analytical Insights](https://arxiv.org/abs/2602.19856)
*Iqra Kanwal,Jianghao Hao,Muhammad Fahim Aslam,Mauricio Sep√∫lveda-Cort√©s*

Main category: math.AP

TL;DR: The paper analyzes a nonlinear plate equation with fractional damping and time-delay, establishing local existence, stability/decay results via Lyapunov functionals, and blow-up for negative initial energy via concavity method, with numerical validation.


<details>
  <summary>Details</summary>
Motivation: To understand the competing effects of fractional damping and delayed feedback on viscoelastic and feedback-controlled elastic structures, particularly how these factors influence stability and potential blow-up behavior in nonlinear plate systems.

Method: 1) Semigroup theory for local existence/uniqueness of weak solutions; 2) Construction of Lyapunov functional for stability and energy decay analysis; 3) Concavity method to prove finite-time blow-up for negative initial energy; 4) Numerical simulations to validate analytical results.

Result: 1) Local existence and uniqueness of weak solutions established; 2) Stability and energy decay results obtained via Lyapunov functional; 3) Solutions with negative initial energy blow up in finite time; 4) Numerical simulations confirm analytical predictions of both stability and blow-up dynamics.

Conclusion: Fractional damping and time-delay feedback have competing effects on nonlinear plate systems: damping promotes stability and energy decay, while delayed feedback can lead to instability and finite-time blow-up under certain initial conditions, with numerical results validating the theoretical analysis.

Abstract: This paper studies a nonlinear plate equation with internal fractional damping and a time-delay term, driven by a polynomial-type nonlinear source. Such a model arises naturally in the description of viscoelastic and feedback-controlled elastic structures. We first establish the local existence and uniqueness of weak solutions using semigroup theory. The long-time behavior of solutions is then analyzed by constructing a suitable Lyapunov functional, from which stability and energy decay results are obtained. Moreover, by applying the concavity method, we prove that solutions associated with negative initial energy blow up in finite time. These results highlight the competing effects of fractional damping and delayed feedback on the qualitative behavior of the system. Finally, numerical simulations are presented to confirm the analytical results and to illustrate both stability and blow-up dynamics.

</details>


### [47] [Measuring the Infinite: An Expository Journey Through Interpolation Theory, Lorentz Spaces, and Dispersive PDEs](https://arxiv.org/abs/2602.19911)
*Asuman G√ºven Aksoy,Daniel Akech Thiong*

Main category: math.AP

TL;DR: Interpolation theory and Lorentz spaces provide essential tools for analyzing PDEs when classical Lebesgue spaces fail at critical endpoints, enabling rigorous study of thermal diffusion and quantum dispersion phenomena.


<details>
  <summary>Details</summary>
Motivation: Classical Lebesgue spaces (Lp) often fail to bound linear and nonlinear evolution operators at critical endpoints (p=1 or p=‚àû) because they conflate function amplitude with spatial spread, creating an analytic bottleneck in PDE analysis.

Method: Introduces distribution functions and decreasing rearrangements to construct Lorentz spaces (Lp,q), then applies Complex (Riesz-Thorin) and Real (Peetre's K-functional) interpolation methods to create sensitive intermediate spaces that bridge endpoint extremes.

Result: Successfully applies the framework to derive continuous smoothing decay for the parabolic Heat equation and establishes foundational dispersive Strichartz estimates for the hyperbolic free Schr√∂dinger equation.

Conclusion: Interpolation theory serves as the essential mathematical language for quantifying both thermal diffusion and quantum dispersion phenomena in PDE analysis, with Lorentz spaces providing the necessary refinement beyond classical Lebesgue spaces.

Abstract: This expository article explores the vital role of interpolation theory and Lorentz spaces in the rigorous analysis of partial differential equations (PDEs). While classical Lebesgue spaces ($L_{p}$) successfully measure the magnitude of functions, they frequently fail to bound linear and non-linear evolution operators at critical endpoints of $p=1$ or $p = \infty$ because they conflate a function's amplitude with its spatial spread. To resolve this analytic bottleneck, we introduce distribution functions and decreasing rearrangements, culminating in the construction of Lorentz spaces ($L_{p, q}$). By utilizing the Complex (Riesz-Thorin) and Real (Peetre's K-functional) methods of interpolation, these highly sensitive intermediate spaces act as geometric bridges between endpoint extremes. We apply this framework to two distinct physical models: deriving the continuous smoothing decay of the parabolic Heat equation, and establishing the foundational dispersive Strichartz estimates for the hyperbolic free Schr√∂dinger equation. Ultimately, interpolation theory is shown to be the essential mathematical language for quantifying both thermal diffusion and quantum dispersion.

</details>


### [48] [On compressible magnetic relaxation in planar symmetry](https://arxiv.org/abs/2602.19947)
*Taehun Kim*

Main category: math.AP

TL;DR: Compressible Magnetic Relaxation Equations on 3D torus, derived from MHD with Darcy friction. Under planar symmetry: local well-posedness, magnetic relaxation near constant states, and no vacuum/implosion before singularities.


<details>
  <summary>Details</summary>
Motivation: To study compressible magnetic relaxation dynamics derived from magnetohydrodynamics, replacing acceleration with Darcy friction. Understanding relaxation behavior and singularity formation in compressible magnetic systems.

Method: Analyze compressible Magnetic Relaxation Equations on 3D torus under planar symmetry. Use mathematical analysis techniques to prove: (1) local well-posedness for smooth initial data, (2) magnetic relaxation near constant steady states, (3) absence of vacuum/implosion before potential singularities.

Result: Three main results established: (1) Local well-posedness for smooth initial data, (2) Magnetic relaxation for smooth perturbations of constant steady states, (3) No vacuum states or implosions occur prior to or at potential singularity times.

Conclusion: The compressible Magnetic Relaxation Equations exhibit well-posed behavior under planar symmetry, demonstrate magnetic relaxation near equilibrium, and avoid vacuum formation or implosion before potential singularities, providing insights into compressible magnetic relaxation dynamics.

Abstract: We consider the compressible Magnetic Relaxation Equations on the three-dimensional torus $\mathbb{T}^{3}$. The system is derived from compressible magnetohydrodynamics (MHD) by replacing the acceleration term with a Darcy-type friction. Under planar symmetry, we establish three main results: (1) local well-posedness for smooth initial data, (2) magnetic relaxation for smooth perturbations of constant steady states, and (3) the absence of vacuum states or implosions prior to and at the time of a potential singularity.

</details>


### [49] [1D Scattering through time dependent media with memory](https://arxiv.org/abs/2602.19981)
*Jeffrey Galkowski,Zhen Huang,Maciej Zworski*

Main category: math.AP

TL;DR: This paper constructs a scattering matrix with operator-valued entries to describe solutions to the 1+1 wave equation with time-space dependent permittivities that have memory effects.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a rigorous mathematical framework for scattering phenomena in wave equations with memory-dependent, time-space varying permittivities, extending the classical scattering matrix approach to more complex media.

Method: The authors construct a scattering matrix with operator-valued entries (rather than frequency-dependent functions) that describes solutions to the 1+1 wave equation. This extends the classical scattering matrix approach to handle permittivities with memory and time-space dependence.

Result: The main result is the construction of this operator-valued scattering matrix, which provides a mathematical explanation for numerical constructions in recent work by Horsley et al. An appendix by Huang and Zworski presents a numerical scheme for solving such wave equations.

Conclusion: The paper successfully develops a mathematical framework for scattering in wave equations with memory-dependent, time-space varying permittivities, bridging theoretical analysis with numerical applications.

Abstract: We construct a scattering matrix with operator valued entries describing solutions to the 1+1 wave equation where permittivities has memory and depends on time and space. It is the analogue of the scattering matrix for spatially localised perturbations where the entries are functions of frequency and appear as Fourier multipliers in solutions of the wave equation. This provides a mathematical explanation of the numerical construction in the recent paper by Horsley et al. The appendix by Zhen Huang and Maciej Zworski presents a numerical scheme for solving the wave equation considered in this article.

</details>


### [50] [Global and local properties of solutions of elliptic equations with a nonlinear term involving the product of the function and its gradient](https://arxiv.org/abs/2602.19999)
*Zhihao Lu*

Main category: math.AP

TL;DR: The paper establishes optimal Liouville theorems for global positive solutions and optimal gradient estimates for local positive solutions to quasi-linear elliptic equations with gradient and power nonlinearities, extending results to Riemannian manifolds.


<details>
  <summary>Details</summary>
Motivation: To strengthen previous results by Bidaut-V√©ron, Garc√≠a-Huidobro, and V√©ron on quasi-linear elliptic equations with gradient and power nonlinearities, providing both global and local perspectives with optimal conditions.

Method: 1. Develops an optimal new identity for the modulus squared of the logarithmic gradient to prove Liouville theorems. 2. Discovers a mutual control relationship between two nonlinear terms to derive gradient estimates. Extends results to Riemannian manifolds.

Result: 1. Optimal and improved Liouville theorems for global positive solutions. 2. Several optimal gradient estimates for local positive solutions. 3. Corresponding Harnack inequality as a direct corollary. All results are optimal and strengthen previous work.

Conclusion: The paper provides comprehensive optimal results for quasi-linear elliptic equations with gradient and power nonlinearities, establishing both global nonexistence theorems and local regularity estimates that improve upon existing literature.

Abstract: We study the global and local properties of positive solutions to the quasi-linear elliptic equation:
  \d u+|\nabla u|^q u^p=0,\quad x\in √ò\subset \mathbb{R}^n,\nonumber where $q\ge 0$ and $p\in\mathbb{R}$. Our contributions are twofold: 
  1. Based on an optimal and new identity for the modulus squared of the logarithmic gradient, we establish optimal and improved Liouville theorems for global positive solutions, and generalize these findings to the framework of Riemannian manifolds.
  2. Based on a newly discovered mutual control relationship of two nonlinear iterms, for all index pairs \( (p, q) \) where the Liouville theorem holds, we derive several optimal gradient estimates for local positive solutions. As a direct corollary, we obtain the corresponding Harnack inequality.
  These results strengthen the related conclusions in Bidaut-V√©ron--Garc√≠a-Huidobro--V√©ron \cite {BGV} from both global and local perspectives.

</details>


### [51] [Existence of weak solutions for incompressible fluid-Koiter shell interactions with Navier slip boundary condition](https://arxiv.org/abs/2602.20016)
*Claudiu M√ÆndrilƒÉ,Arnab Roy*

Main category: math.AP

TL;DR: Global existence of weak solutions for 3D fluid-structure interaction with Koiter shell and Navier slip boundary conditions, valid until first possible shell self-intersection.


<details>
  <summary>Details</summary>
Motivation: Study 3D fluid-structure interaction problems involving incompressible viscous fluid coupled with deformable elastic shells, addressing challenges from moving domains, geometric nonlinearities, and slip boundary conditions.

Method: Use incompressible Navier-Stokes equations on time-dependent domains coupled with nonlinear Koiter shell model. Employ Navier slip boundary conditions with tangential slip penalized by friction. Develop approximation schemes, compactness arguments for slip framework, and divergence-free test function extension operators.

Result: Prove global-in-time existence of weak solutions up to first possible shell self-intersection for arbitrarily large initial data with finite energy in fully 3D setting.

Conclusion: Successfully addresses major mathematical challenges of moving domains, geometric nonlinearities, and reduced regularization from slip conditions. Provides framework for treating nonlinear Koiter shell models with direct approach to strong convergence of shell displacement derivatives.

Abstract: We study a three-dimensional fluid-structure interaction problem describing the motion of an incompressible, viscous fluid coupled with a deformable elastic shell of Koiter type that forms part of the fluid boundary. The fluid motion is governed by the incompressible Navier--Stokes equations posed on a time-dependent domain, while the shell evolution is described by a nonlinear elastic model. At the fluid-structure interface, we impose Navier slip boundary conditions, allowing for tangential slip penalized by friction. Our main result establishes the global-in-time existence of weak solutions up to the first possible self-intersection of the shell, for arbitrarily large initial data with finite energy. The analysis is carried out in a fully three-dimensional setting and addresses the major mathematical challenges arising from the moving domain, the geometric nonlinearity of the shell, and the reduced regularization induced by the slip boundary condition. The proof relies on a careful construction of suitable approximation schemes, novel compactness arguments adapted to the slip framework, and a new extension operator for divergence-free test functions compatible with the fluid-shell coupling. As a further contribution, we provide a direct approach to the strong convergence of second-order spatial derivatives of the shell displacement, which allows us to treat nonlinear Koiter shell models within the same framework.

</details>


### [52] [A Liouville-type theorem for $2$-Monge-Amp√®re equation in dimension three](https://arxiv.org/abs/2602.20090)
*Weisong Dong*

Main category: math.AP

TL;DR: Entire solutions with quadratic growth to 2-Monge-Amp√®re equation on ‚Ñù¬≥ are quadratic polynomials when lying in a suitable cone.


<details>
  <summary>Details</summary>
Motivation: To characterize entire solutions to the 2-Monge-Amp√®re equation on ‚Ñù¬≥ with quadratic growth, determining when such solutions must be quadratic polynomials.

Method: First establish a concavity inequality, then derive a Pogorelov-type interior C¬≤ estimate to prove the result.

Result: Proved that every entire solution with quadratic growth (lying in a suitable cone) to the 2-Monge-Amp√®re equation on ‚Ñù¬≥ is a quadratic polynomial.

Conclusion: The 2-Monge-Amp√®re equation on ‚Ñù¬≥ has rigidity properties: entire solutions with quadratic growth in appropriate cones are necessarily quadratic polynomials.

Abstract: We prove that every entire solution with quadratic growth, lying in a suitable cone, to the 2-Monge-Amp√®re equation on $\mathbb{R}^3$ is a quadratic polynomial. The proof proceeds by first establishing a concavity inequality, and then deriving a Pogorelov-type interior $C^2$ estimate.

</details>


### [53] [Global dynamics of a single vortex ring](https://arxiv.org/abs/2602.20131)
*Dengjun Guo,In-Jee Jeong,Lifeng Zhao*

Main category: math.AP

TL;DR: Global-in-time dynamics of vortex rings in 3D Euler equations: vortex filament conjecture validated for generic initial data, with universal filamentation mechanism causing linear stretching and instability.


<details>
  <summary>Details</summary>
Motivation: To understand the global dynamics of vortex rings in 3D incompressible Euler equations and validate the vortex filament conjecture for generic initial data, while investigating filamentation mechanisms and stability properties.

Method: Study axisymmetric flows without swirl, analyze vortex ring dynamics using macroscopic invariants, prove vorticity concentration and propagation with Kelvin-Hicks speed, identify filamentation mechanism driven by competition between core translation and local induction.

Result: First global-in-time validation of vortex filament conjecture for single vortex ring from generic data; vorticity remains sharply concentrated and propagates with Kelvin-Hicks speed; universal filamentation mechanism causes linear-in-time stretching of vortex support, leading to W^{2,‚àû} dynamical instability of thin vortex rings.

Conclusion: Vortex filament conjecture holds globally for generic vortex ring initial data, but thin vortex rings are dynamically unstable due to universal filamentation mechanism causing linear stretching, challenging stability assumptions in vortex ring theory.

Abstract: We study the global-in-time dynamics of vortex rings for the three-dimensional incompressible Euler equations, under the assumption of axisymmetric flows without swirl.
  For a broad class of initial data sharing only the macroscopic invariants with a thin vortex ring, we prove that the vorticity remains sharply concentrated and propagates along the symmetry axis with leading-order speed given by the Kelvin--Hicks formula, providing the first global-in-time validation of the vortex filament conjecture for a single vortex ring arising from generic initial data.
  We further identify a universal filamentation mechanism driven by the competition between rapid core translation and slower local induction. This mechanism gives linear-in-time stretching of the vortex support under very general assumptions on the data, yielding dynamical instability of any thin vortex ring configurations in the $W^{2,\infty}$ norm.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [54] [Updating DMD Operators for Changes in Domain Properties](https://arxiv.org/abs/2602.18441)
*Dimitrios Voulanas,Eduardo Gildin*

Main category: physics.comp-ph

TL;DR: Lightweight update methods for Dynamic Mode Decomposition (DMD) models that adapt to changing reservoir properties without retraining or new simulation data, enabling fast optimization for carbon storage.


<details>
  <summary>Details</summary>
Motivation: High-fidelity multiphase simulators are too expensive for optimization and uncertainty analysis in geological carbon storage. While DMD offers data-driven reduction, conventional approaches require retraining when reservoir properties change, eliminating speed advantages.

Method: Two complementary update strategies: 1) For uniform permeability changes, adjust model's internal dynamics and control response to match new flow timescales; 2) For spatially varying permeability, modify spatial representation to give greater influence to high-permeability zones in the reduced basis.

Result: Numerical experiments show the updates recover plume migration and pressure build-up within 3% of freshly trained surrogates while executing hundreds of times faster than full retraining.

Conclusion: The lightweight update methods enable real-time optimization and rapid what-if studies for carbon storage while preserving required physical fidelity, without needing new simulation data or retraining.

Abstract: Fast and reliable surrogate models are critical for optimization, control and uncertainty analysis in geological carbon-storage projects, yet high-fidelity multiphase simulators remain too expensive. Dynamic Mode Decomposition (DMD) offers an attractive data-driven reduction framework, but its operators are trained for a single set of reservoir properties. When permeability or well location changes, conventional practice is to regenerate snapshots and retrain the surrogate, erasing most of the speed advantage. This work presents a lightweight alternative that updates an existing DMD or DMD-with-control model without incorporating new simulation data or retraining. Two complementary update strategies are introduced. For cases where permeability changes uniformly across the domain, the proposed updates adjust the models internal dynamics and control response to match the new flow timescale. When permeability varies in space, the approach modifies the spatial representation so that high-permeability zones are given greater influence on the models reduced basis. Numerical experiments demonstrate that the proposed updates recover plume migration and pressure build-up within three percent of a freshly trained surrogate yet execute hundreds of times faster than full retraining. These methods therefore enable real-time optimization and rapid what-if studies while preserving the physical fidelity demanded by carbon-storage workflows.

</details>


### [55] [Boltzmann Generators for Condensed Matter via Riemannian Flow Matching](https://arxiv.org/abs/2602.18482)
*Emil Hoffmann,Maximilian Schebek,Leon Klein,Frank No√©,Jutta Rogal*

Main category: physics.comp-ph

TL;DR: Riemannian flow matching with periodicity for equilibrium sampling in condensed-phase systems, validated on monatomic ice with accurate free energy estimates.


<details>
  <summary>Details</summary>
Motivation: Flow matching shows promise for generative modeling but remains unexplored for equilibrium sampling in condensed-phase systems, which require handling periodicity and accurate thermodynamic reweighting.

Method: Incorporates periodicity into continuous normalizing flows using Riemannian flow matching, mitigates computational cost with Hutchinson's trace estimator and bias-correction via cumulant expansion for rigorous thermodynamic reweighting.

Result: Validated on monatomic ice, demonstrating ability to train on unprecedented system sizes and obtain highly accurate free energy estimates without traditional multistage estimators.

Conclusion: The approach successfully applies flow matching to equilibrium sampling in condensed-phase systems, overcoming computational challenges and enabling accurate free energy estimation for large systems.

Abstract: Sampling equilibrium distributions is fundamental to statistical mechanics. While flow matching has emerged as scalable state-of-the-art paradigm for generative modeling, its potential for equilibrium sampling in condensed-phase systems remains largely unexplored. We address this by incorporating the periodicity inherent to these systems into continuous normalizing flows using Riemannian flow matching. The high computational cost of exact density estimation intrinsic to continuous normalizing flows is mitigated by using Hutchinson's trace estimator, utilizing a crucial bias-correction step based on cumulant expansion to render the stochastic estimates suitable for rigorous thermodynamic reweighting. Our approach is validated on monatomic ice, demonstrating the ability to train on systems of unprecedented size and obtain highly accurate free energy estimates without the need for traditional multistage estimators.

</details>


### [56] [Multiphysics Modelling of the Molten Salt Fast Reactor using NekRS and the Fission Matrix Method](https://arxiv.org/abs/2602.18626)
*Maximiliano Dalinger,Elia Merzari,Saya Lee*

Main category: physics.comp-ph

TL;DR: Researchers developed a multiphysics computational model for Molten Salt Fast Reactors using a reduced-order neutronics approach (Fission Matrix Method) integrated with thermal-hydraulics in the Cardinal/MOOSE framework.


<details>
  <summary>Details</summary>
Motivation: The MSFR's unique design where coolant also serves as fuel creates strong coupling between neutronics and thermal hydraulics, requiring multiphysics computational models for accurate analysis.

Method: Developed a neutronic thermal-hydraulic model using the Fission Matrix Method (a reduced-order neutronics approach) instead of OpenMC, integrated within the Cardinal code (MOOSE framework) that couples with NekRS for CFD.

Result: Proposed a computational framework that enables fast and accurate neutronics simulations for MSFR analysis by using precalculated Monte Carlo databases with the Fission Matrix Method.

Conclusion: The Fission Matrix Method provides an efficient alternative to full Monte Carlo simulations for MSFR multiphysics modeling, enabling faster yet accurate coupled neutronics-thermal hydraulics analysis.

Abstract: The Molten Salt Fast Reactor (MSFR) has the particularity that the coolant is also the fuel, which tightens the coupling between neutronics and thermal hydraulics as the fuel circulates through the primary system. Therefore, developing computational models to analyze the MSFR requires a multiphysics approach. In this paper, we propose developing a neutronic thermal-hydraulic computational model of the MSFR that uses a reduced-order model to solve the neutronics equations. The principal computational tool chosen for this purpose is the high-fidelity code Cardinal, a wrapping within the MOOSE framework that integrates the Computational Fluid Dynamics code NekRS and the Monte Carlo particle transport code OpenMC. However, we use the Fission Matrix (FM) Method to solve the neutronics equations instead of OpenMC. The FM method can perform fast and still accurate neutronics simulations. It relies on precalculated databases obtained through a Monte Carlo simulation.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [57] [Suppression of Electromagnetic Pulses from Laser-Target Interactions by Strong Magnetic Fields](https://arxiv.org/abs/2602.18619)
*P. V. Heuer,J. L. Peebles,J. R. Davies,D. H. Barnak,B. Stanley,N. Pelepchan,M. Cufari,J. A. Frenje,C. Niemann,N. A. Rongione,C. Constantin,E. Cisneros,P. Pribyl,H. Sio,H. Chen*

Main category: physics.plasm-ph

TL;DR: Applying magnetic fields suppresses EMP for nanosecond laser pulses but enhances EMP for high-intensity picosecond pulses.


<details>
  <summary>Details</summary>
Motivation: EMP from laser-target interactions interferes with measurements and damages equipment, so finding mitigation methods is important.

Method: Conducted experiments with different geometries (spherical vs. planar), laser intensities (10¬π¬≥-10¬π‚Åπ W/cm¬≤), pulse durations (ns vs. ps), and magnetic field strengths (0.1-12 T), measuring EMP suppression/enhancement in ~1 GHz band.

Result: For ns pulses: EMP decreased by factors of 0.65√ó (12 T, 10¬π¬≥ W/cm¬≤) and 0.32√ó (0.1 T, 10¬π‚Åµ W/cm¬≤). For ps pulses at 10¬π‚Åπ W/cm¬≤: EMP increased by factor of 1.75√ó with 6-10 T fields.

Conclusion: Magnetic fields suppress EMP for ns pulses but enhance it for high-intensity ps pulses, making them unsuitable for EMP mitigation on high-intensity laser facilities where EMP is most damaging.

Abstract: Laser-target interactions generate intense electromagnetic pulses (EMP) that can interfere with measurements and damage equipment. In this paper we show that applying a magnetic field to nanosecond pulse laser-target interactions decreases the magnitude of EMP. We demonstrate this effect in two experiments with different geometries (spherical vs. planar), laser intensities (${\sim}10^{13}$ vs. ${\sim} 10^{15}$~W/cm$^2$) and applied field strength (12~T vs. 0.1~T) that both observed suppression of EMP in the ${\sim} 1$~GHz band (by factors of $0.65\times$ and $0.32\times$ respectively). We then observe the opposite effect at high intensities with a picosecond pulse: for planar experiments with laser intensities ${\sim}10^{19}$~W/cm$^2$ and magnetic fields of 6--10~T, the magnitude of EMP is increased by a factor of $1.75\times$. These results provide a benchmark for models of EMP generation, but suggest that magnetic fields are not a viable solution for mitigating EMP on the high intensity laser facilities where it is most damaging.

</details>


### [58] [A tutorial on inversion-based shape control with design application to NSTX-U](https://arxiv.org/abs/2602.18667)
*J. T. Wai,M. D. Boyer,D. J. Battaglia,F. Carpanese,F. Felici,W. P. Wehner,A. S. Welander,E. Kolemen*

Main category: physics.plasm-ph

TL;DR: The paper provides a systematic overview of inversion-based shape control (IBSC) for tokamak magnetic control, addressing variations like dynamic voltage mappings and constrained control, with specific application to NSTX-U's vertical control challenges.


<details>
  <summary>Details</summary>
Motivation: To comprehensively describe the IBSC framework and address less-known extensions and challenges, particularly the interaction between shape control and vertical control that degraded performance on NSTX-U.

Method: Systematic overview of IBSC framework including variations like dynamic voltage mappings and quadratic-program constrained control, with specific focus on decoupling shape and vertical control interactions.

Result: Application to NSTX-U shows vertical control bobble can be removed via decoupling, and vertical control phase margin can be improved by 6 degrees by including PF1A and PF2 as vertical actuators.

Conclusion: The paper provides a comprehensive systematic overview of IBSC with practical design procedures and tutorials, demonstrating successful application to NSTX-U's vertical control challenges.

Abstract: One of the most common designs for magnetic control in tokamaks is to ``linearize an equilibrium'' to obtain a sensitivity mapping, then invert this mapping in order to determine the feedback control currents or voltages. For this work, we refer to this broad class of methods as inversion-based shape control (IBSC). In this work we describe the IBSC framework in a comprehensive manner and show how variations such as using dynamic voltage mappings and quadratic-program constrained control fit naturally into the framework. Despite the prevalence of IBSC, some of these extensions and the challenges associated with specific variations of IBSC are less widely known. We pay special attention to the challenge of decoupling the interaction between shape control and vertical control, which was a source of degraded vertical control performance on NSTX-U. This work is intended to provide a systematic overview of IBSC, and to that end, we have provided background material, proposed design procedures, and tutorials on the magnetic control design process within the appendices. Applying the systematic design procedure to NSTX-U, we find that the vertical control bobble on NSTX-U can be removed via decoupling, and that the vertical control phase margin can be improved by 6 degrees just by including PF1A and PF2 as vertical actuators.

</details>


### [59] [How Does The Magnetic Gradient Scale Length Influence Complexity of Filamentary Coils in Stellarators?](https://arxiv.org/abs/2602.18974)
*John Kappel,Matt Landreman,Philipp Jura≈°iƒá,Sophia A Henneberg*

Main category: physics.plasm-ph

TL;DR: The paper shows that the minimum magnetic gradient scale length (min(L_gradB)) correlates with coil-surface and coil-coil distances in stellarator optimization, and improving min(L_gradB) can enhance particle confinement up to a point.


<details>
  <summary>Details</summary>
Motivation: To determine if min(L_gradB) is an effective proxy for coil-surface distance in filament coil optimizations and whether it serves as a useful objective function for improving stellarator engineering feasibility and particle confinement.

Method: 1) Analyzed QUASR dataset equilibria for correlation between min(L_gradB) and coil-surface distance; 2) Optimized quasihelically symmetric equilibria for improved min(L_gradB) and designed coils via continuation method; 3) Compared min(L_gradB) to coil distances for random boundary shapes with finite beta equilibria; 4) Conducted alpha particle tracing for confinement testing.

Result: min(L_gradB) correlates with both minimum coil-surface and coil-coil distances when sufficient coil length is allowed. Optimizing for improved min(L_gradB) can enhance particle confinement, but trade-offs exist with confinement proxies due to coil ripple effects.

Conclusion: min(L_gradB) is a valid proxy for coil-surface distance in filament coil optimizations and can be used as an effective objective function to improve stellarator engineering feasibility and particle confinement, though careful balance is needed to avoid coil ripple-induced losses.

Abstract: The distance between the last closed flux surface (LCFS) and the nearest electromagnetic coils is a dominating factor in the cost, size, and engineering difficulty of stellarators. The smallest magnetic gradient scale length on the LCFS - denoted L_gradB - has been shown to be a good proxy for minimum coil-surface distance in optimizations of a current potential on a winding surface, such as through the REGCOIL method. However, it has not been shown the same is true for filament coils, or that the magnetic gradient scale length is an effective objective function in optimization. In this paper, we explore examples in which min(L_gradB) is correlated with the minimum coil-surface distance for filament coils. First, we analyze a subset of the single-stage-optimized equilibria from the QUASR dataset [Giuliani et al. JPP (2024)]. We find that the majority of configurations have min(L_gradB) located nearby the point of closest coil-surface distance. Second, we optimize quasihelically symmetric equilibria to have improved min(L_gradB), and optimize coils via a continuation method. We then traced alpha particles to test confinement. Finally, we compare min(L_gradB) to the minimum coil-surface distance with filament coils optimized for a set of finite beta equilibria with random boundary shapes. For all datasets, we find that min(L_gradB) is correlated with both the minimum coil-surface and coil-coil distances if sufficient coil length is allowed. Even when there is a trade-off with proxies for confinement, optimizing for improved min(L_gradB) can result in better confinement in the presence of coils, up to a point. This is because - when holding coil-coil distance constant - equilibria with lower min(L_gradB) have a larger normal field error dominated by coil ripple causing particle loss. Both can be reduced by increasing coil-surface distance for equilibria with a high min(L_gradB).

</details>


### [60] [Machine learning prediction of plasma behavior from discharge configurations on WEST](https://arxiv.org/abs/2602.19110)
*Chenguang Wan,Feda Almuhisen,Philippe Moreau,Remy Nouailletas,Zhisong Qu,Youngwoo Cho,Robin Varennes,Kyungtak Lim,Kunpeng Li,Jia Huang,Weidong Chen,Jiangang Li,Xavier Garbet*

Main category: physics.plasm-ph

TL;DR: Transformer-based ML model predicts key plasma parameters in WEST tokamak using pre-discharge signals, achieving high accuracy and fast inference for scenario design and control.


<details>
  <summary>Details</summary>
Motivation: Physics-based modeling codes for plasma prediction are computationally expensive, limiting their use for fast scenario design and control optimization in tokamak experiments.

Method: Developed a transformer-based machine learning model that uses pre-discharge signals (magnetic coil currents, auxiliary heating power, plasma current reference, line-averaged density) to predict key plasma parameters including Œ≤n, Œ≤t, Œ≤p, Wmhd, q0, and q95.

Result: Model trained on 550 WEST discharges achieved average MSE loss of 0.026, average R¬≤ of 0.94, and inference times of ~0.1 seconds, demonstrating high accuracy and computational efficiency.

Conclusion: Data-driven surrogate models like the proposed transformer-based approach show strong potential for assisting in discharge planning, scenario evaluation, and real-time control of tokamak plasmas.

Abstract: Accurately predicting plasma behavior based on discharge configurations is essential for the safe and efficient operation of tokamak experiments. While physics-based integrated modeling codes provide valuable insights, their high computational cost limits their applicability for fast scenario design and control optimization. In this study, we propose a transformer-based machine learning model to predict key global plasma parameters on the WEST tokamak, including the normalized beta ($Œ≤_{n}$), toroidal beta ($Œ≤_{t}$), poloidal beta ($Œ≤_{p}$), plasma stored energy ($W_{\mathrm{mhd}}$), safety factor at the magnetic axis ($q_{0}$), and safety factor at the 95% flux surface ($q_{95}$). The model uses only signals that can be defined before the discharge, such as magnetic coil currents, auxiliary heating power, plasma current reference, and line-averaged plasma density. Trained on 550 discharges from the WEST campaigns, the model demonstrates an average mean square error (MSE) loss of 0.026, an average coefficient of determination $R^{2}$ of 0.94, and achieves inference times on the order of 0.1 seconds. These results highlight the potential of data-driven surrogate models for assisting in discharge planning, scenario evaluation, and real-time control of tokamak plasmas.

</details>


### [61] [Neoclassical transport and profile prediction in transport barriers](https://arxiv.org/abs/2602.19291)
*Silvia Trinczek,Felix I. Parra*

Main category: physics.plasm-ph

TL;DR: Extended neoclassical theory for tokamak pedestals shows particle-momentum coupling enables multiple transport solutions, potentially explaining H-L transitions.


<details>
  <summary>Details</summary>
Motivation: Standard neoclassical theory assumes gentle gradients, but tokamak pedestals have steep gradients (~ion poloidal gyroradius scale) where neoclassical transport dominates. Need theory valid for strong gradient regions.

Method: Extended neoclassical theory for large aspect ratio tokamaks, accounting for non-flux-function profiles (density, potential, flow, temperature) with poloidal variations. Includes nonlinear coupling with quasineutrality.

Result: Particle and momentum transport are coupled - parallel momentum sources can drive significant ion particle flux. Multiple co-existing solutions emerge from nonlinear transport equations, potentially corresponding to low/high transport states.

Conclusion: Extended theory reveals multiple transport solutions in pedestals; jumps between solutions could explain H-L back-transitions in tokamaks.

Abstract: Strong gradient regions in tokamaks, such as the pedestal or internal transport barriers, are regions of reduced turbulence where neoclassical transport can play a dominant role. However, standard neoclassical transport theory assumes that the gradient length scales of density, temperature, and potential are of the order of the system size. In the pedestal, gradient length scales are much shorter and are measured to be of the order of the ion poloidal gyroradius. We present an extension of neoclassical theory that is applicable in transport barriers of large aspect ratio tokamaks. We show that particle and momentum transport are connected in such a way that a source of parallel momentum can drive a significant neoclassical ion particle flux. In strong gradient regions, density, electric potential, mean parallel flow, and ion temperature are shown to no longer be flux functions. Instead, they have a small but important poloidally varying piece that modifies the transport equations to lowest order. This introduces a nonlinearity in the transport problem through the coupling with quasineutrality that yields multiple co-existing solutions when solving for the plasma profiles. The different solutions could be connected to low and high transport states and jumps between solutions could be an indication of H-L back-transitions.

</details>


### [62] [Three Dimensional Multiphysics Modelling of Helicon Wave Heating and Antenna Plasma Coupling for Boundary Density Control in Toroidal Fusion Plasmas](https://arxiv.org/abs/2602.19472)
*Hua Zhou,Lei Chang,GuoSheng Xu,YiWei Zhang,Matthew Hole,Dan Du,ZhiSong Qu,MuQuan Wu*

Main category: physics.plasm-ph

TL;DR: THEMIS code models 3D helicon wave propagation for fusion plasma density control, identifies electron Landau damping as dominant heating mechanism, and designs optimized racetrack antenna with 10x better coupling efficiency.


<details>
  <summary>Details</summary>
Motivation: Active control of scrape-off layer (SOL) density is critical for improving ion cyclotron resonance heating and enabling high-performance steady-state operation in future fusion devices. Helicon waves offer promising physics-based approach for high-density boundary plasmas with high ionization efficiency and low impurity release.

Method: Developed THEMIS code - a fully 3D multiphysics model of helicon wave propagation and power deposition using finite temperature thermal dielectric tensor. Analyzed four planar antenna geometries, introduced recessed window launch scheme, performed systematic parameter scans of window position, antenna geometry, and installation orientation.

Result: Slow wave propagation and electron Landau damping dominate heating in Helimak device. Geometric cutoffs and SOL density gradients limit power penetration. Identified key principles for efficient coupling: open circuit termination, maximized strap length/width, controlled inter-turn spacing, sufficient clearance from walls. Designed optimized racetrack spiral antenna achieving >10x coupling efficiency improvement.

Conclusion: THEMIS code successfully models helicon wave physics for fusion applications. Recessed window scheme and optimized racetrack antenna design significantly improve coupling efficiency, addressing critical constraints for practical implementation in magnetic confinement fusion devices.

Abstract: Active control of scrape off layer density is emerging as a critical requirement for improving ion cyclotron resonance heating and enabling high performance steady state operation in future magnetic confinement fusion devices. Helicon wave excitation offers a promising physics based approach to generating high density boundary plasmas with high ionization efficiency and low impurity release. In this work, we develop THEMIS code, a fully three dimensional (3D) multiphysics model of helicon wave propagation and power deposition in a toroidal fusion relevant configuration, employing a finite temperature thermal dielectric tensor. The code quantifies the relative contributions of Doppler shifted cyclotron damping, anomalous Doppler damping, collisional damping, and Landau damping, and demonstrates that slow wave propagation and electron Landau damping dominate the accessible heating regime in Helimak device. A comparative study of four planar antenna geometries under the present protruding window configuration shows that geometric cutoffs and SOL density gradients severely limit power penetration into the core accessible region. To address this constraint, we introduce a recessed window launch scheme that positions the dielectric window inside the vacuum vessel and perform systematic parameter scans of window position, antenna geometry, and installation orientation. From these analyses, we identify the key physics driven principles governing efficient helicon wave coupling: the importance of open circuit termination, maximized strap length and width, controlled inter turn spacing, and sufficient clearance from metallic walls to avoid near field suppression. Guided by these principles, we designed an optimized racetrack spiral antenna that increases coupling efficiency by more than an order of magnitude compared with conventional short circuited rectangular spiral antenna.

</details>


### [63] [Friction-induced scale-selection in the extended Cahn-Hilliard model for zonal staircase](https://arxiv.org/abs/2602.19527)
*M. Leconte,T. S. Hahm*

Main category: physics.plasm-ph

TL;DR: The paper proposes a mechanism to determine the radial scale of zonal flows in plasma turbulence, specifically for E√óB staircases, using an extended Cahn-Hilliard model with zonal flow friction.


<details>
  <summary>Details</summary>
Motivation: To understand and predict the radial scale (step-size) of zonal flows observed in plasma turbulence simulations, particularly the E√óB staircase phenomenon found in global full-f simulations.

Method: Uses 1D numerical simulations of an extended Cahn-Hilliard model that includes zonal flow friction, combined with heuristic nonlinear analysis.

Result: The staircase step-size Œî decreases with increasing dimensionless zonal flow friction Œº, following a scaling relation Œî ‚àº log Œº^Œ± with Œ± ‚âà -0.41, plus a constant offset.

Conclusion: The proposed mechanism provides a theoretical framework to predict the radial scale of zonal flows in plasma turbulence, with the step-size showing logarithmic dependence on zonal flow friction.

Abstract: In this work, we describe a possible mechanism to set the radial scale of zonal flows, which may be applicable to the $E \times B$ staircase found in the global full-f simulations such as [G. Dif-Pradalier et al. Phys. Rev. Lett. 114, 085004 (2015)]. 1D numerical simulation results of the Cahn-Hilliard model - extended to include zonal flow friction - can be understood from a heuristic nonlinear analysis. The staircase step-size $Œî$ is found to decrease as the dimensionless zonal flow friction $Œº$ increases. It scales like $Œî\sim \log Œº^Œ±$, with $Œ±\simeq -0.41$, up to a constant offset.

</details>


### [64] [Gyrokinetic simulation of the effect of transient fueling on plasma turbulence in ADITYA-U tokamak](https://arxiv.org/abs/2602.19676)
*Jaya Kumar Alageshan,Suman Dolui,Joydeep Ghosh,Kishore Mishra,Sarveshwar Sharma,Abhijit Sen,Manjunatha Valmiki,Sandeep Agrawal,Sanjay Wandhekar,Zhihong Lin,Animesh Kuley*

Main category: physics.plasm-ph

TL;DR: Gas puffs suppress microturbulence in ADITYA-U tokamak, improving core temperature and energy confinement by flattening density profile and suppressing TEM turbulence.


<details>
  <summary>Details</summary>
Motivation: To develop an active control mechanism for microturbulence in tokamak plasmas to improve energy confinement time and core temperature.

Method: Injecting short gas puffs to modify radial density profile, combined with global electrostatic gyrokinetic simulations to analyze trapped electron mode (TEM) suppression.

Result: Gas puffs suppress TEM-dominated microturbulence, reduce turbulence-driven heat transport, increase core temperature, and improve overall energy confinement time.

Conclusion: Periodic gas puff injection serves as an effective active control mechanism for suppressing microturbulence and improving plasma confinement in tokamaks.

Abstract: The gradient-driven microturbulence in ADITYA-U tokamak plasmas has been suppressed by injecting short gas puffs. The suppression of microturbulence increases the core temperature and subsequently the energy confinement time following the gas puff. The gas injection modifies the radial density profile, making it relatively flatter near the mid-radius. Global electrostatic gyrokinetic simulations show that this modification to the radial density profile due to gas injection suppresses the existing trapped electron mode (TEM). Simulation results show that the TEM-dominated turbulence suppression reduces the turbulence-driven heat transport, leading to an increase in core temperature. Applying multiple periodic gas-puffs leads to multiple periodic events of TEM suppression, improving the overall energy confinement time, and is used as an active control mechanism to influence microturbulence in ADITYA-U tokamak.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [65] [peapods: A Rust-Accelerated Monte Carlo Package for Ising Spin Systems](https://arxiv.org/abs/2602.19045)
*Yan Ru Pei*

Main category: cond-mat.stat-mech

TL;DR: peapods is an open-source Python package for Monte Carlo simulation of Ising spin systems with arbitrary couplings on hypercubic lattices, combining Python interface with Rust performance.


<details>
  <summary>Details</summary>
Motivation: To provide a high-performance, memory-safe Monte Carlo simulation tool for Ising spin systems that supports arbitrary coupling constants and dimensions while offering an ergonomic Python interface.

Method: Package written in Rust for performance and memory safety, exposed to Python via PyO3. Implements Metropolis, Gibbs single-spin-flip, Swendsen-Wang and Wolff cluster updates, and parallel tempering. Uses Rayon work-stealing scheduler for replica-level parallelism.

Result: Successfully validated implementation against exact critical temperature of 2D Ising model using finite-size scaling of Binder cumulant.

Conclusion: peapods provides a robust, high-performance Monte Carlo simulation package for Ising spin systems that bridges Python usability with Rust performance, validated against known theoretical results.

Abstract: We present peapods (github.com/PeaBrane/peapods), an open-source Python package for Monte Carlo simulation of Ising spin systems with arbitrary coupling constants on arbitrary-dimensional hypercubic lattices with periodic boundary conditions. The computational core is written in Rust and exposed to Python via PyO3, combining the ergonomic interface of Python with the performance of compiled, memory-safe code. The package implements Metropolis and Gibbs single-spin-flip algorithms, Swendsen--Wang and Wolff cluster updates, and parallel tempering. Replica-level parallelism is achieved through the Rayon work-stealing scheduler. We validate the implementation against the exact critical temperature of the two-dimensional Ising model via finite-size scaling of the Binder cumulant.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [66] [Fast reconnection in a coronal torn plasma sheet](https://arxiv.org/abs/2602.18706)
*Zehao Tang*

Main category: astro-ph.SR

TL;DR: Observational study of coronal plasma sheet tearing instability reveals plasmoids as key carriers for fast magnetic flux transfer and reconnection rate enhancement.


<details>
  <summary>Details</summary>
Motivation: Tearing instability (plasmoid instability) is crucial for accelerating magnetic reconnection across various plasma systems, but observational evidence is scarce, hindering understanding of plasmoids' role in reconnection processes.

Method: Used high-spatiotemporal multiwavelength observations from Solar Dynamics Observatory to trace entire evolution of a coronal plasma sheet, analyzing heating processes, plasmoid dynamics, and reconnection rate changes.

Result: Identified two-stage plasma sheet evolution with different tearing frequencies, discovered two key heating processes (plasma sheet tearing and plasmoid coalescence), and found plasmoids are key carriers for fast magnetic flux transfer that significantly enhance reconnection rates.

Conclusion: Plasmoid formation and ejection play critical roles in facilitating fast magnetic reconnection by enhancing reconnection rates and enabling efficient magnetic flux transfer in torn plasma sheets.

Abstract: Tearing instability, also known as plasmoid instability, is an effective mechanism to speed up magnetic reconnection process, working in a wide range of magnetized plasma systems with different spatial scales, ionization degrees, and collisionality. However, due to observational limitations, observations of {plasma sheet} tearing and the resulting plasmoids are rather scarce. This scarcity significantly hinders our understanding of the role of plasmoids in the reconnection process from an observational perspective. Using high-spatiotemporal multiwavelength observations from the Solar Dynamics Observatory, we traced the entire evolution of a coronal {plasma sheet}. Its formation was driven by the emergence of photospheric magnetic flux, followed by tearing, and eventual decay. The evolution of the {plasma sheet} exhibited two distinct stages. Initially, it rose rapidly, lengthened, and underwent tearing at a low frequency. Subsequently, its ascent slowed, it began to shorten, and the tearing occurred more frequently. Detailed analysis of the reconnecting {plasma sheet} focuses on heating, plasmoid dynamics (formation and ejection), and the resulting reconnection rate change. Two key heating processes are identified: {plasma sheet} tearing and coalescence involving plasmoids and magnetic cusps. More importantly, combining observations with analytical studies suggests that plasmoids are key carriers of magnetic flux fast transferring in the observed torn {plasma sheet}, and their formation and ejection significantly enhance the reconnection rate and facilitate the onset of fast reconnection.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [67] [Energy gap of quantum spin glasses: a projection quantum Monte Carlo study](https://arxiv.org/abs/2602.20108)
*L. Brodoloni,G. E. Astrakharchik,S. Giorgini,S. Pilati*

Main category: cond-mat.dis-nn

TL;DR: The paper investigates how quantum annealing performance scales with system size for two spin-glass models, finding fundamentally different behaviors: 2D Edwards-Anderson shows super-algebraic scaling with fat-tailed gap distributions, while Sherrington-Kirkpatrick shows more favorable power-law scaling.


<details>
  <summary>Details</summary>
Motivation: Quantum annealing performance for combinatorial optimization is limited by the minimum energy gap at quantum phase transitions. Understanding how this gap scales with system size for different problem classes is crucial for assessing the practical potential of quantum annealers.

Method: Used a newly proposed unbiased energy-gap estimator for continuous-time projection quantum Monte Carlo simulations, complemented by high-performance sparse eigenvalue solvers. Analyzed gap distributions across disorder realizations for both 2D Edwards-Anderson and Sherrington-Kirkpatrick spin-glass models.

Result: For 2D-EA, the inverse-gap distribution develops a fat tail with infinite variance as system size increases, indicating unfavorable super-algebraic scaling that persists for Gaussian disorder. For SK model, the distribution retains finite variance with disorder-averaged gap following Œî ‚àù N^{-1/3} power law.

Conclusion: 2D spin glasses exhibit universal unfavorable scaling for quantum annealing, while the SK model's more favorable power-law scaling provides promising outlook for quantum annealers on optimization problems with dense connectivity.

Abstract: The performance of quantum annealing for combinatorial optimization is fundamentally limited by the minimum energy gap $Œî$ encountered at quantum phase transitions. We investigate the scaling of $Œî$ with system size $N$ for two paradigmatic quantum spin-glass models: the two-dimensional Edwards-Anderson (2D-EA) and the all-to-all Sherrington-Kirkpatrick (SK) models. Utilizing a newly proposed unbiased energy-gap estimator for continuous-time projection quantum Monte Carlo simulations, complemented by high-performance sparse eigenvalue solvers, we characterize the gap distributions across disorder realizations. It is found that, in the 2D-EA case, the inverse-gap distribution develops a fat tail with infinite variance as $N$ increases. This indicates that the unfavorable super-algebraic scaling of $Œî$, recently reported for binary couplings [Nature 631, 749 (2024)], persists for the Gaussian disorder considered here, pointing to a universal feature of 2D spin glasses. Conversely, the SK model retains a finite-variance distribution, with the disorder-averaged gap following a rather slow power law, close to $Œî\propto N^{-1/3}$. This finding provides a promising outlook for the potential efficiency of quantum annealers for optimization problems with dense connectivity.

</details>


<div id='nlin.SI'></div>

# nlin.SI [[Back]](#toc)

### [68] [Existence of Riemannian invariants for integrable systems of hydrodynamic type](https://arxiv.org/abs/2602.18687)
*Alexey V. Bolsinov,Andrey Yu. Konyaev,Vladimir S. Matveev*

Main category: nlin.SI

TL;DR: Hyperbolic hydrodynamic systems with n symmetries can be diagonalized in a special coordinate system.


<details>
  <summary>Details</summary>
Motivation: To establish that hyperbolic systems of hydrodynamic type possessing multiple symmetries have a special structural property - diagonalizability of both the system generator and all symmetries.

Method: Theoretical analysis showing existence of a coordinate transformation that simultaneously diagonalizes the hyperbolic system and all its n symmetries.

Result: Demonstrates that for such symmetric hyperbolic hydrodynamic systems, there exists a coordinate system where both the system generator and all symmetries become diagonal.

Conclusion: Hyperbolic hydrodynamic systems with multiple symmetries possess a fundamental diagonalization property, revealing their underlying mathematical structure.

Abstract: We show that for a hyperbolic system of hydrodynamic type admitting n symmetries, there exists a coordinate system in which the generator of the system and all the symmetries are diagonal.

</details>


<div id='math.MG'></div>

# math.MG [[Back]](#toc)

### [69] [Stability of optimal transport on metric measure spaces](https://arxiv.org/abs/2602.19175)
*Bang-Xian Han,Zhuo-Nan Zhu*

Main category: math.MG

TL;DR: Quantitative stability of Kantorovich potentials on metric measure spaces with lower Ricci curvature bound, confirming a recent conjecture.


<details>
  <summary>Details</summary>
Motivation: To prove quantitative stability of Kantorovich potentials on spaces with lower Ricci curvature bounds, confirming a recent conjecture by Kitagawa, Letrouit and M√©rigot. This extends stability results beyond settings requiring linear structure or sectional curvature bounds.

Method: Uses heat kernel-regularized c-transform approach. The method avoids reliance on linear structure or sectional curvature bounds, making it applicable to more general metric measure spaces.

Result: Proves quantitative stability of Kantorovich potentials on metric measure spaces with lower Ricci curvature bound. As a corollary, obtains quantitative stability of optimal transport maps on Alexandrov spaces with lower curvature bound.

Conclusion: Confirms the conjecture of Kitagawa, Letrouit and M√©rigot about quantitative stability of Kantorovich potentials. The heat kernel-regularized c-transform method provides a general approach that works without linear structure or sectional curvature assumptions.

Abstract: We prove a quantitative stability of Kantorovich potentials on metric measure spaces with lower Ricci curvature bound, thereby confirming a recent conjecture of Kitagawa, Letrouit and M√©rigot. Our proof, which employs the heat kernel-regularized $c$-transform, does not rely on linear structure or sectional curvature bounds. As a corollary, we get a quantitative stability of optimal transport maps on Alexandrov spaces with lower curvature bound.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [70] [On the importance of stochasticity in closures of turbulence](https://arxiv.org/abs/2602.19875)
*Andr√© Freitas,Luca Biferale,Mathieu Desbrun,Gregory Eyink,Alexei A. Mailybaev,Kiwon Um*

Main category: physics.flu-dyn

TL;DR: Stochastic closures restore correct uncertainty growth in turbulent models where deterministic closures fail.


<details>
  <summary>Details</summary>
Motivation: Deterministic closures for coarse-grained turbulence models reproduce mean statistics but fail to capture finite-time uncertainty growth, which is crucial for predictability in turbulent systems.

Method: Using shell models as a multi-scale testbed, compare fully resolved simulations with large-eddy simulations using either stochastic or deterministic subgrid closures. Introduce uncertainty through initial condition perturbations and analyze variance growth across scales.

Result: Truncation with deterministic closures delays and suppresses variance growth, while a data-driven Langevin-type stochastic closure restores correct timing and magnitude of uncertainty growth across scales.

Conclusion: Sustained stochasticity is essential for predictability in reduced turbulent dynamics, as stochastic closures can properly capture uncertainty growth that deterministic closures miss.

Abstract: Deterministic closures for coarse-grained turbulence models help reproduce mean statistics, but often fail to capture the finite-time growth of uncertainty. Using the framework of shell models as a quantitative multi-scale testbed, we compare fully resolved simulations with large-eddy simulations using either stochastic or deterministic subgrid closures. While in the fully resolved system a single microscopic perturbation is rapidly amplified by strongly chaotic dynamics, truncation produces a strong delay and suppression of variance growth when uncertainty is introduced through initial condition perturbations only. We show that a data-driven Langevin-type stochastic closure restores the correct timing and magnitude of variance growth across scales, demonstrating that sustained stochasticity is essential for predictability in reduced turbulent dynamics.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [71] [The effect of the A-site cation on the phase transition temperature of metal halide perovskites](https://arxiv.org/abs/2602.20058)
*Tom Braeckevelt,Sander Vandenhaute,Sven M. J. Rogge,Johan Hofkens,Veronique Van Speybroeck*

Main category: cond-mat.mtrl-sci

TL;DR: The paper presents a multistep thermodynamic integration approach with machine learning potentials to accurately compute Gibbs free energies for metal halide perovskites, revealing that phase stability is governed by ground-state energy differences rather than thermal effects.


<details>
  <summary>Details</summary>
Motivation: Metal halide perovskites suffer from instability where the desired perovskite phase converts to optically non-active Œ¥ phase. Existing harmonic free energy methods fail for organic cation perovskites due to rotational freedom, requiring more accurate Gibbs free energy calculations.

Method: Developed a multistep thermodynamic integration approach to correct harmonic free energy to Gibbs free energy. Used replica exchange to avoid local minima traps and intermediate potential surfaces for convergence. Benchmarked DFT functionals against RPA+HF, selected PBE+D3(BJ). Trained MACE machine learning potential on DFT data for molecular dynamics simulations within TI framework.

Result: For three materials studied, the free energy difference between Œ≥ and Œ¥ phases shows very similar temperature dependence, indicating phase stability is primarily determined by ground-state energy differences rather than material-specific thermal effects.

Conclusion: The methodology provides a robust framework for investigating phase behavior of metal halide perovskites, enabling discovery of more stable perovskites by focusing on ground-state energy optimization rather than thermal effects.

Abstract: A key challenge for the practical application of metal halide perovskites (MHPs) is the instability of the desired perovskite phase relative to the optically non-active $Œ¥$ phase. To determine the phase stability, we previously developed a procedure to compute the harmonic free energy as a function of temperature, which was suited for CsPbI$_3$ but fails when Cs is replaced by organic cations due to their rotational freedom. Herein we propose a multistep thermodynamic integration (TI) approach that corrects the harmonic free energy to obtain the Gibbs free energy. Given the abundance of local minima in these materials, we employ replica exchange to prevent simulations from getting trapped, while introducing an intermediate potential energy surface to improve convergence and reduce computational cost. Benchmarking energy and forces from different exchange-correlation functionals and dispersion methods against high-level RPA+HF calculations identifies PBE+D3(BJ) as the best trade-off between accuracy, computational efficiency, and precision. To perform molecular dynamics simulations within the TI framework, it was necessary to train a machine learning potential using the MACE architecture on ab initio data calculated with density functional theory. Our results show that, for all three materials, the free energy difference between the $Œ≥$ and $Œ¥$ phases exhibits a very similar temperature dependence. This suggests that phase stability is primarily governed by differences in ground-state energy, rather than by material-specific thermal effects. Beyond these three materials, our methodology provides a robust framework for investigating the phase behavior of other MHPs, paving the way for the discovery of more stable perovskites.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [72] [Scale-PINN: Learning Efficient Physics-Informed Neural Networks Through Sequential Correction](https://arxiv.org/abs/2602.19475)
*Pao-Hsiung Chiu,Jian Cheng Wong,Chin Chun Ooi,Chang Wei,Yuchen Fan,Yew-Soon Ong*

Main category: cs.CE

TL;DR: Scale-PINN introduces a sequential correction algorithm that integrates iterative residual-correction principles from numerical solvers into PINN loss formulation, dramatically improving training speed and accuracy for solving PDEs.


<details>
  <summary>Details</summary>
Motivation: PINNs have shown promise for solving PDEs but face limitations in slow training and modest accuracy compared to modern numerical solvers, hindering their practical adoption in science and engineering applications.

Method: Scale-PINN incorporates the iterative residual-correction principle from numerical solvers directly into the PINN loss formulation, creating a sequential correction algorithm that bridges physics-informed learning with numerical algorithms.

Result: Scale-PINN achieves unprecedented convergence speed across various PDE problems, reducing training time on challenging fluid-dynamics problems from hours to sub-2 minutes while maintaining superior accuracy, enabling application to aerodynamics and urban science problems.

Conclusion: By uniting numerical methods with deep learning, Scale-PINN represents a significant leap toward practical adoption of PINNs in science and engineering through scalable, physics-informed learning.

Abstract: Physics-informed neural networks (PINNs) have emerged as a promising mesh-free paradigm for solving partial differential equations, yet adoption in science and engineering is limited by slow training and modest accuracy relative to modern numerical solvers. We introduce the Sequential Correction Algorithm for Learning Efficient PINN (Scale-PINN), a learning strategy that bridges modern physics-informed learning with numerical algorithms. Scale-PINN incorporates the iterative residual-correction principle, a cornerstone of numerical solvers, directly into the loss formulation, marking a paradigm shift in how PINN losses can be conceived and constructed. This integration enables Scale-PINN to achieve unprecedented convergence speed across PDE problems from different physics domain, including reducing training time on a challenging fluid-dynamics problem for state-of-the-art PINN from hours to sub-2 minutes while maintaining superior accuracy, and enabling application to representative problems in aerodynamics and urban science. By uniting the rigor of numerical methods with the flexibility of deep learning, Scale-PINN marks a significant leap toward the practical adoption of PINNs in science and engineering through scalable, physics-informed learning. Codes are available at https://github.com/chiuph/SCALE-PINN.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [73] [Thin Plate Spline Surface Reconstruction via the Method of Matched Sections](https://arxiv.org/abs/2602.19182)
*Igor Orynyak,Kirill Danylenko,Danylo Tavrov*

Main category: cs.GR

TL;DR: The paper advances the Method of Matched Sections (MMS) as a physics-informed geometric tool for solving PDE-based boundary value problems in surface modeling, bridging isogeometric analysis and computer graphics.


<details>
  <summary>Details</summary>
Motivation: To address critical challenges in isogeometric analysis and computer graphics by bridging the gap between physical accuracy and geometric continuity for surface modeling applications.

Method: Decomposes the domain into an assembly of 1D directional components matched along their entire boundaries, inherently enforcing continuity of all variational parameters including second-order (curvature) and third-order (shear) derivatives.

Result: Demonstrates advanced capabilities in high-fidelity surface reconstruction and blending, consistently generating energetically optimal, fair surfaces even from complex boundary conditions or sparse internal points.

Conclusion: Establishes MMS as a powerful, physics-informed geometric tool that satisfies both rigorous numerical analysis and aesthetic computer-aided design requirements.

Abstract: This paper further develops the Method of Matched Sections (MMS), a robust numerical framework for the solution of boundary value problems governed by partial differential equations, specifically for surface modeling. While originating in mechanics, the method addresses critical challenges in isogeometric analysis and computer graphics by bridging the gap between physical accuracy and geometric continuity. By decomposing the domain into an assembly of 1D directional components matched along their entire boundaries, the method inherently enforces the continuity of all variational parameters, including second-order (curvature) and third-order (shear) derivatives. We demonstrate the method's advanced capabilities in high-fidelity surface reconstruction and blending, showing that it consistently generates energetically optimal, fair surfaces even from complex boundary conditions or sparse internal points. By advancing the application of MMS, this research establishes it as a powerful, physics-informed geometric tool that satisfies the dual demands of rigorous numerical analysis and aesthetic computer-aided design.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [74] [The Role of Inhomogeneities in the Turbulent Accretion of Black Holes](https://arxiv.org/abs/2602.19104)
*Giuseppe Ficarra,Michele Arcuri,Rita Megale,Sergio Servidio*

Main category: gr-qc

TL;DR: High-resolution GRMHD simulations show that density/magnetic perturbations in black hole accretion flows create steeper power spectra and larger coherent structures, affecting accretion rates and providing insights for interpreting Event Horizon Telescope observations.


<details>
  <summary>Details</summary>
Motivation: Event Horizon Telescope observations reveal inhomogeneities around supermassive black holes, likely from density and magnetic field perturbations. Understanding how these perturbations affect accretion dynamics is crucial for interpreting observational data.

Method: Conducted high-resolution 2D GRMHD simulations using BHAC code, comparing unperturbed accretion with cases featuring plasma density bubbles and pressure-balanced magnetic islands of different amplitudes. Used Blackman-Tukey method for power spectrum analysis and spatial auto-correlation analysis of near-horizon turbulence.

Result: Perturbed case shows: (1) steeper spectral indices compared to unperturbed case, deviating from characteristic 1/œâ noise spectrum; (2) increased correlation times, indicating absorption of macro-structures at event horizon; (3) larger energy-containing coherent structures in near-horizon turbulence altering accretion rate.

Conclusion: Near-horizon turbulence plays a key role in accretion processes, with perturbations creating larger coherent structures that affect accretion dynamics. These findings provide new insights for interpreting supermassive black hole observations, particularly from the Event Horizon Telescope.

Abstract: Observations of supermassive black holes by the Event Horizon Telescope reveal significant inhomogeneities, most likely related to density and magnetic field perturbations. To model these features, we conduct high-resolution 2D general-relativistic magnetohydrodynamics (GRMHD) simulations of a Fishbone-Moncrief torus around a Kerr black hole using the Black Hole Accretion Code $\texttt{BHAC}$. We compare unperturbed accretion with a case featuring plasma density bubbles with pressure balanced magnetic islands of different amplitudes. Power spectrum analysis of accretion time series, performed via the Blackman-Tukey method, shows that the perturbed case exhibits (1) steeper spectral indices compared to the unperturbed case, deviating from the characteristic $1/œâ$ noise spectrum, and (2) increased correlation times, providing evidence for absorption of macro-structures at the event horizon. Spatial auto-correlation analysis of near-horizon turbulence confirms larger energy-containing coherent structures in the perturbed case altering the accretion rate. These results provide new insights for interpreting observations of supermassive black hole environments, where near-horizon turbulence may play a key role in the accretion process.

</details>


### [75] [The extremely-tilted fluid regime near asymptotically Kasner big bang singularities](https://arxiv.org/abs/2602.19361)
*Florian Beyer*

Main category: gr-qc

TL;DR: The paper solves relativistic Euler equations with linear barotropic EOS on Kasner big bang spacetimes, extending previous work to the extremely-tilted regime where sound speed is small, proving global existence and asymptotic behavior toward the singularity.


<details>
  <summary>Details</summary>
Motivation: To extend the analysis of relativistic Euler equations on Kasner big bang spacetimes from the asymptotically non-tilted regime (large sound speed) to the asymptotically extremely-tilted regime (small sound speed), addressing the behavior of fluid solutions as they approach the big bang singularity.

Method: The authors solve the Cauchy problem for relativistic Euler equations with linear barotropic equation of state on Kasner background spacetimes. They prove global existence of solutions toward the big bang singularity without symmetry assumptions or smallness conditions on initial data, requiring only sufficiently large mean curvature of the initial hypersurface.

Result: The solutions exist globally in time toward the big bang singularity, and exhibit expected asymptotic behavior: fluid particles are driven toward the speed of light in the direction of the largest Kasner exponent as the singularity is approached.

Conclusion: The paper successfully extends previous results to the extremely-tilted regime, providing rigorous mathematical foundation for the asymptotic behavior of relativistic fluids near Kasner big bang singularities, confirming heuristic expectations about fluid particle acceleration toward light speed in the direction of maximal expansion.

Abstract: In this paper, we solve the relativistic Euler equations with a linear barotropic equation of state on a large class of background spacetimes with Kasner big bang asymptotics. Building on previous work in the asymptotically non-tilted regime, which applies when the speed of sound of the fluid is large in comparison to the Kasner exponents, we now consider the asymptotically extremely-tilted regime when the speed of sound is small. We solve the Cauchy problem for the Euler equations towards the big bang singularity and prove, without any symmetry assumptions or smallness conditions on the Cauchy data, that the solutions exist globally in time provided the mean curvature of the initial hypersurface is sufficiently large. Finally, we prove that the solutions exhibit the asymptotics expected from standard heuristic arguments in this regime; in particular, fluid particles are driven towards the speed of light in the direction of the largest Kasner exponent as the big bang singularity is approached.

</details>


### [76] [The Spacetime Positive Mass Theorem with Multiple Time Dimensions](https://arxiv.org/abs/2602.20081)
*Sven Hirsch,Alec Payne,Yiyue Zhang*

Main category: gr-qc

TL;DR: Generalizes spacetime positive mass theorem to multiple time dimensions, showing energy is bounded below by trace norm of linear momenta, with equality implying foliation by flat submanifolds.


<details>
  <summary>Details</summary>
Motivation: To extend the classical positive mass theorem (which applies to spacetimes with one time dimension) to spacetimes with multiple time dimensions, addressing the behavior of mass/energy in such generalized spacetime settings.

Method: Generalizes the spacetime positive mass theorem framework to accommodate multiple time dimensions, establishing energy inequalities and analyzing conditions for equality.

Result: Proves that energy E is bounded from below by the trace norm of linear momenta J^1,...,J^m. Shows equality implies foliation by flat submanifolds, and with additional umbilicity assumption, initial data set isometrically embeds into generalized pp-wave.

Conclusion: Successfully extends positive mass theorem to multiple time dimensions, establishing fundamental energy inequalities and characterizing the geometric structure when equality holds.

Abstract: We generalize the spacetime positive mass theorem to include multiple time dimensions. In particular, we show that the mass remains nonnegative in the sense that the energy $E$ is bounded from below by the trace norm of the linear momenta $J^1,...,J^m$. Equality in this energy inequality implies a foliation by flat submanifolds of a generalized initial data set. Moreover, under an additional umbilicity assumption, we find that the initial data set isometrically embeds into a generalized pp-wave.

</details>


### [77] [Spherically symmetric solutions to the Einstein-scalar field conformal constraint equations](https://arxiv.org/abs/2602.20099)
*Philippe Castillon,The-Cang Nguyen*

Main category: gr-qc

TL;DR: The paper studies Einstein-scalar field conformal constraint equations under harmonic manifold and radial data assumptions, reducing the system to a single nonlinear equation. Solutions show contrasting behavior on spheres (nonexistence in near-CMC regime, instability) versus Euclidean/hyperbolic manifolds (always solvable). The paper also examines mass sign and provides explicit solution classes.


<details>
  <summary>Details</summary>
Motivation: To gain clearer understanding of the complex Einstein-scalar field conformal constraint equations by studying them under special assumptions (harmonic manifolds with radial data), since previous work showed these equations are generally intractable even in vacuum case.

Method: Study the constraint equations under special assumptions: manifold (M,g) is harmonic and data is radial. This reduces the system to a single nonlinear equation. Analyze solutions on different manifolds (sphere, Euclidean, hyperbolic) and investigate mass sign properties.

Result: On spheres: nonexistence of solutions in near-CMC regime and instability when mean curvature is non-constant. On Euclidean/hyperbolic manifolds: equations always solvable with expected properties. ADM and asymptotically hyperbolic mass can take arbitrary sign with critical decay rate of tensor k. Most solution classes are explicit.

Conclusion: The conformal method has drawbacks on compact manifolds but remains promising for parametrizing solutions on asymptotically flat and hyperbolic manifolds in arbitrary mean curvature regimes. Explicit solutions provide models for general relativity and insights for numerical applications.

Abstract: Recent works by the second author and Dilts et al. have shown that the Einstein-scalar field conformal constraint equations are highly complex and generally intractable, even in the vacuum case. In this article, to gain a clearer understanding and offer a new perspective, we study these equations under special assumptions: the manifold $(M,g)$ is harmonic and data is radial.
  In this setting, the system reduces to a single nonlinear equation and is completely resolved in the standard cases. In particular, on the sphere, our results reveal phenomena that contrast with the well-known achievements on compact manifolds without conformal Killing vector fields, including nonexistence of solutions in the near-CMC regime and instability when the mean curvature is non-constant. By contrast, on Euclidean or hyperbolic manifolds, the equations are always solvable, with all expected properties of solutions satisfied. These findings support the view that, although the conformal method appears to present some drawbacks on compact manifolds, it remains a promising tool for parametrizing solutions to the constraint equations on asymptotically flat and hyperbolic manifolds in arbitrary mean curvature regimes.
  In this article, we also investigate the sign of mass, showing that the ADM and asymptotically hyperbolic mass can take arbitrary sign when the decay rate of symmetric $(0,2)$-tensor $k$ at infinity is critical. Finally, most solution classes in our framework are explicit, providing a variety of models in general relativity and offering insights into the structure and behavior of initial data, particularly in numerical applications.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [78] [Exact Discrete Stochastic Simulation with Deep-Learning-Scale Gradient Optimization](https://arxiv.org/abs/2602.19775)
*Jose M. G. Vilar,Leonor Saiz*

Main category: q-bio.QM

TL;DR: A new method enables exact stochastic simulation of continuous-time Markov chains with gradient-based learning by decoupling forward simulation from backward differentiation using Gumbel-Softmax straight-through surrogate.


<details>
  <summary>Details</summary>
Motivation: Exact stochastic simulation of CTMCs is crucial when discreteness and noise matter, but traditional Gillespie-type algorithms use hard categorical event selection that blocks gradient-based learning, limiting parameter optimization and inference.

Method: Decouple forward simulation from backward differentiation: use hard categorical sampling for exact trajectory generation, while gradients propagate through a continuous massively-parallel Gumbel-Softmax straight-through surrogate.

Result: Achieves accurate optimization at parameter scales over four orders of magnitude beyond existing simulators, with validation showing 0.09% error on dimerization model, 1.2% error on genetic oscillator, 98.4% MNIST accuracy on 203,796-parameter gene regulatory network, and R^2 = 0.987 on ion channel gating. GPU implementation reaches 1.9 billion steps per second.

Conclusion: The approach makes exact stochastic simulation massively parallel and autodiff-compatible, enabling high-dimensional parameter inference and inverse design across systems biology, chemical kinetics, physics, and other CTMC-governed domains.

Abstract: Exact stochastic simulation of continuous-time Markov chains (CTMCs) is essential when discreteness and noise drive system behavior, but the hard categorical event selection in Gillespie-type algorithms blocks gradient-based learning. We eliminate this constraint by decoupling forward simulation from backward differentiation, with hard categorical sampling generating exact trajectories and gradients propagating through a continuous massively-parallel Gumbel-Softmax straight-through surrogate. Our approach enables accurate optimization at parameter scales over four orders of magnitude beyond existing simulators. We validate for accuracy, scalability, and reliability on a reversible dimerization model (0.09% error), a genetic oscillator (1.2% error), a 203,796-parameter gene regulatory network achieving 98.4% MNIST accuracy (a prototypical deep-learning multilayer perceptron benchmark), and experimental patch-clamp recordings of ion channel gating (R^2 = 0.987) in the single-channel regime. Our GPU implementation delivers 1.9 billion steps per second, matching the scale of non-differentiable simulators. By making exact stochastic simulation massively parallel and autodiff-compatible, our results enable high-dimensional parameter inference and inverse design across systems biology, chemical kinetics, physics, and related CTMC-governed domains.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [79] [Certified Uncertainty for Surrogate Models of Neutron Star Equations of State via Mondrian Conformal Prediction](https://arxiv.org/abs/2602.19363)
*Marlon M. S. Mendes,Roberta Duarte Pereira,Mariana Dutra da Rosa Louren,C√©sar H. Lenzi*

Main category: astro-ph.HE

TL;DR: A multitask surrogate for neutron-star equations of state with certified uncertainty via conformal prediction, achieving high accuracy and reliable uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Neutron-star equations of state (EoS) modeling requires uncertainty quantification for reliable inference. Existing methods lack distribution-free, certified uncertainty guarantees needed for robust astrophysical predictions.

Method: Uses split conformal prediction and Mondrian variant with multitask surrogate that ingests 6-parameter piecewise-polytropic representation. Jointly performs validity classification under physical constraints and regression of key neutron-star properties (M_max, R(M_max), R_1.4, Œõ_1.4). Trained on 40,000 balanced EoSs.

Result: Near-perfect discrimination (AUC ‚âà 0.997), sub-percent relative errors for masses/radii, few-percent error for tidal deformability. Empirical coverages closely track 1-Œ± for both Standard and Mondrian CP across Œ±‚àà[0.05,0.25]. Mondrian yields narrower average physical widths at comparable coverage in conservative regimes.

Conclusion: First application of class-conditioned (Mondrian) conformal calibration to neutron-star EoS surrogates, enabling efficient, reproducible, and uncertainty-aware inference. Framework is extensible to functional targets like full R(M) curves.

Abstract: We present a multitask surrogate for neutron-star equations of state (EoSs) that delivers \emph{distribution-free}, certified uncertainty via split conformal prediction (CP) and its Mondrian variant. The surrogate ingests a six-parameter piecewise-polytropic representation $(\log_{10}p_1,Œì_1,Œì_2,Œì_3,œÅ_1,œÅ_2)$ -- with fixed transition densities $œÅ_1$ and $œÅ_2$ -- and jointly performs (i) validity classification under physical/observational constraints and (ii) regression of $M_{\max}$, $R(M_{\max})$, $R_{1.4}$, and $Œõ_{1.4}$. Trained on a balanced set of $40{,}000$ EoSs, the model attains near-perfect discrimination (AUC $\approx 0.997$) and sub-percent relative errors for masses and radii, with few-percent error for tidal deformability. Across $Œ±\in[0.05,0.25]$, empirical coverages closely track $1-Œ±$ for both Standard and Mondrian CP; in conservative regimes, Mondrian yields narrower average physical widths at comparable coverage. To our knowledge, this is the first application of class-conditioned (Mondrian) conformal calibration to neutron-star EoS surrogates, enabling efficient, reproducible, and uncertainty-aware inference; the framework is readily extensible to functional targets (e.g., full $R(M)$ curves).

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [80] [Chemotaxis of cell aggregates: morphology and dynamics of migrating active droplets](https://arxiv.org/abs/2602.20088)
*Giulia L. Celora,Benjamin J. Walker,Mohit P. Dalwadi,Philip Pearce*

Main category: cond-mat.soft

TL;DR: Active droplets of biological cells exhibit proliferation-driven morphological transitions during chemotaxis, with transitions occurring continuously or discontinuously due to exponentially small asymptotic effects from contact lines.


<details>
  <summary>Details</summary>
Motivation: To understand how emergent fluid-like properties of biological tissues affect tissue structure and function, particularly inspired by experiments showing cell aggregates behaving as active droplets during collective chemotaxis.

Method: Developed a minimal model of a growing thin active droplet driven by self-generated chemical gradient. Used dynamic simulations and multiple scales analysis to show droplet dynamics follow traveling wave solutions defined by nonlinear eigenvalue problem parametrized by slowly increasing droplet volume.

Result: Chemotacting droplets exhibit proliferation-driven morphological transitions that can occur continuously or through discontinuous bifurcation. Transitions arise from exponentially small asymptotic terms originating from rear and front contact lines. Nature of transitions determined by two key dimensionless parameters: internal stress balance and coupling strength between migration dynamics and external chemical field.

Conclusion: Provides complete characterization of morphodynamics of migrating active thin droplets, with implications for biological systems where cell aggregates exhibit fluid-like behavior during collective migration.

Abstract: Biological tissues have been observed to display emergent fluid-like properties, owing to physical interactions between cells. However, it remains unclear in general how these fluid-like properties affect tissue structure and function. Here, we are motivated by recent experiments in which cell aggregates were observed to behave as active droplets during collective migration along chemical gradients, or chemotaxis. To understand this process, we develop a minimal model of a growing thin active droplet driven by a self-generated chemical gradient. In broad agreement with the experiments, dynamic simulations reveal that chemotacting droplets exhibit proliferation-driven morphological transitions. To fully characterise these transitions, we perform a multiple scales analysis to show that the droplet dynamics follow a sequence of travelling wave solutions defined by a nonlinear eigenvalue problem parametrised by the slowly increasing droplet volume. Our analysis reveals that morphological transitions can occur continuously or through a discontinuous bifurcation. Further asymptotic analysis of the travelling wave problem reveals that these morphological transitions arise from exponentially small ("beyond-all-orders") asymptotic terms that originate from the rear and front contact lines. Moreover, we show that the nature of the transitions is fully determined by two key dimensionless parameters, which quantify the internal stress balance within the droplet and the strength of the coupling between the droplet migration dynamics and the external chemical field. Overall, our results provide a complete characterisation of the morphodynamics of a class of migrating active thin droplets, with implications in a range of biological systems where cell aggregates exhibit fluid-like behaviour.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [81] [Trotter Error and Orbital Transformations in Quantum Phase Estimation](https://arxiv.org/abs/2602.18913)
*Marvin Kronenberger,Mihael Erakovic,Markus Reiher*

Main category: quant-ph

TL;DR: Investigating orbital transformation effects on Trotter error in quantum computation, finding local orbitals don't cause large errors and reliable error reduction is challenging.


<details>
  <summary>Details</summary>
Motivation: While localised orbitals reduce circuit depth in quantum computation, literature suggests they cause large Trotter errors. This paper investigates whether orbital transformations can effectively reduce these errors.

Method: Three strategies: (1) a priori selection of low-error orbital basis, (2) deriving orbital basis with zero Trotter error in ground state energy, (3) using propagators that change computational basis between Trotter steps.

Result: Reliably reducing Trotter error by orbital transformations is challenging despite analytical suggestions. Importantly, localised orbitals don't produce large Trotter errors in molecular calculations.

Conclusion: Orbital transformations are difficult for reliably reducing Trotter error, but localised orbitals remain viable for efficient QPE setups as they don't cause large errors.

Abstract: Quantum computation with Trotter product formulae is straightforward and requires little overhead in terms of logical qubits. The choice of the orbital basis significantly affects circuit depth, with localised orbitals yielding lowest circuit depths. However, literature results point to large Trotter errors incurred by localised orbitals. Here, we therefore investigate the effect of orbital transformations on Trotter error. We consider three strategies to reduce Trotter error by orbital transformation: (i) The a priori selection of an orbital basis that produces low Trotter error. (ii) The derivation of an orbital basis that produces a ground state energy free of Trotter error (as we observed that the Trotter error is a continuous function in the Givens-rotation parameter, from which continuity of this error upon orbital transformation can be deduced). (iii) Application of propagators that change the computational basis between Trotter steps. Our numerical results show that reliably reducing Trotter error by orbital transformations is challenging. General recipes to produce low Trotter errors cannot be easily derived, despite analytical expressions which suggest ways to decrease Trotter error. Importantly, we found that localised orbital bases do not produce large Trotter errors in molecular calculations, which is an important result for efficient QPE set-ups.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [82] [Nanometer-scale pre-bunched electron beams generated from all-optical plasma-based acceleration](https://arxiv.org/abs/2602.19081)
*Zhenan Wang,Zewei Xu,Qianyi Ma,Yuhui Xia,Letian Liu,Chenxu Wang,Thamine Dalichaouch,Xueqing Yan,Xinlu Xu,Warren B. Mori*

Main category: physics.acc-ph

TL;DR: An all-optical plasma-based scheme produces pre-bunched electron beams at nanometer scales for generating coherent, high-intensity x-rays.


<details>
  <summary>Details</summary>
Motivation: High-quality prebunched electron beams are essential for producing coherent x-rays with high intensity and narrow bandwidth for modern light sources, but existing methods may lack compactness or efficiency.

Method: Using two low-intensity counter-propagating lasers to create density modulation in uniform plasma, which modulates the phase velocity of a plasma wake excited by an intense driver laser. This modulation turns electron injection on/off at twice the colliding laser frequency, creating microbunched electrons at Doppler-shifted wavelengths.

Result: The scheme can produce beams with exotic pre-bunched structures by controlling drive and colliding laser properties, enabling generation of ultrafast high-power x-rays.

Conclusion: This compact, all-optical scheme for ultra-bright pre-bunched electron beams enables novel ultrafast x-ray applications and has broad interest across various fields.

Abstract: High-quality and prebunched electron beams can produce coherent x-rays with high intensity and narrow bandwidth, which is essential for modern light sources. An all-optical scheme based on plasma-based acceleration to produce bright electron beams that are pre-bunched on nanometer-scales is proposed. By using a density modulation created by two low intensity counter-propagating lasers, the phase velocity of the plasma wake excited by an intense driver laser in a uniform plasma can be modulated at a frequency twice that of the colliding lasers and thus turn the injection on and off. The injected electrons are micro-bunched at the Doppler shifted wavelength of the modulated wavelength using the corresponding phase velocity of the gradual expansion of the wakefield. It is demonstrated that by controlling the properties of the drive and colliding lasers, that beams with exotic pre-bunched structures can be produced, which may have critical applications in ultrafast high power x-rays. This extremely compact, all-optical scheme to produce ultra-bright pre-bunched electron beams may therefore enable novel applications for ultrafast x-ray users and arouse general interest in various fields.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [83] [Extended scattering channels for random matrix simulations of polarized light transport](https://arxiv.org/abs/2602.18685)
*Niall Byrnes,Sulagna Dutta,Matthew R. Foreman*

Main category: physics.optics

TL;DR: A random matrix simulation framework for modeling polarized light transport through random media with arbitrary scatterers, featuring extended scattering channels and rigorous treatment of scattering matrix correlations.


<details>
  <summary>Details</summary>
Motivation: Understanding and controlling wave transport in optical and mesoscopic applications requires accurate modeling of light propagation through disordered media. Current approaches lack rigorous treatment of scattering matrix correlations and flexible representations of arbitrary illumination/detection profiles.

Method: Random matrix simulation framework using extended scattering channels applied to angular spectral decompositions of fields. This enables flexible representations of arbitrary illumination and detection profiles while providing rigorous treatment of scattering matrix correlations.

Result: The framework offers novel geometric insights into the optical memory effect and enables modeling of polarized light transport through random media composed of arbitrary particulate scatterers. Numerical simulations demonstrate key features of the approach.

Conclusion: The presented framework advances modeling capabilities for light transport in disordered media, providing both theoretical rigor and practical implementation through an accompanying codebase, with applications in optical and mesoscopic systems.

Abstract: Modeling the propagation of light through disordered media is central to understanding and controlling wave transport in diverse optical and mesoscopic applications. Here, we present a random matrix simulation framework for modeling the transport of polarized light through random media composed of arbitrary particulate scatterers. Our approach employs extended scattering channels applied to angular spectral decompositions of the underlying fields, enabling flexible representations of arbitrary illumination and detection profiles. In contrast to previous work, this framework provides a rigorous treatment of scattering matrix correlations and offers novel geometric insights into the optical memory effect. We provide a detailed exposition of the underlying theory and illustrate several key features through numerical simulations. Our work is supported by a free accompanying codebase.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [84] [The Burgers Transform: From Holomorphic Functions to Rigid Elliptic Structures](https://arxiv.org/abs/2602.19251)
*Daniel Alay√≥n-Solarz*

Main category: math.CV

TL;DR: The Burgers transform is a nonlinear bijection between holomorphic functions and rigid elliptic structures, where holomorphicity is necessary for rigidity, not just sufficient.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous connection between holomorphic functions and rigid variable elliptic structures, closing a gap in existing literature about the necessity of holomorphicity for rigidity.

Method: Introduces the Burgers transform $\mathcal{B}$ defined implicitly by $Œª= f(y-Œªx)$, which maps holomorphic functions to structures satisfying the conservative complex Burgers equation $Œª_x+ŒªŒª_y=0$.

Result: Proves holomorphicity is necessary for rigidity, identifies $\operatorname{Hol}(U,\mathbb{C}^+)$ as maximal seed space, provides obstruction formula for non-holomorphicity, and establishes $f$-analytics hierarchy.

Conclusion: Rigidity emerges as a genuinely complex phenomenon, with the transform creating a hierarchy of analytic structures where seed complexity doesn't determine resulting structure complexity.

Abstract: We introduce the Burgers transform $\mathcal{B}$, a nonlinear bijection between holomorphic functions $f\colon U\to\mathbb{C}^+$ and rigid variable elliptic structures on the plane, defined implicitly by $Œª= f(y-Œªx)$. The output automatically satisfies the conservative complex Burgers equation $Œª_x+ŒªŒª_y=0$.
  Our main result is that holomorphicity of the seed $f$ is necessary, not merely sufficient, for rigidity: any $C^1$ function whose implicit solution satisfies $Œª_x+ŒªŒª_y=0$ must be holomorphic. This closes a gap in the existing literature and identifies $\operatorname{Hol}(U,\mathbb{C}^+)$ as the maximal seed space compatible with rigidity. The obstruction formula $H|_{x=0} = 2i\,(\operatorname{Im} f)\,f_{\bar{w}}$ quantifies the cost of non-holomorphicity at the level of the initial data.
  The transform establishes a hierarchy we call $f$-analytics: standard complex analysis ($f=i$), $p(x)$-analytics ($f\colon U\to i\mathbb{R}_+$), and the full rigid class ($f\colon U\to\mathbb{C}^+$ holomorphic), in which rigidity -- absent from the real diameter of $\mathbb{D}$ -- emerges as a genuinely complex phenomenon. We characterise the domain of $\mathcal{B}$ through shock formation, its interaction with affine automorphisms of $\mathbb{C}^+$, and the infinitesimal structure: the propagator $\mathcal{P}_f = D\mathcal{B}_f$ satisfies a Jacobian-twisted multiplicativity that deforms the seed algebra by the density of characteristics. Four worked examples -- affine, exponential, inverse, and trigonometric seeds -- show that the complexity class of a seed and that of the resulting structure are generically unrelated.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [85] [A Computationally Efficient Multidimensional Vision Transformer](https://arxiv.org/abs/2602.19982)
*Alaa El Ichi,Khalide Jbilou*

Main category: cs.LG

TL;DR: TCP-ViT: A tensor-based Vision Transformer using Tensor Cosine Product for efficient attention with 1/C parameter reduction while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers have state-of-the-art performance but face practical deployment limitations due to high computational and memory costs. There's a need for more efficient transformer architectures that can maintain performance while reducing resource requirements.

Method: Proposes a novel tensor-based framework for Vision Transformers using Tensor Cosine Product (Cproduct). Exploits multilinear structures in image data and orthogonality of cosine transforms to enable efficient attention mechanisms and structured feature representations. Develops theoretical foundations of tensor cosine product, analyzes its algebraic properties, and integrates it into TCP-ViT architecture.

Result: Achieves uniform 1/C parameter reduction (where C is number of channels) while maintaining competitive accuracy on standard classification and segmentation benchmarks.

Conclusion: The proposed TCP-ViT framework provides an efficient alternative to standard Vision Transformers by leveraging tensor structures and cosine transforms, enabling practical deployment with significantly reduced parameters while preserving performance.

Abstract: Vision Transformers have achieved state-of-the-art performance in a wide range
  of computer vision tasks, but their practical deployment is limited by high
  computational and memory costs. In this paper, we introduce a novel tensor-based
  framework for Vision Transformers built upon the Tensor Cosine Product
  (Cproduct). By exploiting multilinear structures inherent in image data and the
  orthogonality of cosine transforms, the proposed approach enables efficient
  attention mechanisms and structured feature representations. We develop the
  theoretical foundations of the tensor cosine product, analyze its algebraic
  properties, and integrate it into a new Cproduct-based Vision Transformer
  architecture (TCP-ViT). Numerical experiments on standard classification and
  segmentation benchmarks demonstrate that the proposed method achieves a uniform
  1/C parameter reduction (where C is the number of channels) while
  maintaining competitive accuracy.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [86] [The Free-Electron Laser Model of Magnetospheric Chorus](https://arxiv.org/abs/2602.18544)
*Brandon Bonham*

Main category: physics.space-ph

TL;DR: This dissertation develops a nonlinear model of magnetospheric chorus waves using free-electron laser theory, deriving equations to describe wave-particle interactions and predicting solitary chorus wave behavior.


<details>
  <summary>Details</summary>
Motivation: Chorus waves in Earth's magnetosphere can amplify rapidly and accelerate electrons, endangering space technologies. Understanding their amplification mechanism is crucial for space weather prediction and satellite protection.

Method: Uses free-electron laser model approach, derives 2N+2 equations for N resonant electrons, reduces to three nonlinear equations via collective variables, develops Ginzburg-Landau equation for wave packet behavior.

Result: Develops novel nonlinear model of whistler-mode chorus, predicts solitary chorus waves, analyzes single-mode solutions including linear stability and mode condensation from noisy spectra.

Conclusion: The free-electron laser model provides effective framework for understanding chorus wave amplification, with nonlinear equations enabling prediction of solitary waves and mode condensation phenomena.

Abstract: Chorus waves are electromagnetic waves named for their resemblance to birds chirping at dawn when their radio frequencies are played as audio. The amplification of chorus in Earth's magnetosphere has been the subject of intense scientific inquiry since the discovery of the Van Allen radiation belts in 1958. Resonant interactions between chorus and radiation belt electrons can lead to the exponential growth of small seed waves by a factor of fifty within milliseconds. These powerful modes can cause rapid acceleration of electrons and endanger space-based technologies. Recent efforts to understand chorus amplification have drawn upon parallels to free-electron lasers, laboratory devices that generate intense coherent light with tunable frequencies. This approach, known as the free-electron laser model of magnetospheric chorus, is the subject of this dissertation. In this work, we build on previous research on the free-electron laser model, ultimately presenting a novel nonlinear model of whistler-mode chorus in the magnetosphere. In the first chapter, we provide a brief introduction to whistlers, magnetospheric chorus, and free-electron lasers. We also derive the 2N+2 equations foundational to the interaction of chorus with N resonant electrons. In the second chapter, we derive a reduced set of just three nonlinear equations using the method of collective variables. We then derive a Ginzburg-Landau equation (GLE) for the behavior of a chorus wave packet with a spectrum of frequencies with spatially varying amplitudes and discuss the prediction of solitary chorus waves. In the third chapter, we focus on the behavior of the single-mode solutions predicted by the GLE, including their linear stability and the phenomenon of mode condensation, where a single mode can emerge from a noisy spectrum. In the final chapter, we summarize the results and discuss open questions and future directions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [87] [Smoothness Adaptivity in Constant-Depth Neural Networks: Optimal Rates via Smooth Activations](https://arxiv.org/abs/2602.19691)
*Yuhao Liu,Zilin Wang,Lei Wu,Shaobo Zhang*

Main category: stat.ML

TL;DR: Smooth activation functions enable neural networks to achieve minimax-optimal approximation and estimation rates for arbitrary smoothness classes, unlike non-smooth activations like ReLU which require depth growth to capture higher-order smoothness.


<details>
  <summary>Details</summary>
Motivation: The theoretical advantages of smooth activation functions over non-smooth ones (like ReLU) remain poorly understood, despite their ubiquity in deep learning. The paper aims to characterize the approximation and statistical properties of networks with smooth activations.

Method: Constructive approximation framework that produces explicit neural network approximators with carefully controlled parameter norms and model size. Analysis focuses on Sobolev spaces W^{s,‚àû}([0,1]^d) for arbitrary smoothness s>0.

Result: Smooth activations enable constant-depth networks to automatically exploit arbitrarily high orders of target function smoothness, achieving minimax-optimal approximation and estimation error rates (up to logarithmic factors). Non-smooth activations like ReLU lack this adaptivity and require depth growth proportional to smoothness order.

Conclusion: Activation smoothness is identified as a fundamental mechanism, alternative to depth, for attaining statistical optimality. The constructive framework removes impractical sparsity constraints required in prior analyses and ensures statistical learnability under empirical risk minimization.

Abstract: Smooth activation functions are ubiquitous in modern deep learning, yet their theoretical advantages over non-smooth counterparts remain poorly understood. In this work, we characterize both approximation and statistical properties of neural networks with smooth activations over the Sobolev space $W^{s,\infty}([0,1]^d)$ for arbitrary smoothness $s>0$. We prove that constant-depth networks equipped with smooth activations automatically exploit arbitrarily high orders of target function smoothness, achieving the minimax-optimal approximation and estimation error rates (up to logarithmic factors). In sharp contrast, networks with non-smooth activations, such as ReLU, lack this adaptivity: their attainable approximation order is strictly limited by depth, and capturing higher-order smoothness requires proportional depth growth. These results identify activation smoothness as a fundamental mechanism, alternative to depth, for attaining statistical optimality. Technically, our results are established via a constructive approximation framework that produces explicit neural network approximators with carefully controlled parameter norms and model size. This complexity control ensures statistical learnability under empirical risk minimization (ERM) and removes the impractical sparsity constraints commonly required in prior analyses.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [88] [Quantitative Estimates on the Topology and Singular Set of Prescribed Mean Curvature Hypersurfaces](https://arxiv.org/abs/2602.19804)
*Nicolau S. Aiex,Sean McCurdy,Paul Minter*

Main category: math.DG

TL;DR: The paper establishes quantitative topological and singularity bounds for prescribed mean curvature (PMC) hypersurfaces in Riemannian manifolds, extending previous results from minimal hypersurfaces to PMC hypersurfaces.


<details>
  <summary>Details</summary>
Motivation: To extend known topological and singularity bounds from minimal hypersurfaces to prescribed mean curvature hypersurfaces, which are more general geometric objects that arise naturally in geometric analysis and PDE theory.

Method: The authors use geometric measure theory and analysis techniques to establish quantitative bounds that depend only on geometric quantities. They build upon previous work on minimal hypersurfaces and extend it to the PMC setting, leveraging the recent construction of PMC hypersurfaces via min-max techniques by Bellettini-Wickramasekera.

Result: Two main results: (1) For dimensions 3-7, a bound on the sum of Betti numbers in terms of the index; (2) For dimensions 8+, a bound on the Minkowski content of the singular set in terms of the index. Both bounds depend only on dimension, manifold geometry, area bound, and the C^{1,Œ±} norm of the prescribed mean curvature function.

Conclusion: The paper successfully extends quantitative topological and singularity estimates from minimal hypersurfaces to prescribed mean curvature hypersurfaces, providing important tools for studying the structure of PMC hypersurfaces in geometric analysis.

Abstract: We establish quantitative topological and singularity properties for (certain) prescribed mean curvature (PMC) hypersurfaces $V^n$ in Riemannian manifolds $(N^{n+1},h)$. Indeed, if $V$ has area at most $A>0$ with PMC given by a $C^{1,Œ±}$ function $g:N\to \mathbb{R}$ with the bound $|g|_{C^{1,Œ±}}\leq Œì$, we show that there exists a constant $C$ depending only on $n,h,A,Œì$ and geometric quantities such that: \[\sum^n_{i=0}b^i(V) \leq C(1+\text{index}(V)) \quad \text{if }3\leq n+1\leq 7;\] \[M^{*n-7}(\text{sing}(V)) \leq C(1+\text{index}(V)) \quad \text{if }n+1\geq 8.\] Here, $b^i$ denote the Betti numbers over any field, $M^{*n-7}$ denotes the upper $(n-7)$-dimensional Minkowski content, and $\text{sing}(V)$ is the singular set of $V$. The first inequality extends the work of Song from the minimal hypersurface setting to the PMC hypersurface setting, whilst the second extends work of the authors. Our results apply to the PMC hypersurfaces constructed recently through min-max techniques by Bellettini--Wickramasekera.

</details>


### [89] [New minimal surfaces in the sphere from capillary minimal cones](https://arxiv.org/abs/2602.20124)
*Benjy Firester,Raphael Tsiamis*

Main category: math.DG

TL;DR: Construct minimal embeddings of S^p √ó S^q √ó S^1 in S^{p+q+2} using free-boundary minimal cones with bi-orthogonal symmetry, solving Hsiang-Lawson and Hsiang-Hsiang problems.


<details>
  <summary>Details</summary>
Motivation: Solve long-standing problems in minimal surface theory posed by Hsiang-Lawson and Hsiang-Hsiang regarding minimal embeddings of product manifolds in spheres.

Method: Use equivariant reduction to transform minimal surface PDE to ODE, construct capillary minimal cones for all contact angles via singular shooting problem with infinite initial slope, then obtain free-boundary solutions as limits.

Result: Successfully construct minimal embeddings of S^p √ó S^q √ó S^1 in S^{p+q+2} for all p,q‚â•1, and establish connection between one-phase free boundaries and minimal surfaces through capillary functional.

Conclusion: The paper solves classical problems in minimal surface theory and reveals deep connections between minimal surfaces, capillary surfaces, and one-phase Bernoulli problems through equivariant methods and singular shooting techniques.

Abstract: For every $p,q\geq 1$, we construct minimal embeddings of $\mathbb{S}^p \times \mathbb{S}^q \times \mathbb{S}^1$ in $\mathbb{S}^{p + q + 2}$ by doubling the links of free-boundary minimal cones in $\mathbb{R}^{p+q+3}$ with bi-orthogonal symmetry. This solves problems posed by Hsiang-Lawson and Hsiang-Hsiang. The equivariance reduces the minimal surface equation to an ODE, and we prove the existence of capillary minimal cones for every contact angle. We obtain free-boundary solutions as limits of capillary surfaces via a singular shooting problem with infinite initial slope. As the contact angle degenerates to $0$, rescalings of the capillary cones converge to a homogeneous solution of the one-phase Bernoulli problem, further illustrating the connection between one-phase free boundaries and minimal surfaces through the capillary functional.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [90] [Schauder estimates for germs of distributions on smooth manifolds](https://arxiv.org/abs/2602.19593)
*Beatrice Costeri,Claudio Dappiaggi,Paolo Rinaldi,Matteo Savasta*

Main category: math-ph

TL;DR: The paper develops multi-level Schauder estimates for distributions on Riemannian manifolds without geometric assumptions, extending reconstruction theorems and regularity results to this setting.


<details>
  <summary>Details</summary>
Motivation: To establish Schauder estimates for distributions on Riemannian manifolds without requiring additional geometric assumptions, extending classical results from Euclidean spaces to curved geometries.

Method: 1. Define coherence and homogeneity for distribution germs on ‚Ñù^d. 2. Formulate reconstruction theorem and Schauder estimates in Euclidean setting. 3. Use exponential map to extend results to Riemannian manifolds. 4. Develop Œ≤-regularizing kernels on manifolds to prove Schauder estimates for coherent/homogeneous germs.

Result: Successfully extended reconstruction theorem to Riemannian manifolds, proved regularity of reconstructed distributions in H√∂lder-Zygmund spaces, and established Schauder estimates for coherent/homogeneous germs using novel Œ≤-regularizing kernels.

Conclusion: The paper provides a comprehensive framework for analyzing distributions on Riemannian manifolds, establishing fundamental results like reconstruction theorems and Schauder estimates without geometric restrictions, with potential applications in geometric analysis and PDE theory.

Abstract: We discuss germs of distributions on $d-$dimensional smooth Riemannian manifolds and, in particular, we derive \emph{multi-level Schauder estimates} without making any further assumptions on the underlying geometry. As a preliminary step, we define the notions of coherence and homogeneity for germs of distributions on open subsets of $\mathbb{R}^d$, $d \ge 1$. Subsequently, we formulate both the reconstruction theorem, cf., [CZ20], and the Schauder estimates, cf., [BCZ24], in this setting. Leveraging the properties of the exponential map, we extend these results to Riemannian manifolds. Specifically, we devise a counterpart of the reconstruction theorem previously established in the literature [RS21], while additionally proving the regularity of the reconstructed distribution in suitable H√∂lder-Zygmund spaces. Finally, by introducing a novel concept of $Œ≤$-regularizing kernels on Riemannian manifolds, we establish Schauder estimates for coherent and homogeneous germs in this context.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [91] [Optimal design with uncertainties: a risk-averse approach](https://arxiv.org/abs/2602.19869)
*Amal Alphonse,Petar Kun≈°tek,Marko Vrdoljak*

Main category: math.OC

TL;DR: This paper studies stochastic optimal design for elliptic PDEs with mixed materials, minimizing risk measures of system response under loading uncertainty, using homogenization theory and developing a numerical algorithm with Karhunen-Lo√®ve expansion for VaR/CVaR computations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address optimal design problems for elliptic PDEs where material coefficients represent mixtures of two conducting materials, incorporating uncertainty in loading conditions through probability distributions, and minimizing generalized risk measures of system response.

Method: The method uses homogenization theory to establish existence of relaxed optimal designs, derives first-order stationarity conditions, develops an optimality criteria algorithm, and employs truncated Karhunen-Lo√®ve expansion to handle stochastic components for evaluating VaR and CVaR contributions.

Result: The paper establishes existence of relaxed optimal designs via homogenization theory, derives first-order stationarity conditions for optima, develops a numerical algorithm, and demonstrates the approach with a CVaR-based compliance minimization example.

Conclusion: The paper provides a theoretical and computational framework for stochastic optimal design of elliptic PDE systems with mixed materials under loading uncertainty, combining homogenization theory, risk measure optimization, and numerical algorithms with practical applications to compliance minimization.

Abstract: We study a class of stochastic optimal design problems for elliptic partial differential equations in divergence form, where the coefficients represent mixtures of two conducting materials. The objective is to minimize a generalized risk measure of the system response, incorporating uncertainty in the loading through probability distributions. We establish existence of relaxed optimal designs via homogenization theory and derive first-order stationarity conditions satisfied by the optima. Based on these conditions, we develop an optimality criteria algorithm for numerical computations. The stochastic component is treated using a truncated Karhunen--Lo√®ve expansion, allowing evaluation of the value-at-risk (VaR) and conditional value-at-risk (CVaR) contributions arising from the sensitivity analysis and featured in the algorithm. The method is illustrated for an example involving CVaR-based compliance minimization.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [92] [Data-Driven Bath Fitting for Hamiltonian-Diagonalization Dynamical Mean-Field Theory](https://arxiv.org/abs/2602.19637)
*Taeung Kim,Jeongmoo Lee,Ara Go*

Main category: cond-mat.str-el

TL;DR: ML-based initialization method improves bath fitting in HD-DMFT by predicting near-optimal discrete bath parameters from hybridization functions, reducing optimization sensitivity and accelerating convergence.


<details>
  <summary>Details</summary>
Motivation: HD-DMFT suffers from nonlinear bath-fitting bottleneck where optimization becomes sensitive to initial guesses and prone to local minima as bath sites increase, slowing DMFT self-consistency loops.

Method: Reformulate bath fitting as supervised regression, train kernel ridge regression model to predict bath parameters from hybridization functions. Use training data from ruthenate models with systematic structural deformations, incorporate time-reversal symmetry in representations.

Result: ML initialization reduces initial fitting error, decreases conjugate-gradient iterations, improves robustness against local minima across bath sizes. Transfers to interacting DMFT for Sr‚ÇÇRuO‚ÇÑ, yielding faster convergence than heuristic baseline while preserving final solution.

Conclusion: Machine learning initialization effectively overcomes nonlinear bath-fitting bottleneck in HD-DMFT, providing systematic improvement in optimization efficiency and robustness while maintaining physical consistency and transferability to interacting systems.

Abstract: We propose a machine-learning-based initialization method to overcome the nonlinear bath-fitting bottleneck in Hamiltonian-diagonalization-based dynamical mean-field theory (HD-DMFT). In HD-DMFT, the continuous hybridization function is approximated by a finite set of bath-site energies and hybridization amplitudes, determined by minimizing a highly non-convex multivariable cost function. As the number of bath sites increases, the optimization becomes more sensitive to the initial guess and more prone to suboptimal local minima, which can slow or destabilize the DMFT self-consistency loop. We reformulate bath fitting as a supervised regression problem and train a kernel ridge regression model to predict near-optimal discrete bath parameters directly from the target hybridization function on the Matsubara axis. To ensure physical relevance and data diversity, we construct the training dataset from tight-binding Hamiltonians of layered-perovskite-like ruthenate models across systematically deformed structures, instead of relying on naive random parameter sampling, and obtain high-quality labels through fully converged conventional bath fitting. Time-reversal symmetry is explicitly incorporated in both feature and target representations to reduce effective dimensionality and enforce physical consistency. Benchmarks in the non-interacting limit show that the learned initialization systematically reduces the initial fitting error, decreases the number of conjugate-gradient iterations, and improves robustness against local minima over a wide range of bath sizes. We further demonstrate transferability to interacting DMFT calculations for $\mathrm{Sr_{2}RuO_{4}}$ solved with an adaptive-truncation impurity solver, where the ML initialization yields consistently faster convergence than a symmetry-preserving heuristic baseline while preserving the final fitted solution.

</details>
