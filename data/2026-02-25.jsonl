{"id": "2602.20282", "pdf": "https://arxiv.org/pdf/2602.20282", "abs": "https://arxiv.org/abs/2602.20282", "authors": ["Egor P. Berezin", "Robert T. Zaks", "German Z. Alekhin", "Stanislav V. Morozov", "Sergey A. Matveev"], "title": "Two approaches to low-parametric SimRank computation", "categories": ["math.NA", "cs.DM"], "comment": "13 pages, 3 figures, 1 table, 3 algorithms, 24 references", "summary": "In this work, we discuss low-parametric approaches for approximating SimRank matrices, which estimate the similarity between pairs of nodes in a graph. Although SimRank matrices and their computation require a significant amount of memory, common approaches mostly address the problem of algorithmic complexity. We propose two major formats for the economical embedding of target data. The first approach adopts a non-symmetric form that can be computed using a specialized alternating optimization algorithm. The second is based on a symmetric representation and Newton-type iterations. We propose numerical implementations for both methodologies that avoid working with dense matrices and maintain low memory consumption. Furthermore, we study both types of embeddings numerically using real data from publicly available datasets. The results show that our algorithms yield a good approximation of the SimRank matrices, both in terms of the error norm (particularly the Chebyshev norm) and in preserving the average number of the most similar elements for each given node."}
{"id": "2602.20390", "pdf": "https://arxiv.org/pdf/2602.20390", "abs": "https://arxiv.org/abs/2602.20390", "authors": ["James Chen", "Alan Edelman", "John Urschel"], "title": "The largest 5th pivot may be the root of a 61st degree polynomial", "categories": ["math.NA"], "comment": null, "summary": "This paper introduces a number of new techniques in the study of the famous question from numerical linear algebra: what is the largest possible growth factor when performing Gaussian elimination with complete pivoting? This question is highly complex, due to a complicated set of polynomial inequalities that need to be simultaneously satisfied. This paper introduces the JuMP + Groebner basis + discriminant polynomial approach as well as the use of interval arithmetic computations. Thus, we are introducing a marriage of numerical and exact mathematical computations.\n  In 1988, Day and Peterson performed numerical optimization on $n=5$ with NPSOL and obtained a largest seen value of $4.1325...$. This same best value was reproduced by Gould with LANCELOT in 1991. We ran extensive comparable experiments with the modern software tool JuMP and also saw the same value $4.1325...$. While the combinatorial explosion of possibilities prevents us from knowing there may not be a larger maximum, we succeed in obtaining the exact mathematical value: the number $4.1325...$ is exactly the root of a 61st degree polynomial provided in this work, and is a maximum given the equality constraints seen by JuMP. In light of the numerics, we pose the conjecture that this lower bound is indeed the maximum. We also apply this technique to $n = 6$, $7$, and $8$.\n  Furthermore, in 1969, an upper bound of $4\\frac{17}{18}\\approx 4.94$ was produced for the maximum possible growth for $n = 5$. We slightly lower this upper bound to $4.84$."}
{"id": "2602.20395", "pdf": "https://arxiv.org/pdf/2602.20395", "abs": "https://arxiv.org/abs/2602.20395", "authors": ["Tristan Goodwill", "Jeremy Hoskins", "Zydrunas Gimbutas", "Bowei Wu"], "title": "A parametrix for the surface Stokes equation", "categories": ["math.NA"], "comment": null, "summary": "We introduce an integral equation formulation of the surface Stokes equations, constructed using two-dimensional Stokeslets. The resulting integral equations are Fredholm integral equations of the second kind and can be discretized to high order using standard tools. Since the resulting discrete linear systems are dense, we describe and analyze a proxy shell method to construct fast direct solvers for these systems. The properties of our integral equation, and the performance of the resulting numerical scheme, are illustrated with several representative numerical examples."}
{"id": "2602.20679", "pdf": "https://arxiv.org/pdf/2602.20679", "abs": "https://arxiv.org/abs/2602.20679", "authors": ["Andreu Martorell", "Pep Mulet", "Dionisio F. Yáñez"], "title": "Implicit-explicit all-speed schemes for compressible Cahn-Hilliard-Navier-Stokes equations", "categories": ["math.NA"], "comment": null, "summary": "We propose a second-order implicit-explicit (IMEX) time-stepping scheme for the isentropic, compressible Cahn-Hilliard-Navier-Stokes equations in the low Mach number regime.\n  The method is based on finite differences on staggered grids and is specifically designed to handle the challenges posed by the low Mach number limit, where the system approaches to an incompressible behavior.\n  In this regime, standard explicit schemes suffer from severe time-step restrictions due to fourth-order diffusion terms and the stiffness induced by fast acoustic waves.\n  To overcome this, we employ an IMEX strategy which splits the governing equations into stiff and non-stiff components.\n  The stiff terms, arising from pressure, viscous forces and fourth-order Cahn-Hilliard contributions, are treated implicitly, while the remaining are dealt explicitly."}
{"id": "2602.20738", "pdf": "https://arxiv.org/pdf/2602.20738", "abs": "https://arxiv.org/abs/2602.20738", "authors": ["Karel L. K. De Witte", "Tom Braeckevelt", "Massimo Bocus", "Sander Vandenhaute", "Veronique Van Speybroeck"], "title": "A Novel NPT Thermodynamic Integration Scheme to Derive Rigorous Gibbs Free Energies for Crystalline Solids", "categories": ["physics.comp-ph"], "comment": null, "summary": "Thermodynamic Integration (TI) is the state-of-the-art computational technique for accurate Gibbs free energy predictions of solids. Conventional TI schemes start from an NVT harmonic reference and require three successive corrections to recover the Gibbs free energy of the real crystal in the NPT ensemble. However, the NVT-to-NPT correction neglects full cell flexibility. Here, we present a rigorous (and only) two-step TI scheme that operates entirely in the NPT ensemble, eliminating the need for the approximate NVT-to-NPT step. The key methodological advancement is the novel NPT reference that explicitly accounts for full cell fluctuations. The new approach is compared with the conventional one via two complementary case studies. For ice polymorphs, having simple cell-shape distributions, the new approach reproduces conventional TI results with excellent agreement. For CsPbI3, whose black phase exhibits complex cell-shape behavior, we demonstrate that our novel method provides more accurate Gibbs free energy differences than the conventional one. Moreover, the proposed framework maintains comparable computational cost while offering a simplified workflow. Overall, the new NPT TI scheme provides rigorous and direct Gibbs free energy calculations for solids."}
{"id": "2602.20564", "pdf": "https://arxiv.org/pdf/2602.20564", "abs": "https://arxiv.org/abs/2602.20564", "authors": ["T. Simpson", "R. A. Badcock", "T. Berry", "C. S. Chisholm", "P. J. Fimognari", "P. Fisher", "D. T. Garnier", "K. Lenagh-Glue", "B. Leuw", "R. Mataira", "L. Meadows", "T. McIntosh", "J. Poata", "K. Richardson", "B. Smith", "A. Simpson", "J. D. Tyler", "T. Wordsworth"], "title": "Deuterium-Tritium Levitated Dipole Fusion Power Plants", "categories": ["physics.plasm-ph", "physics.acc-ph"], "comment": "31 pages, 23 figures, Submitted to Fusion Engineering and Design", "summary": "Levitated dipole reactors offer an attractive path towards economic fusion power generation. The intrinsic decoupling of the confining magnetic field-generating REBCO magnets and the vacuum vessel offer unparalleled accessibility and maintainability, allowing for high plant duty factors and theoretically low electricity prices. In order to achieve rapid deployment of fusion power to the grid, the use of the Deuterium-Tritium (DT) fuel cycle is required due to its lower required plasma triple products. Historically, designs of levitated dipole fusion power plants have targeted advanced fuels as a DT device was seen to be infeasible due to the high fluxes of 14.1 MeV neutrons on the superconducting core magnet. This study presents high level designs for two feasible first-of-a-kind (FOAK) DT levitated dipole fusion power plants, the larger of which produces 667 MW of fusion power and is predicted to produce 208 MW of net electric power. Both designs consist of a heavily neutron-shielded, high-field REBCO core magnet capable of producing peak magnetic field strengths of 23 T while keeping peak mechanical strains below 0.4%. The neutron shielding is comprised of a layered structure of tungsten and boron carbide, which allows for 92% of the heat deposited in the neutron shield to be radiated out to the first wall while still providing sufficient neutron attenuation to give adequate REBCO conductor lifetimes. The core magnet REBCO coil is comprised of a small \"sacrificial\" section and a larger semi-permanent section. The sacrificial section, comprising ~20% of the coil, will have a neutron damage limited lifetime of ~1 year, after which the core magnet will be quickly removed from the vacuum vessel and replaced. This allows the damaged core magnet to be refurbished and reused, reducing cost and allowing for economic fusion power generation from a DT levitated dipole reactor."}
{"id": "2602.20381", "pdf": "https://arxiv.org/pdf/2602.20381", "abs": "https://arxiv.org/abs/2602.20381", "authors": ["Camille Labourie"], "title": "Uniform rectifiability of brittle fractures in linear elasticity", "categories": ["math.AP"], "comment": null, "summary": "We prove the uniform rectifiability of brittle fractures in arbitrary dimension. The existing approach for the Mumford-Shah functional, which relies on separation-type properties of the singular set, faces serious obstacles in the Griffith setting due to the lack of coarea formula for the symmetric gradient. We present an alternative route to uniform rectifiability for free-discontinuity problems by proving that cracks have ``plenty of big projections''."}
{"id": "2602.20697", "pdf": "https://arxiv.org/pdf/2602.20697", "abs": "https://arxiv.org/abs/2602.20697", "authors": ["Vladimír Lukeš", "Eduard Rohan"], "title": "Reduced-order computational homogenization for hyperelastic media using gradient based sensitivity analysis of microstructures", "categories": ["math.NA", "math.AP"], "comment": null, "summary": "We propose an algorithm for the computational homogenization of locally periodic hyperelastic structures undergoing large deformations due to external quasi-static loading. The algorithm performs clustering of macroscopic deformations into subsets called \"centroids\", and, as a new ingredient, approximates the homogenized coefficients using sensitivity analysis of micro-configurations with respect to the macroscopic deformation. The novel \"model-order reduction\" approach significantly reduces the number of microscopic problems that must be solved in nonlinear simulations, thereby accelerating the overall computational process. The degree of reduction can be controlled by a user-defined error tolerance parameter. The algorithm is implemented in the finite element framework SfePy, and its performance effectiveness is demonstrated using two-dimensional test examples, when compared with solutions obtained by the proper orthogonal decomposition method, and by the full \"FE-square\" simulations. Extensions beyond the present implementations and the scope of tractable problems are discussed."}
{"id": "2602.17158", "pdf": "https://arxiv.org/pdf/2602.17158", "abs": "https://arxiv.org/abs/2602.17158", "authors": ["Jiace Sun", "Garnet Kin-Lic Chan"], "title": "Stochastic tensor contraction for quantum chemistry", "categories": ["physics.chem-ph", "cond-mat.str-el", "physics.comp-ph", "quant-ph"], "comment": null, "summary": "Many computational methods in ab initio quantum chemistry are formulated in terms of high-order tensor contractions, whose cost determines the size of system that can be studied. We introduce stochastic tensor contraction to perform such operations with greatly reduced cost, and present its application to the gold-standard quantum chemistry method, coupled cluster theory with up to perturbative triples. For total energy errors more stringent than chemical accuracy, we reduce the computational scaling to that of mean-field theory, while starting to approach the mean-field absolute cost, thereby challenging the existing cost-to-accuracy landscape. Benchmarks against state-of-the-art local correlation approximations further show that we achieve an order-of-magnitude improvement in both total computation time and error, with significantly reduced sensitivity to system dimensionality and electron delocalization. We conclude that stochastic tensor contraction is a powerful computational primitive to accelerate a wide range of quantum chemistry."}
{"id": "2602.20757", "pdf": "https://arxiv.org/pdf/2602.20757", "abs": "https://arxiv.org/abs/2602.20757", "authors": ["Chetan Singh", "Harish Kumar", "Deepak Bhoriya", "Dinshaw S. Balsara"], "title": "Entropy stable numerical schemes for divergence diminishing Chew, Goldberger & Low equations for plasma flows", "categories": ["physics.plasm-ph", "math-ph", "math.NA"], "comment": null, "summary": "Chew, Goldberger & Low (CGL) equations are a set of hyperbolic PDEs with non-conservative products used to model the plasma flows, when the assumption of local thermodynamic equilibrium is not valid, and the pressure tensor is assumed to be rotated by the magnetic field. This results in the pressure tensor, which is described by the two scalar components. As the magnetic field also evolves, controlling the divergence of the magnetic field is important. In this work, we consider the generalized Lagrange multiplier (GLM) technique for the CGL model. The resulting model is referred to as the GLM-CGL system. To make the system suitable for entropy-stable schemes, we reformulate the GLM-CGL system by treating some conservative terms as non-conservative. The resulting system has a non-conservative part that does not affect entropy evolution. We then propose entropy stable numerical methods for the GLM-CGL model. The numerical results for the GLM-CGL system are then compared with the CGL system without the GLM divergence diminishing approach to demonstrate that the GLM approach indeed leads to significant improvement in the magnetic field divergence diminishing."}
{"id": "2602.20177", "pdf": "https://arxiv.org/pdf/2602.20177", "abs": "https://arxiv.org/abs/2602.20177", "authors": ["Aniruddha Bora", "Isabel K. Alvarez", "Julie Chalfant", "Chryssostomos Chryssostomidis"], "title": "Enhancing Heat Sink Efficiency in MOSFETs using Physics Informed Neural Networks: A Systematic Study on Coolant Velocity Estimation", "categories": ["cs.NE", "cs.AI", "cs.CE", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "In this work, we present a methodology using Physics Informed Neural Networks (PINNs) to determine the required velocity of a coolant, given inlet and outlet temperatures for a given heat flux in a multilayered metal-oxide-semiconductor field-effect transistor (MOSFET). MOSFETs are integral components of Power Electronic Building Blocks (PEBBs) and experiences the majority of the thermal load. Effective cooling of MOSFETs is therefore essential to prevent overheating and potential burnout. Determining the required velocity for the purpose of effective cooling is of importance but is an ill-posed inverse problem and difficult to solve using traditional methods. MOSFET consists of multiple layers with different thermal conductivities, including aluminum, pyrolytic graphite sheets (PGS), and stainless steel pipes containing flowing water. We propose an algorithm that employs sequential training of the MOSFET layers in PINNs. Mathematically, the sequential training method decouples the optimization of each layer by treating the parameters of other layers as constants during its training phase. This reduces the dimensionality of the optimization landscape, making it easier to find the global minimum for each layer's parameters and avoid poor local minima. Convergence of the PINNs solution to the analytical solution is theoretically analyzed. Finally we show the prediction of our proposed methodology to be in good agreement with experimental results."}
{"id": "2602.20757", "pdf": "https://arxiv.org/pdf/2602.20757", "abs": "https://arxiv.org/abs/2602.20757", "authors": ["Chetan Singh", "Harish Kumar", "Deepak Bhoriya", "Dinshaw S. Balsara"], "title": "Entropy stable numerical schemes for divergence diminishing Chew, Goldberger & Low equations for plasma flows", "categories": ["physics.plasm-ph", "math-ph", "math.NA"], "comment": null, "summary": "Chew, Goldberger & Low (CGL) equations are a set of hyperbolic PDEs with non-conservative products used to model the plasma flows, when the assumption of local thermodynamic equilibrium is not valid, and the pressure tensor is assumed to be rotated by the magnetic field. This results in the pressure tensor, which is described by the two scalar components. As the magnetic field also evolves, controlling the divergence of the magnetic field is important. In this work, we consider the generalized Lagrange multiplier (GLM) technique for the CGL model. The resulting model is referred to as the GLM-CGL system. To make the system suitable for entropy-stable schemes, we reformulate the GLM-CGL system by treating some conservative terms as non-conservative. The resulting system has a non-conservative part that does not affect entropy evolution. We then propose entropy stable numerical methods for the GLM-CGL model. The numerical results for the GLM-CGL system are then compared with the CGL system without the GLM divergence diminishing approach to demonstrate that the GLM approach indeed leads to significant improvement in the magnetic field divergence diminishing."}
{"id": "2602.20697", "pdf": "https://arxiv.org/pdf/2602.20697", "abs": "https://arxiv.org/abs/2602.20697", "authors": ["Vladimír Lukeš", "Eduard Rohan"], "title": "Reduced-order computational homogenization for hyperelastic media using gradient based sensitivity analysis of microstructures", "categories": ["math.NA", "math.AP"], "comment": null, "summary": "We propose an algorithm for the computational homogenization of locally periodic hyperelastic structures undergoing large deformations due to external quasi-static loading. The algorithm performs clustering of macroscopic deformations into subsets called \"centroids\", and, as a new ingredient, approximates the homogenized coefficients using sensitivity analysis of micro-configurations with respect to the macroscopic deformation. The novel \"model-order reduction\" approach significantly reduces the number of microscopic problems that must be solved in nonlinear simulations, thereby accelerating the overall computational process. The degree of reduction can be controlled by a user-defined error tolerance parameter. The algorithm is implemented in the finite element framework SfePy, and its performance effectiveness is demonstrated using two-dimensional test examples, when compared with solutions obtained by the proper orthogonal decomposition method, and by the full \"FE-square\" simulations. Extensions beyond the present implementations and the scope of tractable problems are discussed."}
