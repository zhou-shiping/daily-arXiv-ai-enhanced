<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 11]
- [math.AP](#math.AP) [Total: 17]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 10]
- [eess.IV](#eess.IV) [Total: 1]
- [math.FA](#math.FA) [Total: 3]
- [cs.LG](#cs.LG) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 4]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [An entropy-stable oscillation-eliminating dgsem for the euler equations on curvilinear meshes](https://arxiv.org/abs/2602.16732)
*Jielin Yang,Guosheng Fu*

Main category: math.NA

TL;DR: An entropy-stable high-order DG method for 2D compressible Euler equations on curvilinear meshes, combining SBP-DGSEM with modified OEDG for shock capturing.


<details>
  <summary>Details</summary>
Motivation: To develop robust, high-order numerical methods for compressible flows on complex geometries that maintain entropy stability while effectively controlling spurious oscillations near discontinuities.

Method: Nodal discontinuous Galerkin spectral element method (DGSEM) with summation-by-parts (SBP) property, enhanced with a modified oscillation-eliminating discontinuous Galerkin (OEDG) method reformulated using projection operators for general curvilinear meshes.

Result: The method achieves global discrete entropy inequality, preserves conservation and entropy stability, effectively suppresses spurious oscillations, and reduces computational cost through localized oscillation control using the OEDG damping coefficient as a shock indicator.

Conclusion: The proposed entropy-stable OEDG method demonstrates accuracy, robustness, and effectiveness on both Cartesian and curvilinear meshes for challenging compressible flow problems.

Abstract: We develop an entropy-stable high-order numerical method for the two-dimensional compressible Euler equations on general curvilinear meshes. The proposed approach is based on a nodal discontinuous Galerkin spectral element method (DGSEM) that satisfies the summation-by-parts (SBP) property. At the semidiscrete level, entropy stability is established through the SBP structure and the discrete metric identities associated with curvilinear coordinate mappings. By incorporating entropy-stable numerical fluxes at element interfaces, a global discrete entropy inequality is obtained. To further control nonphysical oscillations near strong discontinuities, the entropy-stable DG formulation is combined with a modified oscillation-eliminating discontinuous Galerkin (OEDG) method, which was originally proposed in [59]. We observe that the zero-order damping coefficient in the original OEDG method naturally serves as an effective shock indicator, which enables localization of the oscillation control mechanism and significantly reduces computational cost. Moreover, while the original OEDG formulation relies on local orthogonal modal bases and is primarily restricted to simplicial meshes, we reformulate the OE procedure using projection operators, allowing for a systematic extension to general curvilinear meshes. The resulting method preserves conservation and entropy stability while effectively suppressing spurious oscillations. A series of challenging numerical experiments is presented to demonstrate the accuracy, robustness, and effectiveness of the proposed entropy-stable OEDG method on both Cartesian and curvilinear meshes.

</details>


### [2] [Fast, High-Accuracy, Randomized Nullspace Computations for Tall Matrices](https://arxiv.org/abs/2602.16797)
*Ethan N. Epperly,Taejun Park,Yuji Nakatsukasa*

Main category: math.NA

TL;DR: RLobPCG is a randomized preconditioned eigensolver for computing smallest singular values of large tall matrices, combining sketching with LOBPCG for up to 12x speedup over classical methods.


<details>
  <summary>Details</summary>
Motivation: Need efficient method to compute smallest singular triplets of large, tall matrices where classical iterative methods (LOBPCG, Lanczos) are slow or fail to converge.

Method: Combines randomized sketching (sketch-and-precondition) with LOBPCG eigensolver: uses small sketch to construct high-quality preconditioner, then runs LOBPCG on Gram matrix to refine singular vectors.

Result: Proves geometric convergence under subspace embedding assumption and modest singular value gap. Numerical experiments show near-optimal accuracy on matrices up to 10^6 rows, 12x speedup over classical methods, and robustness where others fail.

Conclusion: RLobPCG is an efficient, robust method for computing smallest singular values of large matrices, significantly outperforming existing iterative solvers while maintaining accuracy.

Abstract: In this paper, we develop RLOBPCG, an efficient method for computing a small number of singular triplets corresponding to the smallest singular values of large, tall matrices. The algorithm combines randomized preconditioner from the sketch-and-precondition techniques with the LOBPCG eigensolver: a small sketch is used to construct a high-quality preconditioner, and LOBPCG is run on the Gram matrix to refine the singular vector. Under the standard subspace embedding assumption and a modest singular value gap between the two smallest singular values, we prove that RLOBPCG converges geometrically to the minimum singular vector. In numerical experiments, RLOBPCG achieves near-optimal accuracy on matrices with up to $10^6$ rows, outperforming classical LOBPCG and Lanczos methods by a speedup of up to $12\times$ and maintaining robustness when other iterative methods fail to converge.

</details>


### [3] [Domain Decomposition for Mean Curvature Flow of Surface Polygonal Meshes](https://arxiv.org/abs/2602.16874)
*Lenka Ptackova,Michal Outrata*

Main category: math.NA

TL;DR: Domain decomposition enables more efficient mean curvature flow for polygonal surface meshes by solving smaller parallelizable subproblems.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency of mean curvature flow for arbitrary polygonal surface meshes by leveraging parallel processing capabilities through domain decomposition.

Method: Test traditional domain decomposition methods (with/without overlap), present adapted Robin transmission conditions from optimized Schwarz method, decompose mesh into two sub-meshes to solve smaller boundary value problems in parallel.

Result: Enables solving two smaller problems instead of one large problem with near-complete parallel processing, while analyzing smoothing effects on shape quality and texture deformation.

Conclusion: Domain decomposition provides an efficient approach for mean curvature flow of polygonal surface meshes by enabling parallel computation while maintaining geometric quality.

Abstract: We examine the use of domain decomposition for potentially more efficient mean curvature flow of surface meshes, whose faces are arbitrary simple polygons. We first test traditional domain decomposition methods with and without overlap of deconstructed domains. And we present adapted Robin transmission conditions of optimized Schwarz method. We then analyze the resulting smoothing from the point of view of shape quality and texture deformation. By decomposing the initial mesh into two sub-meshes, we solve two smaller boundary value problems instead of one big problem, and we can process these two tasks almost entirely in parallel.

</details>


### [4] [ARCANE: Scalable high-degree cubature formulae for simulating SDEs without Monte Carlo error](https://arxiv.org/abs/2602.17151)
*Peter Koepernik,Thomas Coxon,James Foster*

Main category: math.NA

TL;DR: ARCANE algorithm automatically constructs high-degree cubature formulae for SDEs, achieving D=19 and outperforming Monte Carlo with orders of magnitude smaller error.


<details>
  <summary>Details</summary>
Motivation: Monte Carlo sampling for SDEs requires huge sample sizes for accuracy. Existing cubature methods are limited to low degree (D=7) and difficult to construct manually.

Method: ARCANE algorithm efficiently and automatically constructs cubature formulae of arbitrary degree by matching Brownian signature moments, using deterministic weighted sets of paths instead of random sampling.

Result: Reproduces state-of-the-art (D=7) in seconds, reaches D=19 within hours on modest hardware. Achieves orders of magnitude smaller error than Monte Carlo with same number of paths across multiple SDEs and error metrics.

Conclusion: ARCANE enables practical high-degree cubature for SDEs, providing a superior alternative to Monte Carlo sampling with dramatically reduced computational requirements.

Abstract: Monte Carlo sampling is the standard approach for estimating properties of solutions to stochastic differential equations (SDEs), but accurate estimates require huge sample sizes. Lyons and Victoir (2004) proposed replacing independently sampled Brownian driving paths with "cubature formulae", deterministic weighted sets of paths that match Brownian "signature moments" up to some degree $D$. They prove that cubature formulae exist for arbitrary $D$, but explicit constructions are difficult and have only reached $D=7$, too small for practical use. We present ARCANE, an algorithm that efficiently and automatically constructs cubature formulae of arbitrary degree. It reproduces the state of the art in seconds and reaches $\boldsymbol{D=19}$ within hours on modest hardware. In simulations across multiple different SDEs and error metrics, our cubature formulae robustly achieve an error orders of magnitude smaller than Monte Carlo with the same number of paths.

</details>


### [5] [Invertibility of the Fourier Diffraction Relation in Raster Scan Diffraction Tomography](https://arxiv.org/abs/2602.17344)
*Peter Elbau,Noemi Naujoks*

Main category: math.NA

TL;DR: The paper analyzes Fourier diffraction tomography with focused beams, showing that in dimensions >2, all Fourier coefficients are uniquely recoverable, but in 2D, only partial Fourier coverage is uniquely determined.


<details>
  <summary>Details</summary>
Motivation: Practical imaging systems use focused beams instead of plane waves, creating different Fourier diffraction relations where some measurements correspond to linear combinations of scattering potential coefficients rather than individual ones.

Method: Theoretical analysis of Fourier diffraction theorem adapted to focused beam illumination, investigating which Fourier coefficients of scattering potential can be uniquely recovered from the modified measurement relations.

Result: In dimensions higher than two, all Fourier coefficients appearing in the equations are typically uniquely determined. In two dimensions, only part of the Fourier coverage is uniquely recoverable, while distinct coefficients can produce identical data on the remaining subset.

Conclusion: The dimensionality affects unique recoverability in diffraction tomography with focused beams: higher dimensions enable full unique recovery, while 2D suffers from partial ambiguity in Fourier coefficient determination.

Abstract: Diffraction tomography aims to recover an object's scattering potential from measured wave fields. In the classical setting, the object is illuminated by plane waves from many directions, and the Fourier diffraction theorem gives a direct relation between the Fourier transform of the scattering potential of the object and the Fourier transformed measurements.
  In many practical imaging systems, however, focused beams are used instead of plane waves. These beams are then translated across the object to bring different regions of interest into focus. The Fourier diffraction relation adapted to this setting differs in one crucial point from the plane-wave case: while certain Fourier coefficients of the measurements still directly correspond to individual Fourier coefficients of the scattering potential, others are given by linear combinations of two Fourier coefficients of the scattering potential.
  This article investigates which Fourier coefficients of the scattering potential can be uniquely recovered from these relations. We show that in dimensions higher than two, all coefficients appearing in the equations are typically uniquely determined. In two dimensions, however, only part of the Fourier coverage is uniquely recoverable, while on the remaining subset, distinct coefficients can produce identical data.

</details>


### [6] [Analysis of an exponential integrator for stochastic PDEs driven by Riesz noise](https://arxiv.org/abs/2602.17348)
*Charles-Edouard Bréhier,David Cohen,Lluís Quer-Sardanyons,Johan Ulander*

Main category: math.NA

TL;DR: Explicit exponential integrator for parabolic SPDEs with Riesz kernel noise, with strong error analysis and numerical validation.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze an explicit exponential integrator for solving parabolic stochastic partial differential equations (SPDEs) driven by Gaussian noise that is white in time with spatially correlated Riesz kernel structure, which is relevant for modeling various physical phenomena with long-range spatial correlations.

Method: An explicit exponential integrator numerical scheme for parabolic SPDEs with Gaussian noise (white in time) and spatial correlation given by a Riesz kernel. The method is analyzed theoretically to establish strong error bounds and convergence rates that depend on the Riesz kernel exponent.

Result: Strong error bounds are proven for the explicit exponential integrator, with convergence rates explicitly depending on the exponent in the Riesz kernel. Numerical experiments in spatial dimensions 1 and 2 confirm the theoretical convergence results.

Conclusion: The proposed explicit exponential integrator is effective for parabolic SPDEs with Riesz kernel noise, with proven convergence rates that depend on the spatial correlation structure, and numerical validation supports the theoretical analysis.

Abstract: We present and study an explicit exponential integrator for parabolic SPDEs in any dimension driven by a Gaussian noise which is white in time and with spatial correlation given by a Riesz kernel. Under assumptions on the coefficients of the SPDE, we prove strong error bounds and exhibit how the rate of convergence depends on the exponent in the Riesz kernel. Finally, numerical experiments in spatial dimensions $1$ and $2$ are provided in order to confirm our convergence results.

</details>


### [7] [Raster Scan Diffraction Tomography](https://arxiv.org/abs/2602.17351)
*Peter Elbau,Noemi Naujoks,Otmar Scherzer*

Main category: math.NA

TL;DR: Extends diffraction tomography to incorporate focused beam scanning, deriving new Fourier diffraction relation for quantitative reconstruction from scanning data.


<details>
  <summary>Details</summary>
Motivation: Conventional diffraction tomography assumes monochromatic plane wave illumination, which fails to reflect practical imaging systems like medical ultrasound that use focused beams for scanning. Current measurement setups combining focused illumination with scanning haven't been incorporated into diffraction tomography framework.

Method: Extends diffraction tomography by modeling incident fields as Herglotz waves to incorporate focused beams. Derives a new Fourier diffraction relation as basis for quantitative tomographic reconstruction from scanning data.

Result: Develops theoretical framework for diffraction tomography with focused beam scanning. Systematically analyzes how different scan geometries influence reconstruction.

Conclusion: Bridges gap between conventional diffraction tomography and practical imaging systems by incorporating focused beam scanning, enabling more realistic quantitative reconstruction for applications like medical ultrasound.

Abstract: Diffraction tomography is a widely used inverse scattering technique for quantitative imaging of weakly scattering media. In its conventional formulation, diffraction tomography assumes monochromatic plane wave illumination. This assumption, however, represents a simplification that often fails to reflect practical imaging systems such as medical ultrasound, where focused beams are used to scan a region of interest of the human body. Such measurement setups, combining focused illumination with scanning, have not yet been incorporated into the diffraction tomography framework. To bridge this gap, we extend diffraction tomography by modeling incident fields as Herglotz waves, thereby incorporating focused beams into the theory. Within this setting, we derive a new Fourier diffraction relation, which forms the basis for quantitative tomographic reconstruction from scanning data. Using this result, we systematically analyze how different scan geometries influence the reconstruction.

</details>


### [8] [Application and Evaluation of the Common Circles Method](https://arxiv.org/abs/2602.17353)
*Michael Quellmalz,Mia Kvåle Løvmo,Simon Moser,Franziska Strasser,Monika Ritsch-Marte*

Main category: math.NA

TL;DR: The paper presents a practical implementation of the common circle method for estimating sample motion in optical diffraction tomography of biological tissue, using temporal consistency constraints for stable reconstructions.


<details>
  <summary>Details</summary>
Motivation: When biological tissue samples are confined using contact-free acoustical force fields in optical diffraction tomography, their motion needs to be estimated from captured images for accurate reconstruction.

Method: The common circle method identifies intersections of Ewald spheres in Fourier space to determine rotational motion, with practical implementation incorporating temporal consistency constraints for stability.

Result: Results on both simulated and real-world data demonstrate that the common circle method provides a computationally efficient alternative to full optimization methods for motion detection.

Conclusion: The common circle method offers an effective and efficient approach for motion estimation in optical diffraction tomography of acoustically confined biological tissue samples.

Abstract: We investigate the application of the common circle method for estimating sample motion in optical diffraction tomography (ODT) of sub-millimeter sized biological tissue. When samples are confined via contact-free acoustical force fields, their motion must be estimated from the captured images. The common circle method identifies intersections of Ewald spheres in Fourier space to determine rotational motion. This paper presents a practical implementation, incorporating temporal consistency constraints to achieve stable reconstructions. Our results on both simulated and real-world data demonstrate that the common circle method provides a computationally efficient alternative to full optimization methods for motion detection.

</details>


### [9] [Functional Analysis and Parallel Domain Decomposition for the TV-Stokes Model](https://arxiv.org/abs/2602.17494)
*Andreas Langer,Marc Runft,Talal Rahman,Xue-Cheng Tai,Bin Wu*

Main category: math.NA

TL;DR: The paper establishes a rigorous functional-analytic foundation for the TV-Stokes image denoising model, identifies and resolves analytical inconsistencies in the original formulation, and develops the first parallelizable domain decomposition method for large-scale image processing.


<details>
  <summary>Details</summary>
Motivation: The TV-Stokes model is effective for image denoising but lacks rigorous mathematical analysis and parallelization capabilities. The authors aim to provide a solid functional-analytic foundation and develop scalable algorithms for large-scale applications.

Method: 1) Formulate both steps of TV-Stokes in appropriate infinite-dimensional function spaces with dual formulations; 2) Analyze compatibility and mathematical consistency of the coupled system; 3) Develop domain decomposition method using overlapping Schwarz-type iterations applied to dual formulations; 4) Show divergence-free constraint becomes locally computable in discrete setting.

Result: 1) Established rigorous functional-analytic foundation for TV-Stokes; 2) Identified and resolved analytical inconsistencies in original formulation; 3) Proved existence of orthogonal projection onto divergence-free subspace; 4) Developed first domain decomposition method for TV-Stokes enabling fully parallelizable algorithm; 5) Demonstrated correctness and usability in parallel image reconstruction through numerical experiments.

Conclusion: The work provides a solid mathematical foundation for TV-Stokes denoising, resolves previous analytical inconsistencies, and enables scalable parallel processing through novel domain decomposition methods, making the model suitable for large-scale image processing in memory-constrained environments.

Abstract: The TV-Stokes model is a two-step variational method for image denoising that combines the estimation of a divergence-free tangent field with total variation regularization in the first step and then uses that to reconstruct the image in the second step. Although effective in practice, its mathematical structure and potential for parallelization have remained unexplored. In this work, we establish a rigorous functional-analytic foundation for the TV-Stokes model. We formulate both steps in appropriate infinite-dimensional function spaces, derive their dual formulations, and analyze the compatibility and mathematical consistency of the coupled system. In particular, we identify analytical inconsistencies in the original formulation and demonstrate how an alternative model resolves them. We also examine the orthogonal projection onto the divergence-free subspace, proving its existence in a continuous setting and establishing consistency with its discrete counterpart.
  Building on this theoretical framework, we develop the first domain decomposition method for TV-Stokes by applying overlapping Schwarz-type iterations to the duals of both steps. Although the divergence-free constraint gives rise to a global projection operator in the continuous model, we show that it becomes locally computable in the discrete setting. This insight enables a fully parallelizable algorithm suitable for large-scale image processing in memory-constrained environments. Numerical experiments demonstrate the correctness of the domain decomposition approach and its usability in parallel image reconstruction.

</details>


### [10] [High Order semi-implicit Rosenbrock type and Multistep methods for evolutionary partial differential equations with higher order derivatives](https://arxiv.org/abs/2602.17507)
*Boscarino Sebastiano,Giuseppe Izzo*

Main category: math.NA

TL;DR: Semi-implicit Rosenbrock/IMEX methods for high-order PDEs avoid Newton iterations and severe explicit time-step restrictions, achieving up to 4th-order accuracy with finite difference spatial discretization.


<details>
  <summary>Details</summary>
Motivation: High-order PDEs with spatial derivatives require stable time integration without severe time-step restrictions (Δt = O(Δx^k)) that explicit methods impose, and without complex Newton iterations.

Method: Semi-implicit strategy within Rosenbrock-type and IMEX linear multistep frameworks combined with finite difference spatial discretization, creating linearly implicit schemes without Newton iterations.

Result: Developed methods up to order p=4 that are stable and achieve expected accuracy orders for dissipative, dispersive, and biharmonic-type equations, avoiding severe time-step restrictions.

Conclusion: The semi-implicit approach provides flexible, stable, and efficient time integration for high-order PDEs without Newton iterations or restrictive explicit time-step constraints.

Abstract: The aim of this work is to apply a semi-implicit (SI) strategy within a Rosenbrock-type and IMEX linear multistep (LM) framework to a sequence of 1D time-dependent partial differential equations (PDEs) with high order spatial derivatives. This strategy provides great flexibility to treat these equations, and allows the construction of simple lienarly implicit schemes without any Newton iteration. Furthermore, the SI schemes so designed do not require the severe time-step restrictions typically encountered when using explicit methods for stability, i.e., $Δt = \mathcal{O}(Δx^k)$ for the $k$-th order PDEs with $k\ge 2$. For space discrertization, this strategy is combined with finite difference schemes. We provide example of methods up to order $p = 4$, and we illustrate the effectiveness of the schemes with appllications to dissipative, dispersive, and biharmonic-type equations. Numerical experiments show that the proposed schemes are stable and achieve the expected orders of accuracy.

</details>


### [11] [Computing the action of a matrix exponential on an interval via the $\star$-product approach](https://arxiv.org/abs/2602.17516)
*Stefano Pozza,Shazma Zahid*

Main category: math.NA

TL;DR: New method for computing matrix exponential action e^{At}v using orthogonal polynomial series expansion over bounded intervals, based on representation in $\star$-algebra of bivariate distributions.


<details>
  <summary>Details</summary>
Motivation: Need efficient method to compute action of matrix exponential on vector for all t within bounded intervals, improving upon existing state-of-the-art techniques.

Method: Expands solution into orthogonal polynomial series, uses new representation of matrix exponential in $\star$-algebra (algebra of bivariate distributions), leads to linear system equivalent to Stein-type matrix equation solvable by direct or Krylov subspace methods.

Result: Numerical experiments demonstrate accuracy and efficiency improvements compared to state-of-the-art techniques.

Conclusion: Proposed method provides effective approach for computing matrix exponential action over bounded intervals using orthogonal polynomial expansion and $\star$-algebra representation.

Abstract: We present a new method for computing the action of the matrix exponential on a vector, $e^{At}v$. The proposed approach efficiently evaluates the solution for all $t$ within a prescribed bounded interval by expanding it into an orthogonal polynomial series. This method is derived from a new representation of the matrix exponential in the so-called $\star$-algebra, an algebra of bivariate distributions. The resulting formulation leads to a linear system equivalent to a matrix equation of Stein type, which can be solved by either direct or Krylov subspace methods. Numerical experiments demonstrate the accuracy and efficiency of the proposed approach in comparison with state-of-the-art techniques.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [12] [Inverse problems in electrolysers](https://arxiv.org/abs/2602.16906)
*Giovanni S. Alberti,Wadim Gerner,Matteo Santacesaria*

Main category: math.AP

TL;DR: Paper investigates relationship between electric potential in electrolyser cells and temperature/particle concentrations, identifying required measurements for unique potential reconstruction.


<details>
  <summary>Details</summary>
Motivation: Understanding how electric potential in electrolyser cells depends on temperature and particle concentrations is crucial for optimizing electrolysis processes and improving cell performance.

Method: Theoretical analysis to identify the specific measurements needed to uniquely reconstruct the electric potential function φ as a function of temperature and concentration variables.

Result: Identification of required measurement parameters that enable unique reconstruction of the electric potential function in electrolyser cells.

Conclusion: The paper establishes the measurement requirements for determining the functional relationship between electric potential and temperature/concentration in electrolyser cells.

Abstract: We investigate the relationship between the electric potential within an electrolyser cell and its temperature and particle concentrations. Specifically, we identify the measurements required to uniquely reconstruct the potential $φ$ as a function of temperature and concentration.

</details>


### [13] [Self-similar extinction for a fast diffusion equation with weighted absorption](https://arxiv.org/abs/2602.16940)
*Razvan Gabriel Iagar,Diana-Rodica Munteanu*

Main category: math.AP

TL;DR: The paper studies finite time extinction for bounded solutions to a fast diffusion equation with spatially inhomogeneous absorption, proving extinction occurs in contrast to standard cases where solutions remain positive.


<details>
  <summary>Details</summary>
Motivation: To understand how spatially inhomogeneous absorption affects the extinction behavior of solutions to fast diffusion equations, particularly contrasting with the known positivity of solutions in the standard case (σ=0).

Method: Analysis of the fast diffusion equation with inhomogeneous absorption term |x|^σ u^p, establishing finite time extinction for bounded solutions under specific parameter conditions, and constructing self-similar solutions with explicit scaling forms.

Result: Proves finite time extinction for bounded solutions when p>1, m_c < m < 1, and σ > σ_*, and establishes existence of self-similar solutions with specific scaling properties, including both bounded and unbounded types.

Conclusion: Spatially inhomogeneous absorption (σ > 0) fundamentally changes the extinction behavior of fast diffusion equations, causing finite time extinction in contrast to the eternal positivity observed in the homogeneous case.

Abstract: Finite time extinction of any bounded solution to the fast diffusion equation with spatially inhomogeneous absorption $$ \partial_tu=Δu^m-|x|^σu^p, \quad (x,t)\in\mathbb{R}^N\times(0,\infty), $$ with $N\geq1$ and exponents $$ p>1, \quad m_c=\frac{(N-2)_+}{N}<m<1, \quad σ>σ_*:=\frac{2(p-1)}{1-m}, $$ is established. Moreover, the existence of self-similar solutions of the form $$ U(x,t)=(T-t)^αf(|x|(T-t)^β), \quad α=\frac{σ+2}{(1-m)(σ-σ_*)}, \ β=\frac{p-m}{(1-m)(σ-σ_*)}, $$ with $f(0)>0$, $f'(0)=0$ and $$ \lim\limits_{ξ\to\infty}ξ^{(σ+2)/(p-m)}f(ξ)=L\in(0,\infty). $$ is proved, together with some unbounded self-similar solutions as well. The property of finite time extinction is in striking contrast to the standard fast diffusion equation with absorption (that is, $σ=0$), where the strict positivity of solutions for any $t\in(0,\infty)$ is well-known.

</details>


### [14] [Divergence-form equations admitting nowhere $C^1$ Lipschitz weak solutions](https://arxiv.org/abs/2602.17012)
*Menglan Liao,Baisheng Yan*

Main category: math.AP

TL;DR: The paper shows that divergence-form PDEs can have highly irregular Lipschitz weak solutions that are nowhere C¹, using convex integration techniques adapted from diffusion equations.


<details>
  <summary>Details</summary>
Motivation: To understand the existence of highly irregular weak solutions for divergence-form PDEs, particularly whether such equations can admit Lipschitz solutions that fail to be C¹ anywhere, extending results from diffusion equations to time-independent settings.

Method: Reformulate divergence-form equations as first-order PDE relations, then adapt the convex integration scheme from GKY26 for irregular diffusion equations. Use the same structural Condition O_N and construct new building blocks from wave cones and T_N-configurations employed in diffusion equation analysis.

Result: For divergence-form equations satisfying Condition O_N, there exist Lipschitz weak solutions that are nowhere C¹ in bounded domains with suitable boundary data. Specifically, for smooth strongly polyconvex functions on ℝ^{2×n} (n≥2), the associated Euler-Lagrange equations admit such irregular solutions with zero boundary conditions.

Conclusion: The same structural condition that ensures irregular solutions for diffusion equations also works for time-independent divergence-form PDEs, demonstrating the existence of highly irregular Lipschitz weak solutions through convex integration techniques.

Abstract: We study a class of partial differential equations in divergence form that admit highly irregular Lipschitz weak solutions. By reformulating these divergence-form equations as a first-order partial differential relation and adapting the convex integration scheme recently developed in \cite{GKY26} for irregular diffusion equations, we show that the same structural Condition~$O_N$ introduced there also ensures the existence of Lipschitz weak solutions that are nowhere $C^1$ for the corresponding time-independent equations in bounded domains, under suitable boundary data. In particular, for the smooth strongly polyconvex functions on $\mathbb{R}^{2\times n}$ constructed in that paper for all $n \ge 2$, the associated Euler--Lagrange equations admit Lipschitz weak solutions that are nowhere $C^1$ and satisfy zero boundary conditions in any bounded domain of $\mathbb{R}^n$. Our approach relies on new building blocks constructed from the same wave cone and $\mathcal{T}_N$-configurations employed in the analysis of diffusion equations.

</details>


### [15] [Asymptotic stability of symmetric flows with viscous inflow boundary condition](https://arxiv.org/abs/2602.17059)
*Yan Guo,Zhuolun Yang*

Main category: math.AP

TL;DR: The paper studies 2D Navier-Stokes equations in a channel with small viscosity, constructs exact steady solutions near shear flows, and develops a weighted vorticity energy method to prove enhanced dissipation and nonlinear stability in different channel length regimes.


<details>
  <summary>Details</summary>
Motivation: To understand stability properties of shear flows in channels with small viscosity and Navier slip conditions, particularly how perturbations behave near symmetric base profiles that vanish on walls.

Method: Constructs exact steady solutions O(ε^{1/3})-close to shear flows, develops new weighted vorticity energy method for stability analysis, uses Rayleigh vorticity to control non-favorable terms in long channels.

Result: Proves uniform linear stability with enhanced dissipation (exponential decay on O(ε^{-1/3}) time scale), nonlinear asymptotic stability with threshold O(ε^{2/3}) for short channels, and nonlinear stability with threshold O(ε^{5/6+}) for long channels under concavity and spectral conditions.

Conclusion: The weighted vorticity energy method successfully establishes stability and enhanced dissipation for shear flows in channels with small viscosity, with different stability thresholds depending on channel length and additional structural conditions.

Abstract: We study the two-dimensional incompressible Navier-Stokes equations in a channel $Ω=(0,L)\times(0,H)$ with small viscosity $\varepsilon\ll1$, an $\varepsilon$-Navier slip condition on the horizontal walls, and a viscous inflow condition for the perturbation stream function. For a broad class of symmetric base profiles $u_0(y)$ vanishing on the walls, we construct an exact steady solution $(u_s,v_s)$ that is $O(\varepsilon^{1/3})$-close to the shear $(u_0,0)$. We then develop a new weighted vorticity energy method to prove uniform linear stability and enhanced dissipation: perturbations decay exponentially in a weighted $L^2$ norm on the time scale $O(\varepsilon^{-1/3})$. In the short-channel regime $L\ll1$, the method yields nonlinear asymptotic stability with threshold $O(\varepsilon^{2/3})$. In the long-channel regime, assuming concavity together with a spectral condition, we introduce a quantity \textit{Rayleigh vorticity} to control the non-favorable terms and obtain nonlinear stability with threshold $O(\varepsilon^{5/6+})$.

</details>


### [16] [On sliding methods for mixed local and nonlocal equations and Gibbons' conjecture](https://arxiv.org/abs/2602.17069)
*Yinbin Deng,Pengyan Wang,Zhihao Wang,Leyun Wu*

Main category: math.AP

TL;DR: Develops refined sliding method for mixed local-nonlocal operators, establishes new inequalities and principles, proves monotonicity/symmetry results, resolves Gibbons' conjecture for mixed fractional equations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of analyzing equations with mixed local and nonlocal operators that have different nonlocal structures and incompatible scaling properties, which obstruct classical sliding methods. This motivates developing new techniques specifically tailored for mixed operators.

Method: Develops a refined sliding method for mixed local-nonlocal operators. Establishes new generalized weighted average inequalities, narrow region principles, and maximum principles in bounded/unbounded domains. Uses these tools to analyze elliptic and parabolic equations with mixed operators.

Result: Derives monotonicity and one-dimensional symmetry results for mixed elliptic equations in bounded domains, half-spaces, and whole space. Extends analysis to parabolic equations with mixed time derivatives. Resolves Gibbons' conjecture for a class of mixed fractional equations.

Conclusion: The paper successfully develops a refined sliding method and establishes new analytical tools that overcome the challenges posed by mixed local-nonlocal operators, enabling rigorous analysis of both elliptic and parabolic equations with such mixed operators and resolving important conjectures in the field.

Abstract: We investigate elliptic and parabolic equations involving mixed local and nonlocal operators of the form $(-Δ)^s-Δ$, as well as their parabolic counterparts with both the Marchaud fractional time derivative and the classical first-order derivative.
  A major difficulty in this setting stems from the coexistence of operators with different nonlocal structures and incompatible scaling properties, which obstruct the direct use of classical sliding methods. To address this issue, we develop a refined sliding method suited to mixed local-nonlocal operators. As key technical ingredients, we establish new generalized weighted average inequalities, narrow region principles, and maximum principles in bounded and unbounded domains.
  These tools enable us to derive monotonicity and one-dimensional symmetry results for mixed elliptic equations in bounded domains, half-spaces, and the whole space, and to extend the analysis to parabolic equations with mixed time derivatives. As an application, we resolve the Gibbons' conjecture for a class of mixed fractional equations.

</details>


### [17] [Landau-de Gennes Energy with Weak Planar Anchoring](https://arxiv.org/abs/2602.17233)
*Ho Man Tai,Yong Yu*

Main category: math.AP

TL;DR: Study of Landau-de Gennes energy minimizers for nematic liquid crystals in 3D domains with weak planar anchoring, showing singular set is finite and analyzing boundary singularities.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical properties of nematic liquid crystal configurations in bounded 3D domains with degenerate boundary conditions, particularly the structure of singularities that arise in the limiting behavior as elastic constant tends to zero.

Method: Analyze global minimizers of Landau-de Gennes energy in simply connected bounded smooth 3D domains with weak planar anchoring. Study regime where elastic constant → 0 below nematic-isotropic transition threshold. Use harmonic map theory with strong tangential boundary constraint to analyze singular set.

Result: The singular set of the limiting minimizer within the closure of the domain is a finite set. The structure of singularities located on the boundary is investigated and characterized.

Conclusion: In the small elastic constant regime with weak planar anchoring, nematic liquid crystal minimizers exhibit finite singularities, with boundary singularities having specific mathematical structure, advancing understanding of defect formation in confined liquid crystals.

Abstract: We study global minimizers of the Landau-de Gennes energy for the nematic liquid crystals in simply connected, bounded, smooth domains of dimension 3, subject to a weak planar anchoring. The boundary condition is degenerate. In the regime where the elastic constant tends to zero and the temperature is below the nematic-isotropic transition threshold, the bulk and surface energy enforce the energy minimizers to be uniaxial, with director fields lying in the tangent plane to the boundary in the sense of trace. We establish that the singular set of the limiting minimizer within the closure of the domain is a finite set by studying the associated harmonic map with a strong tangential boundary constraint. The structure of the singularities located on the boundary is also investigated.

</details>


### [18] [Derivation of variational membrane models in the context of anisotropic nonlocal hyperelasticity](https://arxiv.org/abs/2602.17278)
*Dominik Engl,Anastasia Molchanova,Hidde Schönberger*

Main category: math.AP

TL;DR: The paper develops a theory for anisotropic nonlocal gradients to analyze thin structures, providing a unified framework that connects nonlocal, partially nonlocal, and local models through Γ-convergence analysis.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by analyzing thin structures and studying variational dimension reduction of hyperelastic energies involving nonlocal gradients to an effective membrane model. When rescaling thin domains, isotropic interaction ranges become anisotropic, necessitating development of anisotropic nonlocal gradient theory.

Method: The authors develop a theory for anisotropic nonlocal gradients with direction-dependent interaction ranges, using ellipsoidal interaction regions whose principal radii may vanish independently (unlike existing nonlocal derivatives with finite horizon defined via spherical kernels). This provides a unified framework that interpolates between fully nonlocal, partially nonlocal, and purely local models. They employ Γ-convergence analysis for nonlocal thin-film energies.

Result: The Γ-convergence analysis shows that the limit functional retains the structural form of the classical membrane energy. The classical local model is recovered precisely when all interaction radii vanish, demonstrating the framework's ability to connect nonlocal and local models.

Conclusion: The paper successfully develops a unified framework for anisotropic nonlocal gradients that bridges nonlocal and local models, with applications to thin structure analysis and dimension reduction of hyperelastic energies. The theory provides mathematical tools for analyzing materials with direction-dependent nonlocal interactions.

Abstract: Motivated by the analysis of thin structures, we study the variational dimension reduction of hyperelastic energies involving nonlocal gradients to an effective membrane model. When rescaling the thin domain, isotropic interaction ranges naturally become anisotropic, leading to the development of a theory for anisotropic nonlocal gradients with direction-dependent interaction ranges. Unlike existing nonlocal derivatives with finite horizon, which are defined via interaction kernels supported on balls of positive radius, our formulation is based on ellipsoidal interaction regions whose principal radii may vanish independently. This yields a unified framework that interpolates between fully nonlocal, partially nonlocal, and purely local models. Employing these tools, we present a $Γ$-convergence analysis for the nonlocal thin-film energies. The limit functional retains the structural form of the classical membrane energy, and the classical local model is recovered precisely when all interaction radii vanish.

</details>


### [19] [Singular convergence for semilinear wave equations with steep potential well](https://arxiv.org/abs/2602.17279)
*Martino Prizzi*

Main category: math.AP

TL;DR: Solutions of semilinear wave equation with deep potential well converge to solutions of wave equation on well bottom with Dirichlet boundary conditions as well depth approaches infinity.


<details>
  <summary>Details</summary>
Motivation: To understand the limiting behavior of wave equations with deep potential wells, connecting solutions in the whole space to solutions on constrained domains with boundary conditions.

Method: Analysis of semilinear wave equation in whole space with potential well, studying asymptotic behavior as well depth tends to infinity, proving convergence to boundary value problem.

Result: Proved convergence of solutions to wave equation defined on bottom of well with Dirichlet boundary conditions as well depth approaches infinity.

Conclusion: Deep potential wells effectively create constrained domains with boundary conditions in the limit, providing mathematical justification for modeling confined wave phenomena.

Abstract: We consider a semilinear wave equation in the whole space with a deep potential well. We prove that as the depth of the well tends to infinity, the solutions of the equation converge to the solutions of a wave equation defined on the bottom of the well, with Dirichlet condition on the boundary.

</details>


### [20] [Instability of two-pulse periodic waves with long wavelength in some Hamiltonian PDEs](https://arxiv.org/abs/2602.17317)
*Thomas Courant*

Main category: math.AP

TL;DR: The paper studies spectral instability of periodic waves in quasilinear KdV-type equations and compressible fluid models, particularly focusing on waves with both bright and dark soliton components as wavelength increases.


<details>
  <summary>Details</summary>
Motivation: To understand stability properties of periodic waves in generalized KdV equations and compressible fluid models, especially those exhibiting both bright and dark soliton characteristics in the long-wavelength limit.

Method: Combines two approaches: 1) asymptotic expansion of Hessian matrix of action integral to analyze instability when both limiting solitary waves are stable, and 2) studying convergence of spectrum as period goes to infinity using renormalized periodic Evans function when one solitary wave is unstable.

Result: Shows that periodic waves with two pulses (one converging to bright soliton, one to dark soliton) are spectrally unstable for sufficiently large periods.

Conclusion: The spectral instability of these periodic waves is established through complementary analytical methods, covering cases where both limiting solitary waves are stable or when one is unstable.

Abstract: We consider quasilinear generalizations of the Korteweg-de Vries equation and dispersive perturbations of the Euler equations for compressible fluids, either in Lagrangian or in Eulerian coordinates. In particular, our framework includes hydrodynamic formulation of the nonlinear Schrödinger equations. The periodic waves we study exhibit on each period two pulses, one converging to a bright soliton and one converging to a dark soliton, when wavelength goes to infinity. We show that such waves, for sufficiently large periods, are spectrally unstable. To do so, we combine two approaches. The first one is to calculate the asymptotic expansion of the Hessian matrix of the action integral and concludes using arXiv:1505.01382 as in arXiv:1710.03936 . This shows instability when both limiting solitary waves are stable. The second approach studies the convergence of the spectrum when the period goes to infinity and is applied in remaining cases, when one of the solitary waves is unstable. To carry out the latter, we prove the convergence of an appropriate renormalization of the periodic Evans function as in arXiv:1802.02830 .

</details>


### [21] [On the classical Reinforcement problem and Optimisation](https://arxiv.org/abs/2602.17331)
*Emanuele Cristoforoni,Carlo Nitsch,Cristina Trombetti*

Main category: math.AP

TL;DR: Survey paper analyzing classical reinforcement problems for elliptic boundary value problems, focusing on seminal works by Brezis, Caffarelli & Friedman and Acerbi & Buttazzo, and discussing related optimization problems.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive survey of classical reinforcement problems for elliptic boundary value problems, tracing the development from Sanchez-Palencia's 1969 work through key contributions by major researchers in the field.

Method: Literature review and analysis of seminal papers, focusing on mathematical formulations, theoretical approaches, and connections between different research contributions in reinforcement problems for elliptic PDEs.

Result: Systematic organization of classical reinforcement problem literature, highlighting connections between Sanchez-Palencia's foundational work, Brezis-Caffarelli-Friedman's contributions, and Acerbi-Buttazzo's approaches, along with related optimization problems.

Conclusion: The survey provides a coherent overview of the classical reinforcement problem landscape, showing how different research threads connect and contribute to the understanding of elliptic boundary value reinforcement problems and their optimization variants.

Abstract: In the present survey, we consider the classical reinforcement problem for elliptic boundary value problems originally studied by Sanchez-Palencia in 1969. We focus on the seminar papers by Brezis, Caffarelli, & Friedman, and by Acerbi & Buttazzo, and discuss the related optimisation problems proposed by Friedman and by Buttazzo.

</details>


### [22] [Asymptotic analysis for heterogeneous elastic energies with material voids](https://arxiv.org/abs/2602.17374)
*Stefano Almi,Antonio Flavio Donnarumma,Manuel Friedrich*

Main category: math.AP

TL;DR: Γ-convergence analysis of heterogeneous elastic energies with material voids shows effective limit contains additional surface term from displacement jumps, related to void collapse phenomenon.


<details>
  <summary>Details</summary>
Motivation: Study effective behavior of heterogeneous energies modeling material voids in geometrically linear elastic materials to understand how microscopic void structures affect macroscopic material properties.

Method: Use Γ-convergence analysis of functionals with bulk terms depending on symmetrized gradient of displacement and surface terms comparable to void surface area, as microscale ε→0.

Result: Γ-limit admits integral representation with additional surface term from displacement jumps outside void region, related to void collapse; under continuity assumption, limiting jump density is twice void energy density.

Conclusion: Microscopic voids in elastic materials lead to macroscopic surface energy contributions from displacement discontinuities, with specific quantitative relationship between void energy and jump energy densities.

Abstract: We study the effective behavior of heterogeneous energies arising in the modeling of material voids in geometrically linear elastic materials. Specifically, we consider functionals featuring bulk terms depending on the symmetrized gradient of the displacement and terms comparable to the surface area of the material voids inside the material. Under suitable growth conditions for the bulk and surface densities we prove that, as the microscale $\varepsilon$ tends to zero, the $Γ$-limit admits an integral representation that contains an additional surface term expressed by jump discontinuities of the displacement outside of the void region. This term is related to the phenomenon of collapsing of voids in the effective limit. Under a continuity assumption of the surface density at the $\varepsilon$-scale, we show that the limiting density related to jumps is twice the energy density for voids.

</details>


### [23] [Anisotropic Maximal $L^p$-regularity Estimates for a Hypoelliptic Operator](https://arxiv.org/abs/2602.17378)
*Kazuhiro Hirao*

Main category: math.AP

TL;DR: The paper proves anisotropic maximal regularity for a specific Vlasov-Fokker-Planck equation involving Ornstein-Uhlenbeck operators, using pointwise estimates of the fundamental solution.


<details>
  <summary>Details</summary>
Motivation: To establish maximal regularity properties for Vlasov-Fokker-Planck equations, which are important in kinetic theory and statistical physics, particularly for understanding the behavior of particle systems with friction and diffusion.

Method: The authors analyze the operator $\mathcal{A}=Δ_{y}u-y\cdot \nabla_x{u}$ as an Ornstein-Uhlenbeck operator. They prove anisotropic maximal regularity estimates by establishing similar estimates and weak (1,1) estimates for $L=\partial_t-\mathcal{A}$, relying on pointwise estimates of the fundamental solution of $L$.

Result: Existence of solutions satisfying anisotropic maximal regularity estimates for the Vlasov-Fokker-Planck equation $\mathcal{A}u=f$ in Euclidean space, along with similar estimates and weak (1,1) estimates for $L=\partial_t-\mathcal{A}$.

Conclusion: The paper successfully establishes maximal regularity results for a class of Vlasov-Fokker-Planck equations, providing important analytical tools for studying kinetic equations with Ornstein-Uhlenbeck type operators.

Abstract: We consider the maximal regularity of a specific Vlasov-Fokker-Planck equation $\mathcal{A}u=f$ in the Euclidean space. The operator $\mathcal{A}=Δ_{y}u-y\cdot \nabla_x{u}$ is an example of the Ornstein-Uhlenbeck operators. We prove the existence of a solution that satisfies the anisotropic maximal regularity estimates. To prove this we also show a similar estimates and a weak (1, 1) estimate for $L=\partial_t-\mathcal{A}$, which is of independent interest. These results rely on the pointwise estimates of the fundamental solution of $L$.

</details>


### [24] [Wave front set of solutions to the fractional Schrödinger equation](https://arxiv.org/abs/2602.17406)
*Takumi Kanai,Ryo Muramatsu,Yuusuke Sugiyama*

Main category: math.AP

TL;DR: The paper analyzes wave front sets of solutions to fractional Schrödinger equations using wave packet transform, relating fractional order θ to potential growth in singularity propagation, bridging Schrödinger and wave equation mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand how singularities propagate in solutions to fractional Schrödinger equations with potentials, and to clarify the relationship between the fractional order θ and potential growth rates in this propagation.

Method: Uses wave packet transform (short-time Fourier transform) to characterize wave front sets of solutions to fractional Schrödinger equations i∂ₜu = (-Δ)^{θ/2}u + V(x)u with 0<θ<2.

Result: Clarifies the relationship between fractional order θ and potential growth rate in singularity propagation, presenting a theorem that bridges propagation mechanisms between Schrödinger and wave equations.

Conclusion: The wave packet transform provides effective characterization of wave front sets, revealing how fractional order and potential growth interact in singularity propagation, unifying understanding across different equation types.

Abstract: In this paper, we characterize the wave front sets of solutions to fractional Schrödinger equations \(i\partial_{t}u =(-Δ)^{θ/2}u + V(x)u\)
  with $0<θ<2$ via the wave packet transform (short-time Fourier transform). We clarify the relationship between the order \(θ\) of the fractional Laplacian and the growth rate of the potential in the problem of propagation of singularities. In particular, we present a theorem that bridges the propagation mechanisms of singularities for the Schrödinger and wave equations.

</details>


### [25] [Isoperimetric inequalities for the lowest magnetic Steklov eigenvalue](https://arxiv.org/abs/2602.17416)
*Ayman Kachmar,Vladimir Lotoreichik*

Main category: math.AP

TL;DR: The disk maximizes the lowest magnetic Steklov eigenvalue among simply-connected smooth domains of given area for moderate magnetic fields.


<details>
  <summary>Details</summary>
Motivation: To establish isoperimetric inequalities for the lowest eigenvalue of magnetic Steklov problems on planar domains, determining optimal domain shapes under area or perimeter constraints.

Method: Uses torsion-type trial functions for bounded domains and trial functions dependent only on distance to boundary for exterior domains.

Result: For bounded domains: disk maximizes lowest magnetic Steklov eigenvalue among simply-connected smooth domains of given area for moderate magnetic fields. For exterior domains: similar isoperimetric inequality under perimeter constraint with additional geometric/symmetry assumptions.

Conclusion: The disk is optimal for maximizing the lowest magnetic Steklov eigenvalue under area constraints, establishing isoperimetric inequalities for this spectral problem.

Abstract: This paper studies the optimization of the lowest eigenvalue of the magnetic Steklov problem on planar domains. In the bounded domain setting and for magnetic fields of moderate strengths, we prove that among all simply-connected smooth domains of given area, the disk maximises the lowest magnetic Steklov eigenvalue. For exterior domains, we establish a similar isoperimetric inequality for magnetic fields of moderate strength under fixed perimeter constraint and additional geometric and symmetry assumptions. The proofs rely on the method of torsion-type trial functions in the bounded domain case and on the method of trial functions dependent only on the distance to the boundary in the exterior domain case.

</details>


### [26] [Construction of two-bubble blow-up solutions for the mass-critical gKdV equations](https://arxiv.org/abs/2602.17457)
*Yang Lan,Xu Yuan*

Main category: math.AP

TL;DR: Existence of global solution to mass-critical gKdV equation that blows up in infinite time as sum of two decoupled bubbles with opposite signs.


<details>
  <summary>Details</summary>
Motivation: Study blow-up behavior for mass-critical generalized Korteweg-de Vries equation, specifically understanding how solutions can blow up in infinite time as sum of decoupled solitons with opposite signs.

Method: Inspired by techniques from Martel-Raphaël for 2D mass-critical NLS. Uses refined approximate solutions with non-localized profiles and sharp analysis of interactions between solitons and these profiles to handle unstable scaling directions excited by nonlinear interactions.

Result: Proves existence of global solution that blows up in infinite time and approaches sum of two decoupled bubbles with opposite signs.

Conclusion: Successfully establishes existence of infinite-time blow-up solutions for mass-critical gKdV equation as sum of two opposite-sign decoupled bubbles, overcoming difficulties from excited scaling instabilities through refined approximate solutions and interaction analysis.

Abstract: For the mass-critical generalized Korteweg-de Vries equation, $$ \partial_{t}u+\partial_{x}\left( \partial_{x}^{2}u+u^{5}\right)=0,\quad (t,x)\in [0,\infty)\times \mathbb{R}.$$ We prove the existence of a global solution that blows up in infinite time and approaches the sum of two decoupled bubbles with opposite signs. The proof is inspired by the techniques developed for the two-dimensional mass-critical NLS equation in a similar context by Martel-Raphaël [37]. The main difficulty originates from the fact that the unstable directions related to scaling are excited by the nonlinear interactions. To overcome this difficulty, a refined approximate solution that involves some non-localized profiles is needed. In particular, a sharp understanding for the interactions between solitons and such profiles is also required.

</details>


### [27] [A comment on an $L^\frac{2n}{n+2}-L^\frac{2n}{n-2}$ Carleman inequality in relation to "the determination of an unbounded potential from Cauchy data"](https://arxiv.org/abs/2602.17523)
*Mourad Choulli,Hiroshi Takase*

Main category: math.AP

TL;DR: This short note identifies and corrects an error in Proposition 2.1 from two previous papers, providing a new proof with additional hypothesis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to correct a partially incorrect proof in Proposition 2.1 from DKS (arXiv:1104.0232) and Ch (arXiv:2310.17456), where the same error was repeated.

Method: The authors provide a new proof for Proposition 2.1 that requires an additional hypothesis. They also modify this new proof to correct the same error in Ch's paper.

Result: The result is a corrected version of Proposition 2.1 with proper proof, addressing the mathematical error in both referenced papers.

Conclusion: The conclusion is that the original proofs were flawed, but the corrected versions with additional hypothesis are now mathematically sound.

Abstract: The proof of \cite[Proposition 2.1]{DKS}[arXiv:1104.0232] is partially incorrect. In this short note, we provide a new proof, which requires an additional hypothesis. A modification of this new proof also corrects the proof of \cite[Proposition 2.1]{Ch}[arXiv:2310.17456], where the incorrect argument of \cite{DKS} has been repeated.

</details>


### [28] [On putative self-similarity for incompressible 3D Euler](https://arxiv.org/abs/2602.17570)
*Peter Constantin,Mihaela Ignatova,Vlad Vicol*

Main category: math.AP

TL;DR: The paper establishes lower bounds on the similarity exponent γ for hypothetical self-similar finite-time blowup solutions of 3D Euler equations, with different bounds depending on initial energy conditions and solution properties.


<details>
  <summary>Details</summary>
Motivation: To understand the possible scaling behavior of hypothetical finite-time blowup solutions in 3D Euler equations, which is important for the open problem of whether smooth solutions to 3D Euler can develop singularities in finite time.

Method: Mathematical analysis of self-similar blowup scenarios for 3D Euler equations, using energy estimates and scaling arguments. The analysis considers different cases: general solutions with finite kinetic energy, smooth globally self-similar profiles with outgoing property, and axisymmetric solutions without the outgoing property requirement.

Result: For general solutions with finite kinetic energy: γ > 2/5. For smooth globally self-similar profiles with outgoing property: γ ≥ 1/2. For axisymmetric solutions in more general settings: γ ≥ 1/2 even without the outgoing property.

Conclusion: The paper establishes rigorous lower bounds on the similarity exponent for hypothetical self-similar blowup in 3D Euler equations, with stronger bounds (γ ≥ 1/2) for axisymmetric solutions and smooth profiles with outgoing properties, compared to the weaker bound (γ > 2/5) for general finite-energy solutions.

Abstract: We consider hypothetical solutions of 3D Euler which blow up in finite time in a self-similar fashion. We prove that if the initial data has finite kinetic energy, then the similarity exponent $γ$ which governs the rate of zooming in must be larger than $2/5$. If a smooth globally self-similar blowup profile exists, and this profile satisfies an outgoing property, we prove that $γ\geq 1/2$. For axisymmetric solutions, we establish the bound $γ\geq 1/2$ in more general settings, including ones in which the outgoing property is not present.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [29] [Distillation and Interpretability of Ensemble Forecasts of ENSO Phase using Entropic Learning](https://arxiv.org/abs/2602.16857)
*Michael Groom,Davide Bassetti,Illia Horenko,Terence J. O'Kane*

Main category: physics.comp-ph

TL;DR: A distillation framework compresses ensemble eSPA models for ENSO prediction into interpretable distilled models while preserving forecast skill, enabling analysis of ENSO dynamics and precursors.


<details>
  <summary>Details</summary>
Motivation: While ensemble eSPA models achieve state-of-the-art ENSO forecast skill, they are hard to interpret. The authors aim to create interpretable distilled models that maintain performance while enabling diagnostic analysis of ENSO predictability.

Method: Compress ensemble of entropy-optimal Sparse Probabilistic Approximation models by aggregating only correctly-predicting ensemble members into distilled "superclusters." Analyze regime persistence, cross-lead clustering consistency, feature importance vectors, and spatial importance maps to study ENSO dynamics.

Result: Distilled models preserve forecast performance while enabling diagnostics. Analysis shows distilled models capture ENSO spatiotemporal dynamics, reveal peak complexity when crossing boreal spring predictability barrier, identify known physical precursors via spatial importance maps, and trace event evolution from precursors to mature ENSO state.

Conclusion: The distillation framework enables rigorous investigation of long-range ENSO predictability while complementing operational forecasts, providing interpretable models that maintain skill and reveal physical mechanisms.

Abstract: This paper introduces a distillation framework for an ensemble of entropy-optimal Sparse Probabilistic Approximation (eSPA) models, trained exclusively on satellite-era observational and reanalysis data to predict ENSO phase up to 24 months in advance. While eSPA ensembles yield state-of-the-art forecast skill, they are harder to interpret than individual eSPA models. We show how to compress the ensemble into a compact set of "distilled" models by aggregating the structure of only those ensemble members that make correct predictions. This process yields a single, diagnostically tractable model for each forecast lead time that preserves forecast performance while also enabling diagnostics that are impractical to implement on the full ensemble.
  An analysis of the regime persistence of the distilled model "superclusters", as well as cross-lead clustering consistency, shows that the discretised system accurately captures the spatiotemporal dynamics of ENSO. By considering the effective dimension of the feature importance vectors, the complexity of the input space required for correct ENSO phase prediction is shown to peak when forecasts must cross the boreal spring predictability barrier. Spatial importance maps derived from the feature importance vectors are introduced to identify where predictive information resides in each field and are shown to include known physical precursors at certain lead times. Case studies of key events are also presented, showing how fields reconstructed from distilled model centroids trace the evolution from extratropical and inter-basin precursors to the mature ENSO state. Overall, the distillation framework enables a rigorous investigation of long-range ENSO predictability that complements real-time data-driven operational forecasts.

</details>


### [30] [Machine Learning Hamiltonians are Accurate Energy-Force Predictors](https://arxiv.org/abs/2602.16897)
*Seongsu Kim,Chanhui Lee,Yoonho Kim,Seongjun Yun,Honghui Kim,Nayoung Kim,Changyoung Park,Sehui Han,Sungbin Lim,Sungsoo Ahn*

Main category: physics.comp-ph

TL;DR: QHFlow2 is a new SO(2)-equivariant Hamiltonian model that achieves state-of-the-art accuracy for energy and force predictions from predicted Hamiltonians, outperforming previous models by significant margins.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning Hamiltonian models are mainly evaluated by reconstruction metrics, leaving their performance as energy-force predictors unclear. There's a need for proper benchmarking of Hamiltonian models for direct energy and force evaluation.

Method: QHFlow2 uses an SO(2)-equivariant backbone with a two-stage edge update mechanism. The authors establish a benchmark framework for computing energies and forces directly from predicted Hamiltonians.

Result: QHFlow2 achieves 40% lower Hamiltonian error than previous best models with fewer parameters. On MD17/rMD17, it reaches NequIP-level force accuracy with up to 20× lower energy MAE. On QH9, it reduces energy error by up to 20× compared to MACE.

Conclusion: QHFlow2 represents a state-of-the-art Hamiltonian model that demonstrates consistent scaling behavior and shows that improvements in Hamiltonian accuracy directly translate to better energy and force predictions.

Abstract: Recently, machine learning Hamiltonian (MLH) models have gained traction as fast approximations of electronic structures such as orbitals and electron densities, while also enabling direct evaluation of energies and forces from their predictions. However, despite their physical grounding, existing Hamiltonian models are evaluated mainly by reconstruction metrics, leaving it unclear how well they perform as energy-force predictors. We address this gap with a benchmark that computes energies and forces directly from predicted Hamiltonians. Within this framework, we propose QHFlow2, a state-of-the-art Hamiltonian model with an SO(2)-equivariant backbone and a two-stage edge update. QHFlow2 achieves $40\%$ lower Hamiltonian error than the previous best model with fewer parameters. Under direct evaluation on MD17/rMD17, it is the first Hamiltonian model to reach NequIP-level force accuracy while achieving up to $20\times$ lower energy MAE. On QH9, QHFlow2 reduces energy error by up to $20\times$ compared to MACE. Finally, we demonstrate that QHFlow2 exhibits consistent scaling behavior with respect to model capacity and data, and that improvements in Hamiltonian accuracy effectively translate into more accurate energy and force computations.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [31] [Operational measurement of relativistic equilibrium from stochastic fields alone](https://arxiv.org/abs/2602.16765)
*Ira Wolfson*

Main category: physics.plasm-ph

TL;DR: Paper shows how to reconstruct relativistic thermal state (inverse-temperature four-vector β^μ) from electromagnetic fluctuations of a drifting medium without external probes or calibration.


<details>
  <summary>Details</summary>
Motivation: Relativistic equilibrium is described by the inverse-temperature four-vector β^μ rather than frame-dependent scalar temperature. Current methods require external probes, spectral lines, or absolute intensity calibration, which are limiting for studying relativistic media.

Method: Uses electromagnetic fluctuations emitted by drifting medium. Lorentz boost converts isotropic rest-frame noise into correlated electric and magnetic fields, creating gain-independent fluctuation observable that yields drift velocity. Combines this with angle-resolved noise spectra from covariant fluctuation-dissipation theorem to reconstruct β^μ using only electromagnetic measurements.

Result: Monte Carlo analysis demonstrates percent-level accuracy at realistic signal-to-noise ratios. Feasibility estimates indicate sub-microsecond integration times for laboratory plasmas. First method to reconstruct covariant thermal state β^μ from passive stochastic fields alone without absolute calibration, spectral lines, or external probes.

Conclusion: Establishes vacuum electromagnetic fluctuations as a direct operational probe of relativistic equilibrium, enabling reconstruction of relativistic thermal state purely from stochastic electromagnetic data without external interventions.

Abstract: Relativistic equilibrium is described by the inverse-temperature four-vector $β^μ= u^μ/(k_B T_0)$ rather than by a frame-dependent scalar temperature. We show that $β^μ$ can be reconstructed directly from electromagnetic fluctuations emitted by a drifting medium, without external probes, spectral lines, or absolute intensity calibration. A Lorentz boost converts isotropic rest-frame noise into correlated electric and magnetic fields, producing a gain-independent fluctuation observable that yields the drift velocity purely from stochastic data. Combined with angle-resolved noise spectra governed by the covariant fluctuation--dissipation theorem, this enables full reconstruction of $β^μ$ using electromagnetic measurements alone. Monte Carlo analysis demonstrates percent-level accuracy at realistic signal-to-noise ratios, and feasibility estimates indicate sub-microsecond integration times for laboratory plasmas. To our knowledge, this constitutes the first method that reconstructs the covariant thermal state $β^μ$ of a relativistic medium from passive stochastic fields alone, without absolute calibration, spectral lines, or external probes. These results establish vacuum electromagnetic fluctuations as a direct operational probe of relativistic equilibrium.

</details>


### [32] [Enhanced Hot Electron Preheat Observed in Magnetized Laser Direct-Drive Implosions](https://arxiv.org/abs/2602.16884)
*M. Cufari,M. Gatu Johnson,C. K. Li,J. A. Frenje,P. W. Moloney,A. J. Crilly,P. V. Heuer,J. R. Davies*

Main category: physics.plasm-ph

TL;DR: A 10T magnetic field enhances hard x-ray emission by 1.5x in direct-drive implosions by confining hot electrons that would otherwise escape, leading to increased preheat but reduced capsule charging.


<details>
  <summary>Details</summary>
Motivation: To understand how applied magnetic fields affect hot electron preheat, x-ray emission, and capsule charging in direct-drive laser implosions, particularly for optimizing fusion gain and implosion efficiency.

Method: Applied 10T magnetic field to direct-drive implosions, observed hard x-ray emission enhancement, analyzed magnetic field alignment with ablation flow, and studied hot electron confinement via mirror-mode trapping and pitch-angle scattering onto capsule.

Result: Hard x-ray emission increased by factor of 1.5±0.1 with magnetic field; hot electrons confined in mirror-mode scatter onto capsule; charged-fusion product energy decreased due to reduced capsule charging in magnetized implosions.

Conclusion: Magnetic fields enhance hot electron preheat via confinement and scattering, intensifying need to mitigate laser-plasma instabilities for maximizing fusion gain in magnetized implosions.

Abstract: Hard x-ray emission, associated with hot electron preheat, in direct-drive implosions was observed to be enhanced by a factor of $1.5\pm0.1$ by application of a $10$ T magnetic field. The applied magnetic field reaches a quasi steady-state aligned with the ablation flow prior to the onset of laser-plasma instabilities in the corona. Hot electrons that would otherwise escape the corona and lead to capsule charging in unmagnetized implosions are confined in a mirror-mode of the magnetic field in magnetized implosions. These hot electrons are shown to subsequently pitch-angle scatter from the mirror onto the capsule, thereby leading to the observed hard x-ray generation in magnetized implosions. Consequently, the energy of charged-fusion products, associated with the capsule charging, are observed to decrease when the implosion is magnetized. These results intensify the need to mitigate laser-plasma instabilities -- particularly for magnetized implosions -- to maximize fusion gain and implosion efficiency.

</details>


### [33] [Numerical study of electron acceleration by microwave-driven plasma wakefields in rectangular waveguides](https://arxiv.org/abs/2602.16896)
*Jesús E. López,Eduardo A. Orozco-Ospino*

Main category: physics.plasm-ph

TL;DR: Microwave-driven plasma wakefields in rectangular waveguides can accelerate externally injected electrons to ~100 keV over meter-scale lengths with quasi-monoenergetic distributions when electrons are pre-accelerated to match the microwave pulse's group velocity.


<details>
  <summary>Details</summary>
Motivation: Plasma-based acceleration offers compact particle accelerators due to large electric fields in plasmas. While microwave-driven plasma wakefields have been demonstrated, the conditions for efficient electron acceleration in such configurations remain insufficiently characterized.

Method: Used 3D particle-in-cell simulations to analyze electron injection and energy gain in rectangular waveguides filled with low-density plasma. Examined both reduced and fully self-consistent numerical models to study dynamics of externally injected electrons accelerated by microwave-driven plasma wakefields.

Result: Electron acceleration strongly depends on injection phase and initial velocity. Optimal acceleration occurs when electrons are pre-accelerated to velocities close to the driving microwave pulse's group velocity. Achieved ~100 keV energy gains over meter-scale interaction lengths with quasi-monoenergetic distributions under suitable injection conditions. Transverse dynamics and space-charge effects impose additional constraints on acceleration efficiency.

Conclusion: The study provides quantitative assessment of acceleration stage in microwave-driven plasma wakefield schemes, supporting their evaluation as viable platform for compact plasma-based accelerators. Results characterize key dependencies and constraints for efficient electron acceleration in such configurations.

Abstract: Plasma-based acceleration schemes have attracted sustained interest as a pathway toward compact particle accelerators, owing to the large electric fields supported by plasmas. Although recent studies have demonstrated the excitation of plasma wakefields using high-power microwave pulses in plasma-filled waveguides, the conditions required for efficient electron acceleration in such configurations remain insufficiently characterized. In this work, we investigate the acceleration of externally injected electrons by microwave-driven plasma wakefields in rectangular waveguides filled with low-density plasma. Three-dimensional particle-in-cell simulations are employed to analyze the dynamics of electron injection and energy gain under both reduced and fully self-consistent numerical models. The results show that electron acceleration is strongly dependent on the injection phase and initial velocity. Optimal acceleration is achieved when electrons are pre-accelerated to velocities close to the group velocity of the driving microwave pulse. For the parameters considered, energy gains of the order of $10^2 \mathrm{keV}$ are obtained over interaction lengths of the order of meters, while maintaining a quasi-monoenergetic energy distribution under suitable injection conditions. The influence of transverse dynamics and space-charge effects is also examined, revealing additional constraints on acceleration efficiency associated with the transverse electromagnetic field of the driving microwave pulse. These results provide a quantitative assessment of the acceleration stage in microwave-driven plasma wakefield schemes and support their evaluation as a viable platform for compact plasma-based accelerators.

</details>


### [34] [Capturing Secondary Kinetic Instabilities in Three-Dimensional Dayside Reconnection Using an Improved Gradient-Based Closure](https://arxiv.org/abs/2602.16960)
*Kolter Bradshaw,Ammar Hakim,James Juno,Joshua Pawlak,Jason TenBarge,Amitava Bhattacharjee*

Main category: physics.plasm-ph

TL;DR: Improved gradient-based heat flux closure in 16-moment fluid model enables better simulation of kinetic instabilities and turbulence in asymmetric magnetic reconnection.


<details>
  <summary>Details</summary>
Motivation: Previous ten-moment fluid models struggled to replicate current sheet instabilities and mixing seen in kinetic simulations when using local relaxation closure for heat flux, limiting accurate representation of reconnection dynamics.

Method: Using Gkeyll software framework to simulate asymmetric reconnection based on MMS Burch event data, implementing improved gradient-based heat flux closure in the fluid model.

Result: Significant improvement in secondary kinetic instabilities that grow in current sheet, generating turbulence which leads to growth of secondary magnetic islands and flux ropes.

Conclusion: The gradient-based heat flux closure enables better capture of kinetic physics in fluid models, improving simulation of current sheet instabilities and turbulent mixing in magnetic reconnection.

Abstract: Magnetic reconnection is a highly dynamic process that excites a wide variety of kinetic waves and instabilities. Transverse current sheet instabilities such as the lower-hybrid drift and secondary drift-kink instabilities in particular have been shown by kinetic simulations to modify the reconnection and introduce significant turbulence and mixing to the reconnection layer. Past studies using the ten-moment fluid model to capture important kinetic physics such as the electron inertia and full representation of the pressure tensor proved advantageous to a two-fluid representation of reconnection, but the model struggled when using a local relaxation closure for the heat flux to replicate the current sheet instabilities and subsequent mixing seen in kinetic simulations. This work uses the \texttt{Gkeyll} software framework to perform simulations of asymmetric reconnection based on the 16 October 2015 MMS crossing of a diffusion region, the Burch event. An improved gradient-based heat flux closure is implemented, showing significant improvement in secondary kinetic instabilities that grow in the current sheet. These instabilities generate turbulence which leads to growth of secondary magnetic islands and flux ropes.

</details>


### [35] [Resistive instabilities of current sheets in stratified plasmas with a gravitational field](https://arxiv.org/abs/2602.17400)
*Faisal Sayed,Anna Tenerani,Richard Fitzpatrick*

Main category: physics.plasm-ph

TL;DR: Gravity and density stratification significantly alter tearing mode instability in magnetic reconnection, with favorable stratification suppressing reconnection and unfavorable stratification strongly destabilizing it, eliminating the classical constant-ψ regime.


<details>
  <summary>Details</summary>
Motivation: To understand how gravity and density stratification affect magnetic reconnection via tearing instability in astrophysical and fusion plasma environments where heavy-over-light plasma configurations exist in gravitational fields with current sheets.

Method: Linear stability analysis of a slab current sheet with density gradient under constant gravitational acceleration, studying reconnecting modes in both favorable and unfavorable stratification scenarios.

Result: Favorable stratification suppresses reconnection while unfavorable stratification strongly destabilizes tearing mode. Classical constant-ψ regime disappears for strong magnetic Reynolds numbers (S>>1) under unfavorable stratification, transitioning to gravity-driven G-mode with growth rate scaling as S^-1/3.

Conclusion: Gravity and stratification fundamentally modify tearing instability behavior, with unfavorable stratification enabling only rapidly reconnecting modes through transition to gravity-driven G-mode, impacting reconnection in astrophysical and fusion plasma environments.

Abstract: Magnetic reconnection can develop spontaneously via the tearing instability, often invoked to explain disruptive instabilities in fusion devices, solar flares, the generation of periodic density disturbances at the tip of helmet streamers, and flux transfer events at the Earth's dayside magnetopause. However, in many such environments the presence of gravity, magnetic field curvature or other forms of acceleration often result in situations of a heavy-over-light plasma in an effective gravitational field with an embedded current sheet. This paper studies the linear stability of a slab current sheet with respect to reconnecting modes in the presence of a density gradient under the effect of a constant gravitational acceleration. We show that the presence of stratification and gravity modify the properties of the tearing mode instability both in the case of favorable and unfavorable stratification. Favorable stratification suppresses reconnection while unfavorable stratification strongly destabilizes the tearing mode. Furthermore, we show that the classical constant-ψ regime effectively does not exist, even for weak unfavorable stratification, for S>>1. Instead, the gravity-modified tearing progressively transitions into the G-mode, which is a gravity-driven reconnecting mode with a growth rate scaling as S^-1/3. As a consequence, unfavorable stratification only permits rapidly reconnecting modes.

</details>


### [36] [Plasma Mixing in Collisionless Kelvin-Helmholtz Dynamics](https://arxiv.org/abs/2602.17404)
*Silvia Ferro,Fabio Bacchini,Giuseppe Arrò,Francesco Pucci,Pierre Henri*

Main category: physics.plasm-ph

TL;DR: The paper investigates plasma mixing driven by Kelvin-Helmholtz instability in magnetospheric boundary layers using high-resolution PIC simulations, finding localized mixing mainly in vortex structures with ions mixing more effectively than electrons.


<details>
  <summary>Details</summary>
Motivation: To understand the efficiency and physical mechanisms of plasma mixing driven by nonlinear Kelvin-Helmholtz instability evolution in magnetosphere-magnetosheath boundary layers, which influences transport and particle acceleration.

Method: High-resolution two-dimensional Particle-In-Cell (PIC) simulations using finite-Larmor-radius shear-flow initial configuration, with plasma mixing quantified using particle tracking, passive tracers, and magnetic reconnection diagnostics.

Result: Plasma mixing is present but localized, occurring mainly in narrow interface regions and plasma structures; ions mix more effectively than electrons; enhanced mixing correlates with localized reconnection within and between KH vortices; cross-boundary transport is highly localized and mediated by vortex advection and reconnection.

Conclusion: Kinetic-scale transport across collisionless shear layers is highly localized, with electron mixing strongly constrained, providing an upper bound on cross-boundary transport driven by the kinetic Kelvin-Helmholtz instability.

Abstract: Simulations and observations of the low-latitude magnetosphere-magnetosheath boundary layer indicate that the Kelvin-Helmholtz instability (KHI) drives vortex structures that enhance plasma mixing and magnetic reconnection, influencing transport and particle acceleration. We investigate the efficiency and physical mechanisms of plasma mixing driven by the nonlinear evolution of the KHI. We perform high-resolution two-dimensional Particle-In-Cell (PIC) simulations using a finite-Larmor-radius shear-flow initial configuration. Plasma mixing is quantified using particle tracking, passive tracers, and diagnostics of magnetic reconnection. Mixing across the shear layer is present but localized, occurring mainly in narrow interface regions and plasma structures. Ions mix more effectively than electrons, which remain largely frozen to field lines. Enhanced mixing correlates with localized reconnection within and between KH vortices. Cross-boundary transport driven by the kinetic KHI is highly localized and mediated by vortex advection and reconnection. Electron mixing is strongly constrained, providing an upper bound on kinetic-scale transport across collisionless shear layers.

</details>


### [37] [Self-Consistent Dynamics of Electron Radiation Reaction via Structure-Preserving Geometric Algorithms for Coupled Schrödinger-Maxwell Systems](https://arxiv.org/abs/2602.17429)
*Jacob Matthew Molina,Hong Qin*

Main category: physics.plasm-ph

TL;DR: The paper develops structure-preserving algorithms (SPHINX code) to simulate radiation reaction effects on quantum electron states in magnetic fields, showing how coherent states decohere and Landau levels become dressed stationary states.


<details>
  <summary>Details</summary>
Motivation: Classical radiation reaction equations (Abraham-Lorentz/Landau-Lifshitz) fail at atomic scales where quantum effects dominate, and the coupled Schrödinger-Maxwell system is too complex for purely analytical studies. There's a need for computational methods to understand radiation reaction in quantum regimes.

Method: Developed geometric structure-preserving algorithms that preserve gauge invariance, symplecticity, and unitarity on discrete space-time lattices. Implemented in SPHINX code to simulate coupled Schrödinger-Maxwell dynamics, using coherent states constructed from Landau levels.

Result: Simulations show: 1) Electron coherent states in magnetic fields radiate strongly, rapidly lose coherence, and disperse into decoherent wave packets; 2) Landau levels renormalize into stationary dressed eigenstates with constant electromagnetic and kinetic energies under appropriate boundary conditions.

Conclusion: The SPHINX code opens new computational windows into radiation reaction physics, advancing modeling of extreme-field phenomena in fusion plasmas, astrophysics, and next-generation laser experiments by properly handling quantum decoherence effects.

Abstract: Classically, a charged particle in a magnetic field emits radiation, losing momentum and experiencing the Abraham-Lorentz (AL) / Landau-Lifshitz (LL) radiation reaction (RR) force. However, at atomic scales and outside the range of their applicability, the AL/LL equations fail and RR destroys the coherent state of an electron-undermining the very concept of a RR force. This process can be described by the coupled Schrödinger-Maxwell (SM) system under appropriate limits, but the system's nonlinear complexity has long limited purely analytical studies. We present geometric structure-preserving algorithms for the SM system that preserve gauge invariance, symplecticity, and unitarity on the discrete space-time lattice, which are implemented in our Structure-Preserving scHrodINger maXwell (SPHINX) code. By constructing coherent states from the Landau levels, SPHINX simulates the fully-coupled nonlinear dynamics of an electron coherent state, the energy partition evolution, and decoherence/relaxation of the electron wave packet in time due to RR. These simulations indicate that, in an external magnetic field, an electron prepared in an atomic-scale coherent state can radiate strongly, rapidly losing coherence and dispersing into a decoherent wave packet. Additionally, we also present the fully-coupled nonlinear evolution of the non-degenerate ground- and first-excited Landau levels themselves to understand how the coupled SM system modifies the well-known ideal (i.e., Schrödinger-only) dynamics of the Landau Levels. With appropriate boundary conditions, simulations show that the Landau levels are renormalized into stationary dressed eigenstates with constant electromagnetic and kinetic energies. This opens a new computational window into RR physics and advances modeling of extreme-field phenomena in fusion plasmas, astrophysics, and next-generation laser experiments

</details>


### [38] [Tolerances to driver-witness misalignment in a quasilinear plasma wakefield accelerator](https://arxiv.org/abs/2602.17468)
*T. C. Wilson,J. Farmer,K. Lotov,A. Pukhov*

Main category: physics.plasm-ph

TL;DR: Analytical models for transverse witness dynamics in quasilinear plasma wakefield acceleration, showing how misalignment affects emittance preservation with validation using AWAKE Run 2c parameters.


<details>
  <summary>Details</summary>
Motivation: Plasma-based accelerators offer high gradients and scalability for future colliders, but emittance preservation during witness bunch acceleration is critical, especially when driver-witness misalignment occurs in quasilinear regimes.

Method: Developed analytical models to describe witness motion in quasilinear plasma wakefield acceleration, created a metric for emittance preservation based on witness density after phase mixing, and validated with particle-in-cell simulations using AWAKE Run 2c baseline parameters.

Result: Excellent agreement between analytical models and PIC simulations, enabling prediction of emittance preservation and establishment of alignment constraints for AWAKE and similar wakefield acceleration schemes.

Conclusion: The developed models provide a practical framework for setting alignment tolerances in quasilinear plasma wakefield accelerators, supporting both AWAKE experiment optimization and broader applications in wakefield acceleration.

Abstract: Plasma-based accelerators offer high accelerating gradients and scalability through staging or long plasma sources, which makes them good candidates for future accelerator and collider concepts. Proton-driven accelerators in particular have the potential to bring particles to high energy in a single stage. In the quasilinear regime - where the plasma wake is only partially evacuated - a witness bunch of electrons drives a cavitated wake, which acts to preserve the emittance of the portion of the witness inside this self-blowout. In the case of a misalignment between the driver and witness, this behaviour can persist, but its effectiveness is reduced. In this paper, we study transverse witness dynamics in this regime, and develop analytical models to describe the witness motion, and develop a metric to estimate emittance preservation based on a single parameter which estimates the density of the witness after phase mixing. Particle in cell simulations using the AWAKE Run 2c baseline parameters show excellent agreement with the predictive models developed. This work allows alignment constraints to be set both for the AWAKE experiment and other wakefield acceleration schemes operating in the quasilinear regime.

</details>


### [39] [A Kinetic Route to Helicity-Constrained Decay](https://arxiv.org/abs/2602.17514)
*Dion Li*

Main category: physics.plasm-ph

TL;DR: 2D PIC simulations show localized E·B≠0 regions correlate with magnetic helicity reductions. A new source-compensated helicity density enables time-invariant intermediate-scale plateaus in correlation integrals, consistent with 2D decay constraints.


<details>
  <summary>Details</summary>
Motivation: To understand the statistical association between intermittent E·B≠0 regions and magnetic helicity reductions in decaying sub-ion turbulence, and to develop a better helicity measure that captures local balance properties.

Method: 2D3V PIC simulations of freely decaying sub-ion turbulence; proposal of source-compensated, history-dependent helicity density satisfying exact local balance; analysis of Saffman-type two-point correlation integrals with flux-decorrelation assumptions.

Result: Localized E·B≠0 regions statistically associate with magnetic helicity reductions; new helicity measure shows time-invariant intermediate-scale plateaus even as traditional Saffman integral evolves; initially net-helical configurations develop mixed-signed helicity patches with decreasing global fractional helicity.

Conclusion: The decay behavior in kinetic interval is most consistent with cancellation-dominated scaling constraints, and the proposed helicity measure provides better characterization of local balance properties in turbulent plasma systems.

Abstract: Through 2D3V PIC simulations of freely decaying sub-ion turbulence, intermittent localized regions with $\mathbf{E} \cdot \mathbf{B} \neq 0$ are found to be statistically associated with reductions in the magnitude of magnetic helicity while evolving in the early electron-scale interaction phase. Motivated by this behavior, we propose a source-compensated, history-dependent helicity density that satisfies an exact local balance identity by construction, enabling Saffman-type two-point correlation integrals which, under standard flux-decorrelation assumptions, can exhibit intermediate-scale plateaus that are roughly time-independent. In our simulations we demonstrate such plateaus to remain approximately invariant even as the usual Saffman helicity integral plateau value $I_H$ evolves during the early kinetic stage. Under approximate single-scale self-similarity, the plateau behavior of the magnetic integral is consistent with the 2D decay constraint $BL \sim \text{const}$. For initially net-helical configurations, we observe rapid development of mixed-signed magnetic helicity patches and a decrease of the global fractional helicity, such that the decay over the kinetic interval is again most consistent with the cancellation-dominated scaling constraint.

</details>


### [40] [Characterization of compressible fluctuations in solar wind streams dominated by balanced and imbalanced turbulence: Parker Solar Probe, Solar Orbiter and Wind observations](https://arxiv.org/abs/2602.17606)
*C. A. Gonzalez,C. Gonzalez,A. Tenerani*

Main category: physics.plasm-ph

TL;DR: Statistical analysis of solar wind compressible fluctuations shows they're affected by both solar expansion and local plasma conditions, with slow magnetosonic modes dominating and potentially contributing to solar wind heating near the Sun.


<details>
  <summary>Details</summary>
Motivation: Compressible fluctuations in solar wind are important for understanding solar wind acceleration and heating, but their origin and evolution across different turbulence regimes remain poorly understood.

Method: Statistical analysis using in-situ measurements from Wind, Solar Orbiter, and Parker Solar Probe to investigate scale dependence of density and magnetic field fluctuations and their correlations with plasma beta and radial distance.

Result: Solar wind compressibility is affected by both expansion effects and compressible dynamics. Non-Alfvenic wind dominated by anti-correlated fluctuations (slow modes), while Alfvenic wind contains mixture. Anti-correlated component consistent with MHD slow magnetosonic modes, but correlated component not explained by existing theories. Slow modes explain beta dependence and enhanced density fluctuations near Sun.

Conclusion: Slow mode waves contribute significantly to compressible energy budget near the Sun and may play important role in solar wind heating and acceleration close to the Sun.

Abstract: Characterizing compressible fluctuations in the solar wind is essential for understanding their role in solar wind acceleration and heating, yet their origin and evolution across different turbulence regimes remain poorly understood. In this study, we carry out a statistical analysis of the properties of compressible fluctuations in solar wind dominated by balanced and imbalanced turbulence. Using in-situ measurements from Wind, Solar Orbiter, and Parker Solar Probe, we investigate the scale dependence of density and magnetic field fluctuations and their correlations with plasma beta and radial distance. Our results indicate that solar wind compressibility is likely affected by both expansion effects and compressible dynamics governed by local plasma conditions. The non-Alfvenic wind is dominated by anti-correlated fluctuations, whereas the Alfvenic wind contains a mixture of correlated and anti-correlated fluctuations, though the latter remain prevalent. While the anti-correlated component is consistent with MHD slow magnetosonic modes, the correlated (fast mode-like) component is not reproduced by predictions from either linear MHD theory or nonlinear models of forced compressible fluctuations. Nevertheless, the dominant slow mode component explains the observed dependence on beta and the enhanced density fluctuations measured by Parker Solar Probe. This further suggests that slow mode waves contribute significantly to the compressible energy budget near the Sun and may play an important role in solar wind heating and acceleration close to the Sun.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [41] [Structured Analytic Mappings for Point Set Registration](https://arxiv.org/abs/2602.16753)
*Wei Feng,Tengda Wei,Haiyong Zheng*

Main category: eess.IV

TL;DR: Analytic-ICP: A non-rigid point set registration method using multivariate Taylor expansions for smooth deformations with quasi-linear time complexity.


<details>
  <summary>Details</summary>
Motivation: Existing non-rigid registration methods often rely on kernel functions or high-dimensional parameterizations, which can be computationally expensive and lack explicit closed-form representations for smooth deformations.

Method: Uses multivariate Taylor expansion to create structured function space with truncated basis terms. Develops quasi-Newton optimization that progressively lifts identity map into higher-order analytic forms. Embeds this into standard ICP loop with nearest-neighbor correspondences.

Result: Analytic-ICP achieves higher accuracy and faster convergence than classical methods (CPD, TPS-RPM) on 2D and 3D datasets, especially for small and smooth deformations. Has quasi-linear time complexity.

Conclusion: The analytic approximation model provides a unified closed-form framework for rigid, affine, and nonlinear deformations without kernel functions or high-dimensional parameterizations, offering efficient and accurate non-rigid registration.

Abstract: We present an analytic approximation model for non-rigid point set registration, grounded in the multivariate Taylor expansion of vector-valued functions. By exploiting the algebraic structure of Taylor expansions, we construct a structured function space spanned by truncated basis terms, allowing smooth deformations to be represented with low complexity and explicit form. To estimate mappings within this space, we develop a quasi-Newton optimization algorithm that progressively lifts the identity map into higher-order analytic forms. This structured framework unifies rigid, affine, and nonlinear deformations under a single closed-form formulation, without relying on kernel functions or high-dimensional parameterizations. The proposed model is embedded into a standard ICP loop -- using (by default) nearest-neighbor correspondences -- resulting in Analytic-ICP, an efficient registration algorithm with quasi-linear time complexity. Experiments on 2D and 3D datasets demonstrate that Analytic-ICP achieves higher accuracy and faster convergence than classical methods such as CPD and TPS-RPM, particularly for small and smooth deformations.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [42] [Perturbation analysis of tensor $(\mathcal{B},\mathcal{C})$-inverse via Einstein product](https://arxiv.org/abs/2602.16759)
*Daochang Zhang,Jingqian Li,Dijana Mosic,Predrag S. Stanimirovic*

Main category: math.FA

TL;DR: Analyzing how small perturbations affect various tensor generalized inverses, focusing on relationships between perturbed and original tensor inverses.


<details>
  <summary>Details</summary>
Motivation: Small perturbations in tensors make it difficult to analyze generalized inverses of perturbed tensors and understand how perturbations affect the original tensor's generalized inverses.

Method: Investigating the relationship between generalized inverses of perturbed tensor D = A + E and original tensor A, focusing on specific classes including tensor inner, outer, and (B,C) inverses.

Result: Not specified in abstract - appears to be theoretical analysis of perturbation effects on tensor generalized inverses.

Conclusion: The paper aims to establish mathematical relationships between generalized inverses of original and perturbed tensors to understand perturbation sensitivity.

Abstract: We investigate the influence of a relatively small perturbation on various generalized inverses functions or quantities derived from a tensor $\mathcal{A}$.When a small tensor perturbation \(\mathcal{E}\) is introduced, it becomes challenging to analyze generalized inverses of the perturbed tensor \( \mathcal{D} =\mathcal{A}+\mathcal{E}\) and to determine how this perturbation affects a generalized inverse of $\mathcal{A}$.Our main goal is to understand the relationship between $\mathcal{D}^\Game$ and \( \mathcal{A}^\Game \), where $(\cdot)^\Game$ denotes a specific generalized inverse or a class of generalized inverses.In particular, classes of tensor inner, outer, and $(\mathcal{B},\mathcal{C})$ inverses are considered.

</details>


### [43] [Product Hardy Spaces on Spaces of Homogeneous Type: Discrete Product Calderón-Type Reproducing Formula, Atomic Characterization, and Product Calderón--Zygmund Operators](https://arxiv.org/abs/2602.17031)
*Ziyi He,Dachun Yang,Taotao Zheng*

Main category: math.FA

TL;DR: The paper introduces product Hardy spaces on product spaces of homogeneous type via wavelet coefficients, establishes atomic characterization, and proves boundedness results for operators.


<details>
  <summary>Details</summary>
Motivation: To develop a theory of product Hardy spaces on product spaces of homogeneous type that extends classical product Hardy space theory to more general settings, with applications to operator boundedness.

Method: Uses wavelet coefficients on product spaces of homogeneous type, introduces product Carleson measure spaces as test function spaces, establishes a new discrete product Calderón-type reproducing formula with bounded support, and develops atomic decomposition theory.

Result: Successfully defines product Hardy spaces H^p(X_1×X_2), establishes atomic characterization, proves boundedness of linear operators from product Hardy spaces to Lebesgue spaces, and shows boundedness of product Calderón-Zygmund operators on product Hardy spaces.

Conclusion: The paper provides a comprehensive framework for product Hardy spaces on product spaces of homogeneous type with wavelet-based approach, atomic decomposition, and operator theory, overcoming limitations of traditional wavelet reproducing formulas through a new Calderón-type formula.

Abstract: Let $i\in\{1,2\}$ and $X_i$ be a space of homogeneous type in the sense of Coifman and Weiss with the upper dimension $ω_i$. Also let $η_i$ be the smoothness index of the Auscher--Hytönen wavelet function $ψ^{k_i}_{α_i}$ on $X_i$. In this article, for any $p\in(\max\{\frac{ω_1}{ω_1+η_1},\frac{ω_2}{ω_2+η_2}\}, 1]$, by regarding the product Carleson measure space $\mathrm{CMO}^p_{L^2}(X_1\times X_2)$ as the test function space and its dual space $(\mathrm{CMO}^p_{L^2}(X_1\times X_2))'$ as the corresponding distribution space, we introduce the product Hardy space $H^p(X_1\times X_2)$ in terms of wavelet coefficients. Moreover, we establish an atomic characterization of this product Hardy space and, as an application, obtain a criterion for the boundedness of linear operators from product Hardy spaces to corresponding Lebesgue spaces. To escape the wavelet reproducing formula, which is not useful for this atomic characterization because the wavelets have no bounded support, we establish a new discrete product Calderón-type reproducing formula, which holds in the product Hardy space and has bounded support. This reproducing formula also leads to the boundedness of product Calderón--Zygmund operators on the product Hardy space.

</details>


### [44] [Smoothing on $L^1$ for ground state transformed semigroups in non-local settings](https://arxiv.org/abs/2602.17178)
*Miłosz Baraniewicz,Kamil Kaleta*

Main category: math.FA

TL;DR: The paper studies L¹-smoothing properties of semigroups from ground state transformations of Schrödinger semigroups with confining potentials and non-local Lévy operators, where ultracontractivity and hypercontractivity fail.


<details>
  <summary>Details</summary>
Motivation: Inspired by Talagrand's convolution conjecture and developments on classical Ornstein-Uhlenbeck semigroups, the research aims to understand L¹-smoothing properties when standard contraction properties fail, providing estimates that depend on potential and Lévy measures.

Method: The authors study semigroups arising from ground state transformation of Schrödinger semigroups with confining potentials associated with non-local Lévy operators. Their framework is general, encompassing fractional and relativistic Laplacians as kinetic operators.

Result: The paper provides L¹-smoothing estimates with clear dependence on potential and Lévy measure, describing semigroup action on L¹ in terms of Orlicz spaces. Results show L¹-regularizing effects strengthen as time increases to infinity.

Conclusion: The work establishes L¹-smoothing properties for a broad class of semigroups where ultracontractivity fails, with explicit dependence on potential and Lévy measure, and demonstrates strengthening regularization over time through numerous examples.

Abstract: We study the $L^1$-smoothing properties for a broad class of semigroups arising from the ground state transformation of Schrödinger semigroups with confining potentials associated with non-local Lévy operators, for which (asymptotic) ultracontractivity and hypercontractivity fail. Our work is inspired by Talagrand's convolution conjecture in the discrete cube setting, as well as by subsequent developments on the classical Ornstein--Uhlenbeck semigroup. The estimates we provide exhibit a clear dependence on the potential and the Lévy measure defining the kinetic term operator, and they yield a description of the semigroups' action on $L^1$ in terms of Orlicz spaces. Our framework is quite general, encompassing fractional and relativistic Laplacians as kinetic operators. The results are illustrated by numerous examples demonstrating that the $L^1$-regularizing effects become stronger as $t \uparrow \infty$.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling](https://arxiv.org/abs/2602.17089)
*Xinghao Dong,Huchen Yang,Jin-long Wu*

Main category: cs.LG

TL;DR: Flow matching in latent space enables fast single-step sampling for stochastic closure models, 100x faster than diffusion models, with regularization techniques to maintain physical fidelity.


<details>
  <summary>Details</summary>
Motivation: Diffusion models show promise for stochastic closure models due to high-quality diverse samples, but their slow sampling speed is a major limitation. The paper aims to find faster alternatives while maintaining physical fidelity.

Method: Systematic comparison of transport-based generative models on 2D Kolmogorov flows. Uses flow matching in lower-dimensional latent space for single-step sampling. Implements explicit regularization (metric-preserving and geometry-aware constraints) and implicit regularization via joint training to control latent space distortion.

Result: Flow matching in latent space achieves up to two orders of magnitude faster sampling than iterative diffusion approaches. Both explicit and implicit regularization preserve topological information from the original dynamical system, enabling learning with less training data.

Conclusion: Latent space flow matching with appropriate regularization offers a promising approach for fast, physically-faithful stochastic closure modeling, overcoming diffusion models' sampling speed limitations while maintaining diversity and requiring less training data.

Abstract: Diffusion models recently developed for generative AI tasks can produce high-quality samples while still maintaining diversity among samples to promote mode coverage, providing a promising path for learning stochastic closure models. Compared to other types of generative AI models, such as GANs and VAEs, the sampling speed is known as a key disadvantage of diffusion models. By systematically comparing transport-based generative models on a numerical example of 2D Kolmogorov flows, we show that flow matching in a lower-dimensional latent space is suited for fast sampling of stochastic closure models, enabling single-step sampling that is up to two orders of magnitude faster than iterative diffusion-based approaches. To control the latent space distortion and thus ensure the physical fidelity of the sampled closure term, we compare the implicit regularization offered by a joint training scheme against two explicit regularizers: metric-preserving (MP) and geometry-aware (GA) constraints. Besides offering a faster sampling speed, both explicitly and implicitly regularized latent spaces inherit the key topological information from the lower-dimensional manifold of the original complex dynamical system, which enables the learning of stochastic closure models without demanding a huge amount of training data.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [46] [Universal Fine-Grained Symmetry Inference and Enforcement for Rigorous Crystal Structure Prediction](https://arxiv.org/abs/2602.17176)
*Shi Yin,Jinming Mu,Xudong Zhu,Lixin He*

Main category: cond-mat.mtrl-sci

TL;DR: A new crystal structure prediction method using LLMs to generate Wyckoff patterns and constrained optimization for symmetry consistency, achieving state-of-the-art performance without relying on existing databases.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for crystal structure prediction treat symmetry as soft heuristics or rely on database templates, limiting physical fidelity and ability to discover genuinely new structures. There's a need for methods that can explore uncharted materials space without relying on existing structural knowledge.

Method: Uses large language models to encode chemical semantics and generate fine-grained Wyckoff patterns directly from composition. Incorporates domain knowledge through efficient constrained-optimization search that enforces algebraic consistency between site multiplicities and atomic stoichiometry. Integrates symmetry-consistent template into a diffusion backbone to constrain generative trajectory to physically valid geometric manifold.

Result: Achieves state-of-the-art performance across stability, uniqueness, and novelty (SUN) benchmarks, with superior matching performance. Establishes a new paradigm for rigorous exploration of targeted crystallographic space.

Conclusion: The framework enables efficient expansion into previously uncharted materials space without reliance on existing databases or a priori structural knowledge, representing a significant advancement in crystal structure prediction methodology.

Abstract: Crystal structure prediction (CSP), which aims to predict the three-dimensional atomic arrangement of a crystal from its composition, is central to materials discovery and mechanistic understanding. Existing deep learning models often treat crystallographic symmetry only as a soft heuristic or rely on space group and Wyckoff templates retrieved from known structures, which limits both physical fidelity and the ability to discover genuinely new material structures. In contrast to retrieval-based methods, our approach leverages large language models to encode chemical semantics and directly generate fine-grained Wyckoff patterns from composition, effectively circumventing the limitations inherent to database lookups. Crucially, we incorporate domain knowledge into the generative process through an efficient constrained-optimization search that rigorously enforces algebraic consistency between site multiplicities and atomic stoichiometry. By integrating this symmetry-consistent template into a diffusion backbone, our approach constrains the stochastic generative trajectory to a physically valid geometric manifold. This framework achieves state-of-the-art performance across stability, uniqueness, and novelty (SUN) benchmarks, alongside superior matching performance, thereby establishing a new paradigm for the rigorous exploration of targeted crystallographic space. This framework enables efficient expansion into previously uncharted materials space, eliminating reliance on existing databases or a priori structural knowledge.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [47] [AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing](https://arxiv.org/abs/2602.17607)
*Jianda Du,Youran Sun,Haizhao Yang*

Main category: cs.AI

TL;DR: AutoNumerics is a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for PDEs from natural language descriptions, generating transparent solvers grounded in classical numerical analysis rather than black-box neural approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional PDE solver design requires substantial mathematical expertise and manual tuning, while recent neural network approaches are computationally expensive and lack interpretability. There's a need for accessible, automated PDE solving that maintains transparency.

Method: Multi-agent framework with coarse-to-fine execution strategy and residual-based self-verification mechanism. The system generates transparent solvers grounded in classical numerical analysis directly from natural language PDE descriptions.

Result: Experiments on 24 canonical and real-world PDE problems show competitive or superior accuracy compared to existing neural and LLM-based baselines. The framework correctly selects numerical schemes based on PDE structural properties.

Conclusion: AutoNumerics demonstrates viability as an accessible paradigm for automated PDE solving, offering transparent, interpretable solvers that bridge the gap between traditional numerical analysis and modern AI approaches.

Abstract: PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [48] [Modeling of Relativistic Plasmas with a Conservative Discontinuous Galerkin Method](https://arxiv.org/abs/2602.17487)
*James Juno,Grant Johnson,Alexander Philippov,Ammar Hakim,Alexander Chernoglazov,Shuzhe Zeng*

Main category: astro-ph.HE

TL;DR: A noise-free, high-order discontinuous Galerkin method for solving relativistic Vlasov-Maxwell equations on phase-space grids, using adaptive velocity mapping for extreme energy scales.


<details>
  <summary>Details</summary>
Motivation: Traditional Monte-Carlo methods suffer from Poisson noise, limiting insight into plasma dynamics. Need for noise-free approach to study extreme high-energy-density environments like QED pair-production, neutron star magnetospheres, and relativistic reconnection.

Method: Direct discretization of kinetic equation on high-dimensional phase-space grid using discontinuous Galerkin finite elements. Novel velocity-space mapping technique handles wide energy scales efficiently.

Result: Conservative, high-order numerical scheme free from Poisson noise. Enables treatment of relativistic plasmas across extreme energy scales characteristic of astrophysical and laboratory environments.

Conclusion: The noise-free approach provides unique insight into plasma dynamics, enabling detailed analysis of electromagnetic emission and fine-scale phase-space structure in extreme high-energy-density systems.

Abstract: We present a new method for solving the relativistic Vlasov--Maxwell system of equations, applicable to a wide range of extreme high-energy-density astrophysical and laboratory environments. The method directly discretizes the kinetic equation on a high-dimensional phase-space grid using a discontinuous Galerkin finite element approach, yielding a high-order, conservative numerical scheme that is free from the Poisson noise inherent to traditional Monte-Carlo methods. A novel and flexible velocity-space mapping technique enables the efficient treatment of the wide range of energy scales characteristic of relativistic plasmas, including QED pair-production discharges, instabilities in strongly magnetized plasmas surrounding neutron stars, and relativistic magnetic reconnection. Our noise-free approach is capable of providing unique insight into plasma dynamics, enabling detailed analysis of electromagnetic emission and fine-scale phase-space structure.

</details>


### [49] [Theory of striped dynamic spectra of the Crab pulsar high-frequency interpulse](https://arxiv.org/abs/2602.16955)
*Mikhail V. Medvedev*

Main category: astro-ph.HE

TL;DR: The paper develops a theory explaining the zebra pattern in Crab pulsar's high-frequency interpulse radio emission as interference maxima from multiple ray propagation through the magnetosphere, combining gravitational lensing and plasma de-lensing effects.


<details>
  <summary>Details</summary>
Motivation: To explain the observed spectral zebra pattern in the Crab pulsar's high-frequency interpulse radio emission and use it as a tool for magnetospheric tomography.

Method: Developed a theoretical model where the zebra pattern results from interference maxima caused by multiple ray propagation through the pulsar magnetosphere, combining gravitational lensing and plasma de-lensing effects.

Result: The model enables space-resolved tomography of the pulsar magnetosphere, yielding a radial plasma density profile of n_e ∝ r^{-3} that agrees with theory. Predicts zebra pattern change at 42-650 GHz when ray separation becomes smaller than pulsar size.

Conclusion: The zebra pattern theory provides a tool for magnetospheric tomography and predicts observable frequency changes within reach of existing facilities like ALMA and SMA, offering opportunities to study both magnetospheric physics and strong-field gravity near neutron stars.

Abstract: A theory of the spectral "zebra" pattern of the Crab pulsar's high-frequency interpulse (HFIP) radio emission is developed. The observed emission bands are interference maxima caused by multiple ray propagation through the pulsar magnetosphere. The high-contrast interference pattern is the combined effect of gravitational lensing and plasma de-lensing of light rays. The model enables space-resolved tomography of the pulsar magnetosphere, yielding a radial plasma density profile of $n_{e}\propto r^{-3}$, which agrees with theoretical insights. We predict the zebra pattern trend to change at a higher frequency when the ray separation becomes smaller than the pulsar size. This frequency is predicted to be in the range between 42 GHz and 650 GHz, which is within the reach of existing facilities like ALMA and SMA. These observations hold significant importance and would contribute to our understanding of the magnetosphere. Furthermore, they offer the potential to investigate gravity in the strong field regime near the star's surface.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [50] [Order of Magnitude Analysis and Data-Based Physics-Informed Symbolic Regression for Turbulent Pipe Flow](https://arxiv.org/abs/2602.17082)
*Yunus Emre Ünal,Özgür Ertunç,Ismail Ari,Ivan Otić*

Main category: physics.flu-dyn

TL;DR: Deriving scaling relations from fluid dynamics to constrain symbolic regression for friction factor correlations that fit experimental data across roughness levels and Reynolds numbers.


<details>
  <summary>Details</summary>
Motivation: Existing semi-empirical correlations like Colebrook-White don't fully replicate Nikuradse's rough-pipe experiments, creating a need for more physically-grounded friction factor predictions.

Method: Order-of-magnitude analysis of Reynolds-averaged Navier-Stokes and kinetic-energy transport equations to derive scaling relations, then using modified genetic programming (GPTIPS2) with experimental data to find compact correlations that satisfy physical constraints.

Result: Developed interpretable expressions that accurately reproduce friction factors across various roughness levels and Reynolds numbers, validated up to Re ~ 10^7.

Conclusion: Combining physical scaling relations with symbolic regression yields accurate, interpretable friction factor correlations that adhere to fundamental fluid dynamics constraints while fitting experimental data.

Abstract: Friction losses in rough pipes are often predicted using semi-empirical correlations, such as the Colebrook-White equation (Colebrook,1939), which do not fully replicate Nikuradse's rough-pipe experiments (1950). This study derives scaling relations for the viscous and turbulent contributions to the streamwise pressure drop through an order-of-magnitude analysis of the Reynolds-averaged Navier-Stokes equations and the kinetic-energy transport equations. These relations impose constraints on the local sensitivity of the pressure drop to factors such as mean velocity, roughness, viscosity, and density through exponent envelopes and serve as a physical prior for symbolic regression. By combining Nikuradse's rough-pipe and smooth-pipe data of Zagarola and Smits (1998), we aim to derive compact correlations for the friction factor that fit experimental data while adhering to the derived constraints. A modified genetic programming engine (GPTIPS2) optimizes model structure and evaluates it based on fitness, complexity, and constraint violation. This method yields interpretable expressions that accurately reproduce friction factors across various roughness levels and Reynolds numbers, validated up to $Re \sim 10^7$.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [51] [Semi-Local Exchange-Correlation Approximations in Density Functional Theory](https://arxiv.org/abs/2602.17333)
*Fabien Tran,Susi Lehtola,Stefano Pittalis,Miguel A. L. Marques*

Main category: physics.chem-ph

TL;DR: A comprehensive review of semi-local density functional approximations (LDA, GGA, meta-GGA) covering theoretical foundations, historical development, and practical applications in computational chemistry and physics.


<details>
  <summary>Details</summary>
Motivation: Density functional theory is essential for electronic structure calculations but relies on approximations of the unknown exact exchange-correlation functional. Hundreds of approximations exist with varying accuracy and efficiency, requiring a unified framework to understand their development and application.

Method: The paper provides a systematic review of semi-local functionals, starting with Kohn-Sham DFT foundations, then covering construction principles of local density approximations (LDA), generalized gradient approximations (GGA), and meta-generalized gradient approximations. It organizes historical developments and recent advances with attention to physical motivations, mathematical properties, and practical considerations.

Result: A comprehensive reference that consolidates extensive literature on semi-local functionals, providing a unified framework for understanding their construction and application across different chemical and physical systems.

Conclusion: This review serves as both an introduction for newcomers and a reference for practitioners, aiming to facilitate further developments in density functional approximations and their application to modern computational chemistry and condensed matter physics challenges.

Abstract: Density functional theory is the workhorse of modern electronic structure calculations, with wide-ranging applications in chemistry, physics, materials science, and machine learning. At its heart lies the exchange-correlation functional, a quantity which exactly encapsulates the many-body effects stemming from the quantum mechanical interactions between the electrons. Yet, the exact functional is unknown, and computationally tractable approximations are therefore necessary for practical applications. Over the past six decades, hundreds of density functional approximations have been proposed with varying degrees of accuracy and computational efficiency.
  This review surveys the theoretical foundations of semi-local functionals, including local density approximations, generalized gradient approximations, and meta-generalized gradient approximations. We provide a comprehensive, consistently organized discussion that consolidates both historical developments and recent advances in this field. Beginning with the essential concepts of Kohn-Sham density functional theory, we present the construction principles of semi-local exchange-correlation functionals. Special attention is given to the physical motivations underlying functional development, the mathematical properties that guide their construction, and the practical considerations that determine their applicability across different chemical and physical systems.
  This work is intended to serve as both a introduction for newcomers to the field and a comprehensive reference for practitioners. By consolidating the extensive literature on semi-local functionals and providing a unified framework for understanding their construction and application, we aim to facilitate further developments in density functional approximations and their use in tackling the diverse challenges of modern computational chemistry and condensed matter physics.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [52] [Optimal control of stochastic Volterra integral equations with completely monotone kernels and stochastic differential equations on Hilbert spaces with unbounded control and diffusion operators](https://arxiv.org/abs/2602.17578)
*Gabriele Bolli,Filippo de Feo*

Main category: math.OC

TL;DR: This paper develops a framework for optimal control of stochastic Volterra integral equations (SVIEs) with completely monotone kernels by lifting them to infinite-dimensional SDEs on Hilbert spaces and establishing existence/uniqueness results for the associated HJB equations.


<details>
  <summary>Details</summary>
Motivation: Optimal control of SVIEs with completely monotone kernels faces deep mathematical difficulties despite their importance in various fields, particularly in mathematical finance under rough volatility models. Existing dynamic programming approaches struggle with these problems due to the roughness of kernels.

Method: The authors use a Markovian lift to reformulate SVIE control problems as optimal control of SDEs on Hilbert spaces. They then develop a general theory for abstract SDEs with unbounded control/diffusion operators, analyzing the Ornstein-Uhlenbeck transition semigroup's Γ-smoothing property to handle the HJB equations.

Result: The paper establishes existence and uniqueness of mild solutions for the Hamilton-Jacobi-Bellman equation, proves a verification theorem, and constructs optimal feedback controls. These results are applied to SVIEs with completely monotone kernels, representing the first such results for this class of problems.

Conclusion: The developed framework successfully addresses optimal control of SVIEs with completely monotone kernels by lifting them to infinite-dimensional settings and establishing rigorous mathematical foundations for solving the associated HJB equations and constructing optimal controls.

Abstract: The dynamic programming approach is one of the most powerful ones in optimal control. However, when dealing with optimal control problems of stochastic Volterra integral equations (SVIEs) with completely monotone kernels, deep mathematical difficulties arise and it is still not understood. These very classical problems have applications in most fields and have now become even more popular due to their applications in mathematical finance under rough volatility. In this article, we consider a class of optimal control problems of SVIEs with completely monotone kernels. Via a recent Markovian lift \cite{FGW2024}, the problem can be reformulated as an optimal control problem of stochastic differential equations (SDEs) on suitable Hilbert spaces, which due to the roughness of the kernel, presents a generator of an analytic semigroup and unbounded control and diffusion operators.
  This analysis leads us to study a general class of optimal control problems of abstract SDEs on Hilbert spaces with unbounded control and diffusion operators. This class includes optimal control problems of SVIEs with completely monotone kernels, but it is also motivated by other models. We analyze the regularity of the associated Ornstein-Uhlenbeck transition semigroup. We prove that the semigroup exhibits a new smoothing property in control directions through a general observation operator $Γ$, which we call $Γ$-smoothing. This allows us to establish existence and uniqueness of mild solutions of the Hamilton-Jacobi-Bellman equation, establish a verification theorem, and construct optimal feedback controls. We apply these results to optimal control problems of SVIEs with completely monotone kernels. To the best of our knowledge these are the first results of this kind for this abstract class of infinite dimensional problems and for the optimal control of SVIEs with completely monotone kernels.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [53] [A Lower Bound for the First Non-zero Basic Eigenvalue on a Singular Riemannian Foliation](https://arxiv.org/abs/2602.17501)
*Bach Tran*

Main category: math.DG

TL;DR: Generalized eigenvalue bounds for singular Riemannian foliations with basic mean curvature, extending classical Zhong-Yang and Shi-Yang estimates, plus rigidity results.


<details>
  <summary>Details</summary>
Motivation: To extend classical eigenvalue estimates (Zhong-Yang and Shi-Yang) from smooth Riemannian manifolds to singular Riemannian foliations with basic mean curvature, establishing lower bounds for the first non-zero basic eigenvalue.

Method: Uses geometric analysis on singular Riemannian manifolds with foliations, establishing relationships between Ricci curvature bounds, diameter of leaf space, and basic eigenvalues through analytical techniques.

Result: Derived lower bounds for the first non-zero basic eigenvalue based on Ricci curvature and leaf space diameter, and proved a rigidity theorem showing that when eigenvalue equals π²/d², the manifold is isometric to a mapping torus of an isometry.

Conclusion: Successfully generalized classical eigenvalue estimates to singular Riemannian foliations with basic mean curvature, providing both quantitative bounds and qualitative rigidity results that characterize extremal cases.

Abstract: In this paper, we provide the lower bounds of the first non-zero basic eigenvalue on a closed singular Riemannian manifold $(M,\mathcal{F})$ with basic mean curvature that depends on the given non-negative lower bound of the Ricci curvature of $M$ and the diameter of the leaf space $M/\mathcal{F}$. These can be regarded as generalized versions of the Zhong-Yang estimate and a generalized Shi-Yang's estimate for singular Riemannian foliations with basic mean curvature. We also provide a rigidity result corresponding to the generalized Zhong-Yang estimate, which is a generalized Hang-Wang rigidity for singular Riemannian foliations with basic mean curvature. More precisely, when the first basic eigenvalue $λ_1^B$ is equal to $\frac{π^2}{d_{M/\mathcal{F}^2}} $, where $d_{M/\mathcal{F}}$ is the diameter of the leaf space, $M$ is isometric to a mapping torus of an isometry $\varphi:N\to N$ where $N$ is an $(n-1)$-dimensional Riemannian manifold of nonnegative Ricci curvature and $\mathcal{F}$ has the form $\{[\{\text{point}\}\times N]\}$.

</details>


### [54] [Distance Functions, Curvature and Topology](https://arxiv.org/abs/2602.17629)
*Carlo Mantegazza,Francesca Oronzio*

Main category: math.DG

TL;DR: The paper explores properties of distance functions on Riemannian manifolds and their relationship to manifold geometry, providing alternative proofs of classical curvature-topology theorems.


<details>
  <summary>Details</summary>
Motivation: To investigate the properties of distance functions on Riemannian manifolds and understand how their behavior relates to the underlying geometry of the manifolds.

Method: Analyzing distance functions on Riemannian manifolds and establishing connections between their properties and geometric characteristics of the manifolds.

Result: The study leads to alternative proofs of classical theorems that connect curvature and topology in Riemannian geometry.

Conclusion: Distance functions provide a useful perspective for understanding the relationship between curvature and topology in Riemannian manifolds, offering alternative approaches to established theorems.

Abstract: We discuss some properties of the distance functions on Riemannian manifolds and we relate their behavior to the geometry of the manifolds. This leads to alternative proofs of some "classical" theorems connecting curvature and topology.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [55] [On the Concept of Violence: A Comparative Study of Human and AI Judgments](https://arxiv.org/abs/2602.17256)
*Mariachiara Stellato,Francesco Lancia,Chiara Galeazzi,Nico Curti*

Main category: physics.soc-ph

TL;DR: This study compares human and LLM classifications of violence across 22 morally ambiguous scenarios to understand how AI systems operationalize moral constructs and potentially reshape human conceptions of harm.


<details>
  <summary>Details</summary>
Motivation: The research addresses the growing societal debate about what constitutes violence (beyond physical aggression) and the increasing role of LLMs in interpreting complex social behaviors. It investigates whether AI systems reproduce, reshape, or simplify human conceptions of violence as they mediate everyday moral judgments.

Method: Systematic comparison between human judgments and multiple instruction-tuned LLM classifications across 22 carefully designed morally dividing scenarios. The study includes global, sentence-level, and thematic-domain analyses, examining variability across models of different sizes and architectures to assess convergence and divergence patterns.

Result: The study uses violence as a proxy to observe broader belief formation dynamics, revealing how LLMs operationalize ambiguous moral constructs, negotiate conceptual boundaries, and transform plural human interpretations into singular outputs. The findings contribute to understanding AI's epistemic role in shaping interpretations of harm, responsibility, and social norms.

Conclusion: The research highlights the importance of transparency and critical engagement as LLMs increasingly mediate public reasoning about moral concepts like violence. It demonstrates that violence serves as a valuable tool for investigating how AI systems process and potentially reshape complex human moral judgments.

Abstract: Background: What counts as violence is neither self-evident nor universally agreed upon. While physical aggression is prototypical, contemporary societies increasingly debate whether exclusion, humiliation, online harassment or symbolic acts should be classified within the same moral category. At the same time, Large Language Models (LLMs) are being consulted in everyday contexts to interpret and label complex social behaviors. Whether these systems reproduce, reshape or simplify human conceptions of violence remains an open question. Methods: Here we present a systematic comparison between human judgements and LLM classifications across 22 scenarios carefully designed to be morally dividing, spanning from physical and verbally aggressive behavior, relational dynamics, marginalization, symbolic actions and verbal expressions. Human responses were compared with outputs from multiple instruction-tuned models of varying sizes and architectures. We conducted global, sentence-level and thematic-domain analyses, and examined variability across models to assess patterns of convergence and divergence. Findings: This study treats violence as a strategically chosen proxy through which broader belief formation dynamics can be observed. Violence is not the focus of the study, but it serves as a tool to investigate broader analysis. It enables a structured investigation of how LLMs operationalize ambiguous moral constructs, negotiate conceptual boundaries, and transform plural human interpretations into singular outputs. More broadly, the findings contribute to ongoing debates about the epistemic role of conversational AI in shaping everyday interpretations of harm, responsibility and social norms, highlighting the importance of transparency and critical engagement as these systems increasingly mediate public reasoning.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [56] [Linear Landau equation as a limit of a tagged particle in mean field interaction with a free gas](https://arxiv.org/abs/2602.16440)
*Thierry Bodineau,Pierre Le Bris*

Main category: math.PR

TL;DR: Tagged particle in mean field interaction with free gas converges to diffusion process associated with linear Landau equation in dimensions d≥4 as gas density N→∞.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous connection between microscopic particle dynamics and macroscopic diffusion processes described by linear Landau equation, particularly in high dimensions where mean field approximations are valid.

Method: Analyze tagged particle interacting with equilibrium free gas via mean field interactions. Prove convergence using martingale problem approach, relying on long-time stability of microscopic dynamics and control of particle recollision probabilities.

Result: For dimensions d≥4, the trajectory of tagged particle converges to diffusion process associated with linear Landau equation as gas density N→∞.

Conclusion: The paper establishes rigorous derivation of linear Landau equation from microscopic particle dynamics in high dimensions, providing mathematical foundation for mean field approximations in kinetic theory.

Abstract: We consider a tagged particle in mean field interaction with a free gas of density N at equilibrium. In dimensions $d\geq4$, we prove the convergence of its trajectory, as N goes to infinity, to the one of a diffusion process associated with the linear Landau equation. The proof of the convergence of the martingale problem relies on two key ingredients: long time stability results of the microscopic dynamics, and controls on the probability of particle recollisions.

</details>


### [57] [Variance renormalisation in regularity structures -- the case of $2d$ gPAM](https://arxiv.org/abs/2602.17369)
*Máté Gerencsér,Yueh-Sheng Hsu*

Main category: math.PR

TL;DR: The paper addresses variance renormalization for a singular SPDE (2D generalized parabolic Anderson model) with rough noise, requiring both multiplicative and additive renormalization when Da Prato-Debussche trick fails.


<details>
  <summary>Details</summary>
Motivation: To handle singular SPDEs driven by noise rougher than white noise where standard renormalization techniques (Da Prato-Debussche trick) are not applicable, particularly for the 2D generalized parabolic Anderson model.

Method: Uses regularity structures with models that lift zero noises to nontrivial models (analogous to "pure area" in rough paths). Employs BPHZ model over vanishing noise and graphical computations to show convergence.

Result: Demonstrates convergence to the model for the BPHZ model over vanishing noise through graphical computations, handling the discrepancy between approximate and limiting equation regularity structures.

Conclusion: The approach successfully handles variance renormalization for singular SPDEs with rough noise where traditional methods fail, using regularity structures and nontrivial noise lifting models.

Abstract: We consider the variance renormalisation of a singular SPDE for which a Da Prato-Debussche trick is not applicable. The example taken is the $2$-dimensional generalised parabolic Anderson model (gPAM), driven by a much rougher than white noise, necessitating both a multiplicative and an additive renormalisation. To handle the discrepancy between the regularity structures of the approximate and the limiting equations, we consider models that lift $0$ noises to nontrivial models, in analogy with ``pure area'' from rough paths. The convergence to such a model is shown for the BPHZ model over the vanishing noise via graphical computations.

</details>


### [58] [Banach fixed point and flow approach for rough analysis](https://arxiv.org/abs/2602.17437)
*Yvain Bruned,Yingtong Hou,Paul Laubie,Zhicheng Zhu*

Main category: math.PR

TL;DR: The paper shows that the algebraic assumption needed for fixed point arguments in rough differential equations implies the assumption for Bailleul's flow approach, and proves that multi-indices Hopf algebra doesn't satisfy the required cocycle condition.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between algebraic assumptions in different approaches to rough differential equations (fixed point vs Bailleul flow), and to explain why fixed point arguments fail for multi-indices rough paths in practice.

Method: The authors analyze the algebraic structure of rough paths, specifically focusing on Hopf algebras with cocycle conditions. They show that the main algebraic assumption for fixed point arguments implies the assumption for Bailleul's flow approach, and then prove that the Hopf algebra of multi-indices fails to satisfy the cocycle condition.

Result: 1) The algebraic assumption for fixed point arguments implies the assumption for Bailleul's flow approach. 2) The Hopf algebra of multi-indices does not satisfy the required cocycle condition, providing a rigorous explanation for why fixed point arguments fail for multi-indices rough paths in practice.

Conclusion: The paper establishes a connection between algebraic assumptions in different approaches to rough differential equations and provides a rigorous mathematical explanation for the observed impossibility of performing fixed point arguments for multi-indices rough paths and multi-indices in Regularity Structures.

Abstract: In this paper, we show that the main algebraic assumption required to perform a fixed point argument for rough differential equations implies the algebraic assumption for the Bailleul flow approach. This assumption requires that the rough path associated with the equation is given by a Hopf algebra whose coproduct admits a cocycle and has a tree-like basis. We show that the Hopf algebra of multi-indices does not satisfy the cocycle condition. This is a rigorous result on the impossibility, observed in practice, of performing a fixed point argument for multi-indices rough paths and multi-indices in Regularity Structures.

</details>


### [59] [An Allen-Cahn equation with jump-diffusion noise for biological damage and repair processes](https://arxiv.org/abs/2602.17495)
*Andrea Di Primio,Marvin Fritz,Luca Scarpa,Margherita Zanella*

Main category: math.PR

TL;DR: Stochastic Allen-Cahn equation with multiplicative Wiener and jump noise models biomolecular damage/repair dynamics, proving well-posedness, invariant measures, ergodicity, and simulation of damage clustering.


<details>
  <summary>Details</summary>
Motivation: To model biomolecular damage and repair dynamics, capturing both continuous stochastic fluctuations (background noise) and abrupt localized damage from external shocks, using phase-separation dynamics with singular drift.

Method: Stochastic Allen-Cahn equation driven by multiplicative cylindrical Wiener process (continuous noise) and jump-type noise (abrupt damage), with singular drift covering logarithmic Flory-Huggins potential. Proves well-posedness, analyzes long-time behavior, and implements Euler-Maruyama scheme for simulation.

Result: Proves strong probabilistic well-posedness of the model, establishes existence and uniqueness of invariant measures, demonstrates ergodicity and mixing properties, and shows simulations capture biological phenomena like damage clustering and stress-induced topology perturbations.

Conclusion: The stochastic Allen-Cahn framework successfully models biomolecular damage/repair dynamics with both continuous and jump noise, providing rigorous mathematical foundations and computational tools to study fundamental biological damage phenomena.

Abstract: This paper analyzes a stochastic Allen--Cahn equation for the dynamics of biomolecular damage and repair. The system is driven by two distinct noise processes: a multiplicative cylindrical Wiener process, modeling continuous background stochastic fluctuations, and a jump-type noise, modeling the abrupt, localized damage induced by external shocks. The drift of the equation is singular and covers the typical logarithmic Flory-Huggins potential required in phase-separation dynamics. We prove well-posedness of the model in a strong probabilistic sense, and analyze its long-time behavior in terms of existence and uniqueness of invariant measures, ergodicity, and mixing properties. Eventually, we present an Euler--Maruyama scheme to simulate the model and illustrate how it captures fundamental biological phenomena, such as damage clustering, stress-induced topology perturbations, and damage dynamics.

</details>
