<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 16]
- [math.AP](#math.AP) [Total: 6]
- [physics.comp-ph](#physics.comp-ph) [Total: 7]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 7]
- [math.OC](#math.OC) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [nlin.SI](#nlin.SI) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [q-bio.PE](#q-bio.PE) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Temporal-Stability-Enhanced and Energy-Stable Dynamical Low-Rank Approximation for Multiscale Linear Kinetic Transport Equations](https://arxiv.org/abs/2602.12337)
*Shun Li,Yan Jiang,Mengping Zhang,Tao Xiong*

Main category: math.NA

TL;DR: Asymptotic-preserving dynamical low-rank method for multiscale linear kinetic transport equation that achieves unconditional stability in diffusive regime with computational efficiency through low-rank representation.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for multiscale linear kinetic transport equations that can handle different regimes (especially diffusive regime) while maintaining stability and capturing correct asymptotic behavior.

Method: Develops an asymptotic-preserving dynamical low-rank method with low-rank formulation consistent with discrete energy under discrete ordinates discretization, ensuring energy stability.

Result: The proposed scheme is unconditionally stable in diffusive regime, preserves correct asymptotic behavior, achieves significant computational cost reduction through low-rank representation and large time step stability, with proven energy stability.

Conclusion: Numerical experiments confirm energy stability and demonstrate method efficiency while maintaining accuracy across different regimes and capturing correct asymptotic limits.

Abstract: In this paper, we develop an asymptotic-preserving dynamical low-rank method for the multiscale linear kinetic transport equation. The proposed scheme is unconditionally stable in the diffusive regime while preserving the correct asymptotic behavior, and can achieve significant reductions in computational cost through a low-rank representation and large time step stability. A low-rank formulation consistent with the discrete energy is introduced under the discrete ordinates discretization, and energy stability of the resulting scheme is established. Numerical experiments confirm the energy stability and demonstrate that the method is efficient while maintaining accuracy across different regimes and capturing the correct asymptotic limits.

</details>


### [2] [A versatile FEM framework with native GPU scalability via globally-applied AD](https://arxiv.org/abs/2602.12365)
*Mohit Pundir,Flavio Lorez,David S. Kammer*

Main category: math.NA

TL;DR: tatva is an energy-centric finite-element framework that applies global Automatic Differentiation to generate residuals and tangents, enabling scalable GPU-based solutions for large problems with millions of DOFs while supporting complex physics and couplings.


<details>
  <summary>Details</summary>
Motivation: Current energy-based finite-element frameworks face a trade-off: global AD offers expressivity but doesn't scale, while local AD scales but limits physics variety. There's a need for a framework that combines global AD's expressivity with scalability for large problems.

Method: tatva defines physics as a single global functional and applies AD globally. It uses Jacobian-vector products for matrix-free solvers and coloring-based sparse differentiation for materializing sparse tangent matrices. The design leverages GPU acceleration for linear scaling.

Result: The framework scales linearly with problem size on GPUs, handles millions of DOFs without memory exhaustion, and supports complex features like multi-point constraints, mixed-dimensional coupling, and neural network integration while maintaining high performance.

Conclusion: tatva successfully bridges the gap between expressivity and scalability in energy-based finite-element formulations, providing a unified differentiable methodology that can address diverse problems while maintaining GPU performance for large-scale applications.

Abstract: Energy-based finite-element formulations provide a unified framework for describing complex physical systems in computational mechanics. In these energy-based methods, the governing equations can be obtained directly by considering the derivatives of a single global energy functional. While Automatic Differentiation (AD) can be used to automate the generation of these derivatives, current frameworks face a clear trade-off based primarily on the scale upon which the AD method is applied. Globally applied AD offers high expressivity but cannot currently be scaled to large problems. Locally applied AD scales well through traditional assembly methods, but the variety of physics and couplings that the framework can easily represent is more limited than the global approach. Here, we introduce an energy-centric framework tatva (https://github.com/smec-ethz/tatva) that defines the physics of a problem as a single global functional and applies AD globally to generate residual and tangent operators. By leveraging Jacobian-vector products for matrix-free solvers and coloring-based sparse differentiation for materializing sparse tangent stiffness matrices when needed, our flexible design scales linearly with the problem size on GPUs. We demonstrate that our framework can handle large problems (with millions of degrees of freedom) without memory exhaustion. Additionally, it offers a unified, fully differentiable methodology that can address a wide range of problems, including multi-point constraints, mixed-dimensional coupling, and the incorporation of neural networks, while maintaining high performance and scalability on modern GPU architectures.

</details>


### [3] [Dirichlet-Neumann Waveform Relaxation Method with Multiple Subdomains for Reaction-Diffusion Equation with a Time Delay](https://arxiv.org/abs/2602.12409)
*Bankim C. Mandal,Deeksha Tomer*

Main category: math.NA

TL;DR: Numerical study of Dirichlet-Neumann Waveform Relaxation (DNWR) algorithm for reaction-diffusion equations with time delay across multiple subdomains, comparing different transmission condition arrangements.


<details>
  <summary>Details</summary>
Motivation: To investigate efficient domain decomposition methods for solving reaction-diffusion equations with time delay, which are challenging due to their complex temporal and spatial coupling, by exploring different transmission condition arrangements between subdomains.

Method: Numerical investigation using Dirichlet-Neumann Waveform Relaxation (DNWR) algorithm applied to multiple subdomains for reaction-diffusion equations with time delay. Various arrangements of transmission conditions between subdomains are explored and compared through numerical experiments.

Result: Numerical experiments evaluate and compare the efficiency and effectiveness of different transmission condition configurations, though specific quantitative results are not provided in the abstract.

Conclusion: The study provides insights into optimal transmission condition arrangements for DNWR algorithms applied to reaction-diffusion equations with time delay, contributing to improved domain decomposition methods for such complex problems.

Abstract: In this study, we present the numerical investigation of the Dirichlet-Neumann Waveform Relaxation (DNWR) algorithm applied to multiple subdomains for the reaction-diffusion equation with time delay. Various arrangements of transmission conditions between subdomains are explored and a series of numerical experiments are conducted to evaluate and compare the efficiency and effectiveness of these configurations.

</details>


### [4] [Quantile Randomized Kaczmarz Algorithm with Whitelist Trust Mechanism](https://arxiv.org/abs/2602.12483)
*Sofiia Shvaiko,Longxiu Huang,Elizaveta Rebrova*

Main category: math.NA

TL;DR: QRK algorithm for solving linear systems with corrupted equations gets faster convergence with lower corruption rates, and proposed improvements include online row detection and subsampling for practical efficiency.


<details>
  <summary>Details</summary>
Motivation: Randomized Kaczmarz (RK) is fast for consistent overdetermined systems but fragile under noise, especially when equations are corrupted. Need robust methods for systems with sparse corruption where only noisy observations are available.

Method: Reanalyze QuantileRK (QRK) showing convergence improves with lower corruption fraction β; propose online detector to flag/remove unreliable rows; estimate quantiles from small random subsample of rows to reduce computational cost while preserving robustness.

Result: Convergence rate of QRK improves monotonically as corruption fraction β decreases; online detector reduces effective β and speeds convergence; subsampling preserves robustness while lowering per-iteration cost; simulations on imaging and synthetic data demonstrate efficiency.

Conclusion: The proposed enhancements make QRK practical by addressing computational bottlenecks while maintaining robustness against sparse corruption in overdetermined linear systems.

Abstract: Randomized Kaczmarz (RK) is a simple and fast solver for consistent overdetermined systems, but it is known to be fragile under noise. We study overdetermined $m\times n$ linear systems with a sparse set of corrupted equations, $ {\bf A}{\bf x}^\star = {\bf b}, $where only $\tilde{\bf b} = {\bf b} + \boldsymbol{\varepsilon}$ is observed with $\|\boldsymbol{\varepsilon}\|_0 \le βm$. The recently introduced QuantileRK (QRK) algorithm addresses this issue by testing residuals against a quantile threshold, but computing a per-iteration quantile across many rows is costly. In this work we (i) reanalyze QRK and show that its convergence rate improves monotonically as the corruption fraction $β$ decreases; (ii) propose a simple online detector that flags and removes unreliable rows, which reduces the effective $β$ and speeds up convergence; and (iii) make the method practical by estimating quantiles from a small random subsample of rows, preserving robustness while lowering the per-iteration cost. Simulations on imaging and synthetic data demonstrate the efficiency of the proposed method.

</details>


### [5] [Proving periodic solutions and branches in the 2D Swift Hohenberg PDE with hexagonal and triangular symmetry](https://arxiv.org/abs/2602.12491)
*Dominic Blanco*

Main category: math.NA

TL;DR: The paper develops rigorous computational methods to prove existence of periodic solutions in PDEs with hexagonal and triangular symmetries using Fourier series on hexagonal lattices and Newton-Kantorovich approach.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous existence proofs for smooth, periodic solutions in PDEs with hexagonal and triangular symmetries, which are important in pattern formation and crystallography.

Method: Enforce space group symmetries in Fourier series on hexagonal lattice; construct approximate solutions using symmetric hexagons (D₆) and triangles (D₃); develop Newton-Kantorovich approach with approximate inverse; compute explicit bounds; use Chebyshev series for branch continuation.

Result: Successfully demonstrated approach on 2D Swift-Hohenberg PDE by proving existence of D₃ and D₆ periodic solutions; developed algorithmic framework for rigorous proofs with code available on Github.

Conclusion: The paper provides a complete framework for rigorous existence proofs of symmetric periodic solutions in PDEs, combining analytical tools with computational verification, applicable to pattern formation problems.

Abstract: In this article, we enforce space group symmetries in Fourier series to rigorously prove the existence of smooth, periodic solutions in partial differential equations (PDEs) with hexagonal and triangular symmetries. In particular, we provide the necessary analytical and numerical tools to construct Fourier series of functions on the hexagonal lattice. This allows one to build approximate solutions that are periodic. Moreover, to generate the periodic tiling, we can use one symmetric hexagon for $D_6$ symmetry and two symmetric triangles for $D_3$ symmetry. We derive a Newton-Kantorovich approach based on the construction of an approximate inverse around an approximate solution, $\overline{u}$. More specifically, we verify a condition based on the computation of explicit bounds. The strategy for constructing $\overline{u}$, the approximate inverse, and the computation of these bounds will be presented. We demonstrate our approach on the 2D Swift-Hohenberg PDE by proving the existence of $D_3$ and $D_6$ periodic solutions. We then perform proofs of branches of solutions by using Chebyshev series. The algorithmic details to perform the proof can be found on Github.

</details>


### [6] [Convergence Analysis of Block Newton Methods for 1D Shallow Neural Network Approximation](https://arxiv.org/abs/2602.12559)
*Zhiqiang Cai,Anastassia Doktorova,Robert D. Falgout,César Herrera*

Main category: math.NA

TL;DR: The paper analyzes local convergence of block Newton methods for shallow neural network approximation, establishing convergence for diffusion-reaction problems and function approximation.


<details>
  <summary>Details</summary>
Motivation: To provide theoretical analysis of block Newton methods for neural network training, particularly for one-dimensional problems, and to understand convergence properties of these optimization approaches.

Method: Block Newton method with 2x2 block decomposition (linear and nonlinear parameters), using nonlinear Gauss-Seidel, linear Gauss-Seidel, or Jacobi for outer iteration and Newton method for inner iteration. Also introduces reduced BN method that reduces parameter count during optimization.

Result: Establishes local convergence of BN methods and reduced BN method for one-dimensional diffusion-reaction problems and least-squares function approximation under reasonable assumptions.

Conclusion: Block Newton methods provide effective optimization for shallow neural networks with theoretical convergence guarantees, and the reduced BN method offers parameter reduction capability unlike common optimization methods.

Abstract: This paper analyzes local convergence of the block Newton (BN) method introduced in [5, 6] for one-dimensional shallow neural network approximation to functions and diffusion-reaction problems. The BN method consists of the 2x2 block nonlinear Gauss-Seidel, linear Gauss-Seidel, or Jacobi method for outer iteration and the Newton method for inner iteration. The blocks are corresponding to the linear and the nonlinear parameters. Under some reasonable assumptions, we establish local convergence of the BN methods as well as the reduced BN (rBN) method for one-dimensional diffusion-reaction problems and least-squares function approximation. Unlike common optimization methods, the rBN allows for the reduction of the number of parameters during the optimization process when some neurons contribute little to the approximation or are at nearly optimal locations.

</details>


### [7] [Adaptive mesh methods for hyperbolic conservation laws with bound-preserving flux limiters](https://arxiv.org/abs/2602.12580)
*Yaguang Gu,Guanghui Hu,Tao Tang*

Main category: math.NA

TL;DR: BP finite-volume schemes for hyperbolic conservation laws on adaptive moving meshes with convex combination approach and inexpensive flux limiter.


<details>
  <summary>Details</summary>
Motivation: To develop robust, high-order bound-preserving schemes for hyperbolic conservation laws on adaptive moving meshes that maintain physical bounds while being computationally efficient.

Method: Rewrite high-order discretization as convex combination of first-order sub-cell schemes, introduce inexpensive BP flux limiter with CFL conditions dependent only on first-order schemes, derive mild CFL restrictions to retain spatial accuracy.

Result: Successfully developed BP schemes for scalar conservation laws and extended to Euler equations and five-equation transport model for two-medium flows, demonstrating high resolution and strong robustness.

Conclusion: The proposed approach provides efficient, high-order BP schemes for hyperbolic conservation laws on adaptive moving meshes with good computational properties and applicability to nonlinear systems.

Abstract: In this paper, we develop bound-preserving (BP) finite-volume schemes for hyperbolic conservation laws on adaptive moving meshes. For scalar conservative laws, we rewrite the conventional high-order discretization as a convex combination of first-order counterparts on each sub-cell, which is mathematically equivalent to introducing a bound-preserving flux limiter. Such a limiter is inexpensive to evaluate, with a feature that the corresponding BP CFL conditions depend solely on the first-order sub-cell schemes. A mild CFL restriction is derived under which high-order spatial accuracy is retained. The proposed BP schemes are extend to two nonlinear systems, namely, the Euler equations and the five-equation transport model of two-medium flows. Numerical results demonstrate that the present schemes possess high resolution and strong robustness properties.

</details>


### [8] [Semi-implicit Structure Preserving Method for The Landau-Lifshitz Equation](https://arxiv.org/abs/2602.12676)
*Changjian Xie*

Main category: math.NA

TL;DR: Proposed semi-implicit scheme for Landau-Lifshitz equation using BDF1 with extrapolation and Crank-Nicolson-type norm-preserving procedure, ensuring structure preservation, stability, and first-order accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the lack of rigorous theoretical justification for stability in projection methods applied to the Landau-Lifshitz equation, particularly the deficiency in theoretical support for the projection step's stability.

Method: Developed a semi-implicit numerical scheme based on first-order Backward Differentiation Formula (BDF1) with one-sided extrapolation and a Crank-Nicolson-type norm-preserving procedure.

Result: The scheme exhibits three key properties: structure preservation, numerical stability, and first-order accuracy in time. It ensures stable computation, maintains norm constraint, and guarantees uniqueness of numerical solution.

Conclusion: The proposed scheme provides substantial facilitation for theoretical analysis of the normalizing step while overcoming limitations of traditional projection methods for the Landau-Lifshitz equation.

Abstract: A critical challenge inherent to the projection method applied to the Landau-Lifshitz equation is the deficiency of rigorous theoretical justifications for the stability of its projection step. To mitigate this limitation, we introduce a semi-implicit numerical scheme, which is formulated on the basis of the first-order Backward Differentiation Formula (BDF) incorporated with one-sided extrapolation and a Crank-Nicolson-type norm-preserving procedure. This proposed scheme exhibits three fundamental characteristics: structure preservation, numerical stability, and first-order accuracy in time. In practical implementations, the scheme not only ensures stable computation and adheres to the norm constraint but also guarantees the uniqueness of the numerical solution, thereby providing substantial facilitation for the theoretical analysis of the normalizing step.

</details>


### [9] [A parallel space-time $p$-adaptive discontinuous Galerkin method for nonlinear acoustics](https://arxiv.org/abs/2602.12788)
*Daniele Corallo,Pascal Lehner,Christian Wieners*

Main category: math.NA

TL;DR: Space-time p-adaptive discontinuous Galerkin method for nonlinear acoustics with well-posedness analysis and numerical validation.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for nonlinear acoustics problems that can handle complex wave phenomena like harmonic generation, with adaptive refinement capabilities to reduce computational cost while maintaining accuracy.

Method: Space-time discontinuous Galerkin discretization combining symmetric Friedrichs systems discretization for hyperbolic terms with interior penalty discretization for damping terms, using Newton's method for solving the nonlinear system, with p-adaptive refinement.

Result: Well-posedness analysis shows stability for linearized system, extended to nonlinear system via fixed point argument with a priori error estimates. Numerical experiments confirm theoretical convergence rates, demonstrate parallel solvability, and show adaptive refinement reduces DOFs for accurate goal functional approximation while reproducing nonlinear acoustic phenomena like harmonic generation.

Conclusion: The proposed space-time p-adaptive discontinuous Galerkin method is effective for nonlinear acoustics, providing theoretical guarantees, computational efficiency through adaptivity, and accurate reproduction of characteristic nonlinear acoustic phenomena.

Abstract: In this paper, we introduce and analyze a space-time $p$-adaptive discontinuous Galerkin method for nonlinear acoustics. We first present the underlying mathematical model, which is based on a recently derived formulation involving, in particular, only first order in time derivatives. We then propose a spacetime discontinuous Galerkin discretization of this model, combining a symmetric Friedrichs systems discretization for symmetric hyperbolic systems with an interior penalty discretization for damping terms. The resulting nonlinear system is solved using Newton's method. Next, we present a well-posedness analysis of the discrete problem. The analysis begins with a linearized system, for which stability is shown. Using a fixed point argument, these results are extended to the fully discrete nonlinear system, yielding a priori error estimates in a natural discontinuous Galerkin norm. Finally, we present numerical experiments demonstrating the parallel solvability of the spacetime formulation and the effectiveness of p-adaptivity. The results confirm the theoretical convergence rates and show that adaptive refinement can reduce the number of degrees of freedom required to accurately approximate selected goal functionals. Moreover, the experiments demonstrate that the model reproduces characteristic phenomena of nonlinear acoustics, such as harmonic generation, thereby validating the proposed model.

</details>


### [10] [A Stabilized Numerical Framework for Necrotic Tumor Growth via Coupled Boundary Integral and Obstacle Solvers](https://arxiv.org/abs/2602.12790)
*Yu Feng,Shuo Ling,Wenjun Ying,Zhennan Zhou*

Main category: math.NA

TL;DR: A computational framework for simulating tumor growth with necrotic cores in Hele-Shaw cells, addressing challenges in bidirectional coupling between nutrient-pressure fields and evolving domain geometry.


<details>
  <summary>Details</summary>
Motivation: Simulating Hele-Shaw tumor growth with necrotic cores is challenging because the outer boundary evolves via advection while the inner necrotic interface lacks explicit advection structure, causing standard numerical schemes to fail.

Method: Introduces a stabilized predictor-corrector strategy that iteratively resolves bidirectional coupling between nutrient-pressure fields and domain geometry, enabling robust time-stepping for both advection-driven outer surface and obstacle-defined necrotic core.

Result: Establishes rigorous convergence theory for single-interface case and demonstrates method's robustness in capturing topological transitions (necrotic core nucleation) and complex geometric evolution.

Conclusion: The proposed computational framework successfully addresses fundamental challenges in simulating Hele-Shaw tumor growth with necrotic cores, providing a robust method for handling bidirectional coupling between evolving interfaces with different mathematical structures.

Abstract: We present a robust computational framework for Hele-Shaw tumor growth with necrotic cores, a problem identified as the incompressible limit of the Porous Media Equation. Simulating this system presents a fundamental challenge: while the outer boundary evolves via advection, the inner necrotic interface is defined by an obstacle problem and lacks an explicit advection structure, causing standard schemes to fail. To address this, we introduce a stabilized predictor-corrector strategy that iteratively resolves the bidirectional coupling between the nutrient-pressure fields and the domain geometry, ensuring robust time-stepping for both the advection-driven outer surface and the obstacle-defined necrotic core. We establish rigorous convergence theory for the single-interface case and demonstrate the method's robustness in capturing the topological transition of necrotic core nucleation and complex geometric evolution.

</details>


### [11] [Finite Difference Method for Stochastic Cahn-Hilliard Equation Driven by A Fractional Brownian Sheet](https://arxiv.org/abs/2602.12816)
*Nan Deng,Wanrong Cao*

Main category: math.NA

TL;DR: The paper analyzes the stochastic Cahn-Hilliard equation with fractional Brownian sheet noise, studying solution regularity and proposing a fully discrete numerical scheme with proven convergence rate.


<details>
  <summary>Details</summary>
Motivation: The stochastic Cahn-Hilliard equation with fractional Brownian sheet provides a more accurate model for correlated space-time random perturbations in phase separation phenomena, requiring rigorous analysis of solution behavior and effective numerical methods.

Method: Two main contributions: 1) Rigorous analysis of mild solution regularity for the stochastic Cahn-Hilliard equation with fractional Brownian sheet; 2) Development of a fully discrete numerical scheme combining finite difference for spatial discretization and tamed exponential Euler method for temporal discretization.

Result: The proposed numerical scheme achieves a strong convergence rate of O(h^{1-ε} + τ^{H₁-1/8-ε/2}), where ε is an arbitrarily small positive constant, providing a solid foundation for numerical treatment of such equations.

Conclusion: The study successfully analyzes solution regularity and provides an effective numerical scheme for the stochastic Cahn-Hilliard equation with fractional Brownian sheet, advancing both theoretical understanding and computational methods for complex stochastic partial differential equations with correlated space-time noise.

Abstract: The stochastic Cahn-Hilliard equation driven by a fractional Brownian sheet provides a more accurate model for correlated space-time random perturbations. This study delves into two key aspects: first, it rigorously examines the regularity of the mild solution to the stochastic Cahn-Hilliard equation, shedding light on the intricate behavior of solutions under such complex perturbations. Second, it introduces a fully discrete numerical scheme designed to solve the equation effectively. This scheme integrates the finite difference method for spatial discretization with the tamed exponential Euler method for temporal discretization. The analysis demonstrates that the proposed scheme achieves a strong convergence rate of $O\big(h^{1-ε}+τ^{H_1-\frac{1}{8}-\fracε{2}}\big)$, where $ε$ is an arbitrarily small positive constant, providing a solid foundation for the numerical treatment of such equations.

</details>


### [12] [Fast convolution solvers using moment-matching](https://arxiv.org/abs/2602.12850)
*Xin Liu,Qinglin Tang,Yong Zhang*

Main category: math.NA

TL;DR: Fast algorithms for computing singular nonlocal potentials using moment-matching with Gaussian auxiliary functions to accelerate convergence.


<details>
  <summary>Details</summary>
Motivation: Existing methods for computing nonlocal potentials with singular kernels have slow convergence rates. The paper aims to develop fast, easy-to-implement algorithms that significantly improve convergence while requiring minimal modifications to standard methods.

Method: Two moment-matching algorithms: 1) For classical kernels, solve differential/pseudo-differential equations with sine pseudospectral method; 2) For general kernels, use trapezoidal/midpoint quadrature with improved regularity. Both methods construct a Gaussian-based auxiliary function ρ₁ that matches moments of the original density up to order m, making residual potential decay faster. Domain expansion and FFT optimization enhance performance.

Result: The algorithms achieve arbitrary high convergence rates for classical kernels and significantly faster convergence for general kernels. Error analysis and extensive numerical tests demonstrate improved accuracy and efficiency across different nonlocal potentials.

Conclusion: The proposed moment-matching approach provides fast, accurate computation of nonlocal potentials with minimal modifications to existing methods, offering substantial improvements in convergence rates and computational efficiency.

Abstract: We propose two easy-to-implement fast algorithms based on moment-matching to compute the nonlocal potential $\varphi(\textbf{x})=(U\ast ρ)(\textbf{x})$ on bounded domain, where the kernel $U$ is singular at the origin and the density $ρ$ is a fast-decaying smooth function. Each method requires merely minor modifications to commonly-used existing methods, i.e., the sine spectral/Fourier quadrature method, and achieves a much better convergence rate. The key lies in the introduction of a smooth auxiliary function $ρ_1$ whose moments match those of the density up to an integer order $m$. Specifically, $ρ_1$ is constructed using Gaussian function in an explicit way and the associated potential can be calculated analytically. The moments of residual density vanish up to order $m$, and the corresponding residual potential $U \ast (ρ-ρ_1)$ decays much faster than the original potential $\varphi$ at the far field. As for the residual potential evaluation, for classical kernels (e.g., the Coulomb kernel), we solve a differential/pseudo-differential equation on a rectangular domain with homogeneous Dirichlet boundary conditions via sine pseudospectral method, and achieve an arbitrary high convergence rate. While, for general kernels, the regularity of Fourier integrand increase by $m$ thanks to the moments-vanishing property, therefore, the standard trapezoidal rule/midpoint quadrature also converges much faster. To gain a better numerical performance, we utilize the domain expansion technique to obtain better accuracy, and improve the efficiency by simplifying the quadrature into one discrete convolution and applying Fast Fourier Transform (FFT) to a double-sized vector. Rigorous error estimates and extensive numerical investigations showcase the accuracy and efficiency for different nonlocal potentials.

</details>


### [13] [Neural Evolutionary Kernel Method: A Knowledge-Guided Framework for Solving Evolutionary PDEs](https://arxiv.org/abs/2602.12872)
*Shuo Ling,Wenjun Ying,Zhen Zhang*

Main category: math.NA

TL;DR: NEKM integrates boundary integral techniques with operator learning to solve time-dependent PDEs using DNN-based kernel representations, achieving high accuracy and efficiency while supporting multi-PDE prediction.


<details>
  <summary>Details</summary>
Motivation: Traditional PDE solvers struggle with complex domains and high-dimensional problems. While DNNs show promise for PDE solving, there's a need to incorporate prior mathematical knowledge into neural architectures to improve efficiency and accuracy for time-dependent PDEs.

Method: Neural Evolutionary Kernel Method (NEKM) combines boundary integral techniques with operator learning, embedding prior mathematical information of time-dependent PDEs into DNN architectures. The method uses DNN-based kernel representations to learn mappings between function spaces.

Result: Numerical experiments on heat, wave, and Schrödinger equations demonstrate high accuracy and favorable computational efficiency. The framework also supports simultaneous prediction of solutions to multiple PDEs with different coefficients, enabling random PDE solving.

Conclusion: NEKM successfully integrates mathematical PDE knowledge with deep learning, creating an efficient and accurate framework for time-dependent PDE solving that can handle multiple PDE variations simultaneously.

Abstract: Numerical solution of partial differential equations (PDEs) plays a vital role in various fields of science and engineering. In recent years, deep neural networks (DNNs) have emerged as a powerful tool for solving PDEs, leveraging their approximation capabilities to handle complex domains and high-dimensional problems. Among these, operator learning has gained increasing attention by learning mappings between function spaces using DNNs. This paper proposes a novel approach, termed the Neural Evolutionary Kernel Method (NEKM), for solving a class of time-dependent partial differential equations (PDEs) via deep neural network (DNN)-based kernel representations. By integrating boundary integral techniques with operator learning, prior mathematical information of time-dependent partial differential equations (PDEs) is embedded into the design of neural network architectures for predicting their solutions, enhancing both computational efficiency and solution accuracy. Numerical experiments on the heat, wave, and Schrödinger equations demonstrate that the Neural Evolutionary Kernel Method (NEKM) achieves high accuracy and favorable computational efficiency. Furthermore, the operator learning framework inherently supports the simultaneous prediction of solutions to multiple PDEs with different coefficients, rendering its capability for solving random PDEs.

</details>


### [14] [Bifurcation curve detection with deflation for multiparametric PDEs](https://arxiv.org/abs/2602.12940)
*Nitin Kumar,Federico Pichi,Gianluigi Rozza*

Main category: math.NA

TL;DR: A framework combining arclength continuation and deflation techniques to capture bifurcations and track bifurcation curves in multiparametric PDEs, with adaptive parameter selection and zigzag path-following strategies.


<details>
  <summary>Details</summary>
Motivation: Traditional continuation methods for one-dimensional parameterizations are inefficient for multiparametric PDEs - small step sizes increase computational cost while larger steps risk jumping to different solution branches or missing bifurcation points entirely.

Method: Combines arclength continuation (adaptively selecting parameter values in higher dimensions) with deflation technique (discovering multiple branches). Introduces zigzag path-following strategy to robustly track bifurcation curves/surfaces in 2D/3D parameter spaces.

Result: Demonstrated performance on benchmark problems: the Bratu equation and the Allen-Cahn equation, showing effective capture of bifurcating phenomena and detection of bifurcation curves.

Conclusion: The proposed framework successfully addresses limitations of traditional continuation methods for multiparametric PDEs, enabling comprehensive bifurcation analysis through adaptive parameter selection and robust branch tracking.

Abstract: This work presents a comprehensive framework for capturing bifurcating phenomena and detecting bifurcation curves in nonlinear multiparametric partial differential equations, where the system exhibits multiple coexisting solutions for given values of the parameters. Traditional continuation methods for one-dimensional parameterizations employ the previously computed solution as the initial guess for the next parameter value. These are usually very inefficient, since small step sizes increase computational cost, while larger steps could jeopardize the method convergence jumping to a different solution branch or missing the bifurcation point. To address these challenges, we propose a novel framework that combines: (i) arclength continuation, adaptively selecting new parameter values in higher dimension, and (ii) the deflation technique, discovering multiple branches to construct complete bifurcation diagrams. In particular, the arclength continuation method is designed to handle multiparametric scenarios, where the parameter vector $λ\in \mathbb{R}^p$ traces a curve $g(λ)$ within a $p$-dimensional parameter space. In addition, we introduce a zigzag path-following strategy to robustly track the bifurcation curves and surfaces, respectively, for two- and three-dimensional parametric spaces. Finally, we demonstrate its performance on two benchmark problems: the Bratu equation and the Allen-Cahn equation.

</details>


### [15] [Data-Driven Filter Design for Flexible and Noise-Robust Tomographic Imaging](https://arxiv.org/abs/2602.13048)
*Hamid Fathi,Alexander Skorikov,Tristan van Leeuwen*

Main category: math.NA

TL;DR: A data-driven approach learns FBP filters and projection weights from training data to improve robustness of tomographic reconstruction while maintaining computational efficiency of classical back-projection.


<details>
  <summary>Details</summary>
Motivation: Filtered back projection (FBP) degrades with noise, incomplete sampling, or non-standard geometries. Need to improve robustness without sacrificing the speed and simplicity of classical FBP.

Method: Data-driven learning of FBP filters and projection weights directly from training data. Formulated as regularized optimization for linear inverse operator. Learned filters act as data-adaptive gain functions balancing noise amplification and bias.

Result: Consistent improvements over conventional FBP and FDK in 2D and 3D case studies. Filters trained on synthetic laminography data generalize well to real-world measurements, achieving image quality comparable to advanced iterative methods without high computational cost.

Conclusion: The proposed method provides adaptive, robust tomographic reconstruction that retains FBP's computational efficiency while achieving quality comparable to iterative methods, with theoretical guarantees of existence, uniqueness, and stability.

Abstract: While filtered back projection (FBP) is still the method of choice for fast tomographic reconstruction, its performance degrades noticeably in the presence of noise, incomplete sampling, or non-standard scan geometries. We propose a data-driven approach for learning FBP filters and projection weights directly from training data, with the goal of improving robustness without sacrificing computational efficiency. The resulting reconstructions adapt naturally to the noise level and acquisition geometry, while retaining the speed and simplicity of classical back-projection. The proposed method can be formulated as a regularized optimization problem for a linear inverse operator, which allows us to establish existence, uniqueness, and stability of the learned solution. From a spectral viewpoint, the learned filters act as data-adaptive gain functions that explicitly balance noise amplification and bias, in close analogy to a regularized pseudo-inverse. Experiments in both 2D and 3D show consistent improvements over conventional FBP and FDK in different case studies. Finally, we show that filters trained on synthetic laminography data generalize well to real-world measurements, delivering image quality comparable to advanced iterative methods without the high computational cost.

</details>


### [16] [Multi-physics Preconditioning for Thermally Activated Batteries](https://arxiv.org/abs/2602.13079)
*Malachi Phillips*

Main category: math.NA

TL;DR: Advancements in scalable preconditioning strategies for thermal battery simulations using hierarchical block Gauss-Seidel preconditioner to efficiently solve coupled electrochemical systems.


<details>
  <summary>Details</summary>
Motivation: Thermal battery simulations require multiphysics modeling to evaluate performance, but computational costs are dominated by solving tightly coupled electrochemical systems. Current methods need improved efficiency for higher-resolution models and transition from 2D to 3D simulations.

Method: Proposed hierarchical block Gauss-Seidel preconditioner implemented through Teko package in Trilinos, using scalable subblock solvers including smoothed aggregation algebraic multigrid (SA-AMG) methods and domain-decomposition techniques to address coupled physics of charge transport, porous flow, and species diffusion.

Result: Solver handles problem sizes up to 51.3 million degrees of freedom on 2048 processors with near sub-second setup and solve times for end-to-end electrochemical solve. Strong and weak scaling studies demonstrate robust convergence and parallel scalability.

Conclusion: Advancements significantly improve computational efficiency and turnaround time of thermal battery simulations, enabling higher-resolution models and transition from 2D axisymmetric to full 3D simulations.

Abstract: Thermal batteries, also known as molten-salt batteries, are single-use reserve power systems activated by pyrotechnic heat generation, which transitions the solid electrolyte into a molten state. The simulation of these batteries relies on multiphysics modeling to evaluate performance and behavior under various conditions. This paper presents advancements in scalable preconditioning strategies for the Thermally Activated Battery Simulator (TABS) tool, enabling efficient solutions to the coupled electrochemical systems that dominate computational costs in thermal battery simulations. We propose a hierarchical block Gauss-Seidel preconditioner implemented through the Teko package in Trilinos, which effectively addresses the challenges posed by tightly coupled physics, including charge transport, porous flow, and species diffusion. The preconditioner leverages scalable subblock solvers, including smoothed aggregation algebraic multigrid (SA-AMG) methods and domain-decomposition techniques, to achieve robust convergence and parallel scalability. Strong and weak scaling studies demonstrate the solver's ability to handle problem sizes up to 51.3 million degrees of freedom on 2048 processors, achieving near sub-second setup and solve times for the end-to-end electrochemical solve. These advancements significantly improve the computational efficiency and turnaround time of thermal battery simulations, paving the way for higher-resolution models and enabling the transition from 2D axisymmetric to full 3D simulations.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [17] [Global renormalized solutions for hard potential non-cutoff Boltzmann equation without defect measure](https://arxiv.org/abs/2602.12511)
*Yi-Long Luo,Jing-Xin Nie*

Main category: math.AP

TL;DR: The paper proves that for hard potentials (0 ≤ γ ≤ 1), the defect measure in Alexandre-Villani's renormalized solutions to the non-cutoff Boltzmann equation actually vanishes, establishing global existence without defect measure. A counterexample shows this approach fails for soft potentials (-3 < γ < 0).


<details>
  <summary>Details</summary>
Motivation: Alexandre and Villani (2002) established global renormalized solutions for non-cutoff Boltzmann equations using a definition involving a non-negative defect measure. The paper aims to address whether this defect measure is actually necessary or if it can be eliminated, particularly for hard potentials.

Method: The authors exploit the stronger coercivity estimates provided by hard potentials (0 ≤ γ ≤ 1) to prove that the defect measure vanishes. They use the inverse power law model and analyze the properties of solutions to show the defect measure disappears. For soft potentials (-3 < γ < 0), they construct a counterexample demonstrating that their approach fails.

Result: For hard potentials (0 ≤ γ ≤ 1), the defect measure in renormalized solutions to the non-cutoff Boltzmann equation vanishes, establishing global existence of solutions without any defect measure. For soft potentials (-3 < γ < 0), the approach fails as shown by a constructed counterexample.

Conclusion: The paper establishes that for hard potentials, global renormalized solutions to the non-cutoff Boltzmann equation exist in the standard sense without defect measures, resolving an issue from the Alexandre-Villani theory. However, this result does not extend to soft potentials, where the defect measure remains necessary.

Abstract: The existence of global renormalized solutions to the Boltzmann equation with long-range interactions without angular cutoff was first established by Alexandre and Villani [Comm. Pure Appl. Math., 55(1), 30-70, 2002]. Their result relies on a definition of renormalized solutions involving a non-negative defect measure. In this paper, we address this issue for the inverse power law model in the case of hard potentials ($0 \leq γ\leq 1$). By exploiting the stronger coercivity estimates provided by hard potentials, we prove that the defect measure actually vanishes. Consequently, we establish the global existence of renormalized solutions for the non-cutoff Boltzmann equation with hard potentials in the standard sense, without any defect measure. Finally, we construct a counterexample showing that the approach developed for the hard potential case fails for soft potential model ($-3 < γ< 0$).

</details>


### [18] [Asymptotically self-similar graph-like solutions to a multi-dimensional surface diffusion flow equation under contact angle and no-flux boundary conditions](https://arxiv.org/abs/2602.12619)
*Yoshikazu Giga,Sho Katayama*

Main category: math.AP

TL;DR: Global existence and asymptotic behavior of solutions to Mullins' thermal grooving model with contact angle boundary conditions in multi-dimensional half space.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical results for Mullins' thermal grooving model, which describes surface diffusion with contact angle and no-flux boundary conditions, particularly regarding global existence and long-time behavior of solutions.

Method: Mathematical analysis of the surface diffusion flow equation in multi-dimensional half space with contact angle and no-flux boundary conditions. Proving existence of global solutions when initial slope is close to contact angle, and establishing existence of self-similar solutions.

Result: Proved existence of unique global-in-time solutions when initial slope is close to contact angle. Showed existence of self-similar solutions for given behavior at space infinity. Demonstrated convergence of global solutions to self-similar solutions as time tends to infinity for asymptotically homogeneous initial data.

Conclusion: The paper provides complete mathematical analysis of Mullins' thermal grooving model, establishing global existence, self-similar solutions, and asymptotic convergence without restrictions on contact angle size.

Abstract: This paper studies Mullins' model of thermal grooving which consists of a surface diffusion flow equation with contact angle and no-flux boundary conditions. We consider this problem in a multi-dimensional half space and prove that if the slope of the initial data is close to that consistent with the contact angle, then there exists a unique global-in-time solution. In particular, we show the existence of a self-similar solution for a given behavior at the space infinity. We also show that our global solution converges to a self-similar solution as the time tends to infinity if the initial data is asymptotically homogeneous at the space infinity. No assumption on the size of the contact angle is imposed.

</details>


### [19] [Quantitative stability for quasilinear parabolic equations](https://arxiv.org/abs/2602.12657)
*Tapio Kurkinen,Qing Liu*

Main category: math.AP

TL;DR: Analysis of stability and convergence rates for quasilinear parabolic PDEs under perturbations, with applications to p-parabolic equations.


<details>
  <summary>Details</summary>
Motivation: To understand how viscosity solutions behave when perturbation parameters vanish, especially for quasilinear parabolic PDEs that can be singular or degenerate, and to provide quantitative convergence estimates.

Method: Adapt standard comparison arguments to establish explicit convergence rates for viscosity solutions as perturbation parameters approach zero.

Result: Established explicit convergence rates for perturbations vanishing in quasilinear parabolic PDEs, covering both normalized and variational p-parabolic equations, with quantitative estimates for perturbations of exponent p and regularized approximations.

Conclusion: The framework successfully provides quantitative stability results for a broad class of quasilinear parabolic equations, including singular/degenerate cases, with applications to p-parabolic equations and their perturbations.

Abstract: We examine the stability of a class of quasilinear parabolic partial differential equations under perturbations. We are interested in the behavior of viscosity solutions as the perturbation parameter vanishes and establish explicit convergence rates by adapting standard comparison arguments. Despite the possible singular or degenerate nature of the parabolic operator, our framework covers, in particular, both the normalized and the variational $p$-parabolic equations, providing quantitative estimates for perturbations of the exponent $p$ and limits arising from regularized approximations.

</details>


### [20] [Non-uniqueness of smooth solutions of the Navier-Stokes equations from almost the same initial conditions](https://arxiv.org/abs/2602.12666)
*Shijun Liao,Shijie Qin*

Main category: math.AP

TL;DR: Numerical evidence shows Navier-Stokes equations can have distinct global solutions from nearly identical initial conditions with differences as small as 10^-40, challenging uniqueness assumptions.


<details>
  <summary>Details</summary>
Motivation: To investigate the uniqueness and existence properties of Navier-Stokes equations, which are central to one of the Clay Institute's Millennium Prize Problems, using high-precision numerical methods.

Method: Uses Clean Numerical Simulation (CNS) to obtain highly accurate spatiotemporal trajectories of Navier-Stokes turbulence over finite but long time intervals, examining solutions from extremely similar initial conditions.

Result: Found numerical evidence that Navier-Stokes equations can produce distinct global solutions even when starting from initial conditions that differ by as little as 10^-40 magnitude.

Conclusion: These findings provide insights into the uniqueness problem of Navier-Stokes equations and may contribute to understanding the Millennium Prize Problem regarding existence and smoothness of solutions.

Abstract: Using clean numerical simulation (CNS) which can give very accurate spatiotemporal trajectory of Navier-Stokes turbulence in a finite but long enough interval of time, we give some numerical evidences that the Navier-Stokes equations admit distinct global solutions from almost the same initial conditions whose difference is very small, i.e. even at the order $10^{-40}$ of magnitude. Hopefully these examples could provide some enlightenments for the uniqueness and existence of Navier-Stokes equations, which are related to one Millennium Prize Problem of Clay Institute.

</details>


### [21] [New solutions to Schrödinger-Poisson-Slater equations in Coulomb-Sobolev spaces](https://arxiv.org/abs/2602.12784)
*Artur Jorge Marinho,Carlo Mercuri,Kanishka Perera*

Main category: math.AP

TL;DR: The paper proves existence and multiplicity results for a nonlinear nonlocal PDE with Riesz potential and local nonlinearity, using a new scaling-based approach in critical point theory and cohomological index methods.


<details>
  <summary>Details</summary>
Motivation: To study zero-mass problems for nonlinear nonlocal PDEs with Riesz potentials, addressing the challenge of finding solutions when the nonlinearity behaves differently at zero and infinity, and developing methods to handle Sobolev-subcritical, critical, or supercritical growth.

Method: A new scaling-based approach in critical point theory that identifies a scaling invariant PDE interpreted as a nonlinear eigenvalue problem. Uses the ℤ₂-cohomological index of Fadell and Rabinowitz to define eigenvalues and employs critical group estimates with scaling-based linking sets. Works in Coulomb-Sobolev spaces and establishes compactness results for associated action functionals.

Result: Proves existence and multiplicity of solutions sensitive to the "resonance" of the nonlinearity with eigenvalues of the scaling invariant problem. Solutions are found as critical points, with the number of solutions depending on parameter ranges and nonlinearity behavior. The approach works for broad parameter ranges (N, α, p) and various growth conditions on f.

Conclusion: The paper develops a novel scaling-based critical point theory framework for nonlocal PDEs with Riesz potentials, providing a systematic way to handle different behaviors of nonlinearities at zero and infinity. The method extends classical Fredholm alternative concepts to nonlinear settings and works for subcritical, critical, and supercritical growth conditions.

Abstract: We prove existence and multiplicity results for the nonlinear and nonlocal PDE $$ - Δu + (I_α\star |u|^p)\, |u|^{p-2}\, u = f(|x|,u) \quad \textrm{in} \,\,\mathbb {R}^N, $$ where $N \geq 2$, $I_α: \mathbb{R}^N \setminus \{0\} \rightarrow \mathbb{R}$ is the Riesz potential of order $α\in (1,N),$ $p>1,$ and the local nonlinearity $f: [0,\infty) \times \mathbb{R} \rightarrow \mathbb R$ is subject to a new class of assumptions. We find solutions to this zero-mass problem in a Coulomb-Sobolev space using a new scaling based approach in critical point theory, by which we classify the possibly different behaviour of the nonlinearity $f$ at zero and at infinity in terms of the scaling properties of the left hand side of the equation. This is accomplished identifying a scaling invariant PDE which can be interpreted as a nonlinear eigenvalue problem, for which a sequence of eigenvalues $\{λ_k\}$ is conveniently defined via the ${\mathbb{Z}}_2$-cohomological index of Fadell and Rabinowitz. This index allows us to use new critical group estimates (and scaling-based linking sets) which might not be possible via the classical genus. Within a fairly broad set of parameters $N,α, p$ and class of assumptions on the local nonlinearity $f,$ we establish compactness results for an associated action functional and find multiple solutions as critical points, whose existence and number is sensitive to the ''resonance'' of $f$ with the sequence of eigenvalues for the scaling invariant problem, a construction which is at places reminiscent, in the present nonlinear setting, of the classical Fredholm alternative. As a byproduct of our analysis, letting $p\neq 2$ allows us to capture general nonlinearities $f$ of Sobolev-subcritical, critical, or supercritical growth.

</details>


### [22] [Nonlinear Diffusion, and Geometric and Functional Inequalities on Smooth Metric Measure spaces](https://arxiv.org/abs/2602.13025)
*Ali Taheri*

Main category: math.AP

TL;DR: Overview of nonlinear diffusion equations (slow/fast types) and their connections to geometric/functional inequalities in smooth metric measure spaces, presented as summer school lecture notes with new results.


<details>
  <summary>Details</summary>
Motivation: To explore the connections between nonlinear diffusion equations and geometric/functional inequalities in the context of smooth metric measure spaces, providing both educational introduction and new research contributions.

Method: Summer school style introduction combined with mathematical analysis of nonlinear diffusion equations (slow and fast types) within the framework of smooth metric measure spaces, linking them to geometric and functional inequalities.

Result: Presents new results connecting nonlinear diffusion equations with geometric/functional inequalities in smooth metric measure spaces, though specific results are not detailed in the abstract.

Conclusion: The paper establishes important links between nonlinear diffusion phenomena and geometric/functional inequalities in metric measure spaces, serving both educational and research purposes.

Abstract: This extended abstract is based on a talk given at the workshop and summer school ``Direct and Inverse Problems with Applications" in Ghent Analysis and PDE Centre in August 2024. It focuses on nonlinear diffusion equations of slow and fast types and their links with some geometric and functional inequalities in the framework of smooth metric measure spaces. The article presents some introduction in a summer school style as well as several new results.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [23] [A new model for two-layer liquid-gas stratified flows in pipes with general cross sections](https://arxiv.org/abs/2602.12290)
*Sarswati Shah,Gerardo Hernández-Dueñas*

Main category: physics.comp-ph

TL;DR: New hyperbolic model for immiscible two-layer gas-liquid stratified pipe flows with general cross sections, featuring compressible gas layer and incompressible liquid layer coupled through non-conservative terms.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive mathematical model for stratified two-phase flows in pipes that properly accounts for compressibility effects in the gas phase, inter-layer interactions, and works for general pipe cross sections.

Method: Derived coupled PDE system: incompressible liquid layer with hydrostatic pressure (shallow water approximation) + compressible gas layer (ideal gas law) with conservation of mass, momentum, energy. Analyzed hyperbolic properties, entropy inequalities, eigenvalue approximations. Implemented numerical tests for validation.

Result: Successfully developed hyperbolic model with analyzed mathematical properties. Numerical tests demonstrate well-balancedness, accurate Riemann problem solutions, perturbation handling, and convergence to steady states. Model works for both large density differences (water-air) and small differences (gas-liquid hydrogen).

Conclusion: The proposed model provides a robust framework for simulating immiscible two-layer stratified flows in pipes, with proper treatment of compressibility and inter-layer coupling, validated through comprehensive mathematical analysis and numerical testing.

Abstract: In this work, we derive a new model for immiscible two-layer gas-liquid stratified flows in pipes with general cross sections. The bottom layer is occupied by an incompressible fluid in liquid phase with hydrodynamics based on a hydrostatic pressure, following a shallow water approximation. The top layer is occupied by a compressible gas, following an ideal gas law leading to conservation of mass, momentum and energy. The two subsystems are linked through non-conservative products, representing momentum and energy exchanges between layers. The hyperbolic properties of the resulting model are analyzed, including the derivation of entropy inequalities, and the approximations of eigenvalues of the corresponding coefficient matrix. Numerical tests are included to demonstrate the merits of the model and the numerical approximations, including well-balancedness, Riemann problems, and perturbations and convergence toward steady states at rest. Besides simulations of water and air where the density difference between layers is significant, a case where such difference is not so pronounced (like gas and liquid hydrogen) is also shown.

</details>


### [24] [Accelerated Markov Chain Monte Carlo Simulation via Neural Network-Driven Importance Sampling](https://arxiv.org/abs/2602.12294)
*Michael Kim,Wei Cai*

Main category: physics.comp-ph

TL;DR: A neural network-based importance sampling method accelerates rare event sampling in MCMC simulations by using bias potentials, with BRW for efficiency and rigorous rate recovery.


<details>
  <summary>Details</summary>
Motivation: Atomistic simulations are limited by time scale constraints due to systems getting trapped in metastable states, making rare transition events difficult to observe despite their importance for understanding long-term material evolution.

Method: Importance sampling method using neural network bias potentials to enhance rare transition sampling in MCMC while preserving pathway probabilities, with branching random walk for efficiency and rigorous formulation to recover original transition rates.

Result: Method validated on 2D and 14D systems, demonstrating accuracy and scalability in accelerating time scales while maintaining correct transition probabilities.

Conclusion: The proposed approach successfully overcomes time scale limitations in atomistic simulations by efficiently sampling rare events while preserving transition pathway probabilities, enabling study of long-term material evolution.

Abstract: Atomistic simulations provide valuable insights into the physical processes governing material behavior. However, their applicability is fundamentally constrained by the limited time scales accessible to brute-force simulations. This bottleneck often stems from complex energy landscapes where the systems stay trapped in metastable states for long periods of time. Yet, the long-term evolution is controlled by the transitions between the metastable states, which are rare events and difficult to observe. We present an importance sampling method designed to accelerate the time scale of Markov chain Monte Carlo (MCMC) simulations. By employing a bias potential, our approach enhances the sampling of rare transition events while preserving the relative probabilities of distinct transition pathways. The bias potential is represented by a neural network which enables the flexibility needed for high-dimensional systems. We propose a rigorous formulation to obtain the original transition rates between metastable states using transition paths obtained from the biased simulation. We further use a branching random walk (BRW) technique to enhance efficiency and to reduce variance. The proposed methodology is validated on 2-dimensional and 14-dimensional systems, demonstrating its accuracy and scalability.

</details>


### [25] [Electrohydrodynamic instability of Cu, W and Ti metal nanomelts under radiofrequency E-fields from multiphysics molecular dynamics simulations with coarse-grained density field analysis](https://arxiv.org/abs/2602.12558)
*Shangyong Wua,Rui Chua,Wenqian Konga,Hongyu Zhanga,Le Shia,Kai Wua,Yonghong Chenga,Guodong Menga,Bing Xiaoa*

Main category: physics.comp-ph

TL;DR: Study investigates electrohydrodynamic instability and thermal runaway in metal nanotips under RF electric fields using ED-MD simulations and electrocapillary wave theory, revealing non-monotonic frequency dependence and critical field amplitudes.


<details>
  <summary>Details</summary>
Motivation: To understand the structural evolution and thermal runaway processes in metal nanotips under radiofrequency electric fields, which is crucial for applications like field emission devices and nanoscale electrohydrodynamic systems.

Method: Combined electrodynamics coupled with molecular dynamics (ED-MD) simulations for atomistic models with dynamic instability theory of electrocapillary wave. Developed workflows to calculate kinematic viscosity tensor components and mass density spatial distributions for nanomelts under electric fields.

Result: Found non-monotonic variation of time delay versus electric field frequency, critical RF electric field amplitude triggering thermal runaway regardless of frequency, and drastically different mass densities and kinematic viscosities of nanomelts compared to bulk liquid metals. Nanomelt viscosity under RF field is several orders of magnitude higher than bulk liquid metal.

Conclusion: The study reveals significant differences between nanomelt and bulk liquid metal properties under RF fields, with viscosity-dominated regime affecting instability scales. Good agreement between ED-MD simulations and theory for W nanotips, while discrepancies observed for Cu and Ti metals.

Abstract: Employing both electrodynamics coupled with molecular dynamics (ED-MD) simulations for atomistic models and the dynamic instability theory of electrocapillary wave, we investigate the structure evolutions and thermal runaway process of Cu, Ti and W nanotips with radii of curvature of 1 nm and 5 nm under various radiofrequency electric field conditions. The associated critical parameters including the critical electric field, spatial and temporal scales of the electrohydrodynamic instability of molten apexes are obtained by proposing the workflows that utilize the atomistic models in ED-MD simulations to calculate kinematic viscosity tensor components and mass density spatial distributions for the nanomelts with electric fields. Our current ED-MD simulations for nanotips show a non-monotonical variation of the time delay versus the electric field frequency for metal nanotips, and the presence of a critical rf electric field amplitude triggering the thermal runaway regardless of the field frequency. The calculated mass densities and kinematic viscosities of nanomelts for metal nanotips are found to be drastically different to those of bulk liquid metals at the melting point. Specifically, the viscosity of nanomelt under the rf electric field is revealed to be several orders of magnitude higher than the bulk liquid metal, resulting in substantial increase of spatial and temporal scales in the instability theory of electrocapillary wave within the viscosity-dominated regime, compared to the results of ED-MD simulations for Cu and Ti metals, while good agreement between the two methods on the critical wavelength and time delay of thermal runway is found for W nanotips.

</details>


### [26] [A T-matrix scattering formalism for electron-beam spectroscopy](https://arxiv.org/abs/2602.12743)
*P. Elli Stamatopoulou,Carsten Rockstuhl*

Main category: physics.comp-ph

TL;DR: Implementation of electron-beam spectroscopy in a T-matrix scattering framework for simulating cathodoluminescence and electron energy-loss spectroscopy in complex nanophotonic materials.


<details>
  <summary>Details</summary>
Motivation: Advanced computational tools are needed to describe electron interactions with structured nanophotonic materials for theoretical predictions, design tasks, and experimental interpretation, particularly for free-electron-driven nanophotonic light sources.

Method: Extended T-matrix-based scattering formalism to describe interactions with fast electrons, implemented into existing software suite treams as treams_ebeam, enabling simulation of CL and EELS measurements.

Result: Developed fast and accurate numerical tool for simulating electron-beam spectroscopy, demonstrated on various problems including single scatterers, periodic chains of elliptical nanodisks, and finite clusters of nanospheres in 2D lattices.

Conclusion: The framework unites fast-electron physics with advanced scattering theory, unlocking new possibilities for designing, understanding, and engineering next-generation nanoscale light-matter interactions.

Abstract: Advanced computational tools that describe the interaction of electrons with structured nanophotonic materials are crucial for theoretical predictions, specific design tasks, and the interpretation of experimental results. These tools open the door to systematic exploration of free-electron-driven nanophotonic light sources, among others. Here, we report on the implementation of electron-beam spectroscopy in a T-matrix-based scattering formulation. Such a framework is quite versatile in predicting the electromagnetic response of complex photonic materials composed of periodically or aperiodically arranged individual scatterers. By extending this formalism to describe interactions with fast electrons, we provide a fast and accurate numerical tool for simulating cathodoluminescence (CL) and electron energy-loss spectroscopy (EELS) measurements. The desired functionalities are implemented into the existing software suite treams for electromagnetic scattering computations, and the extended code treams_ebeam is available online at https://github.com/tfp-photonics/treams_ebeam. We demonstrate the implementation details on a carefully selected set of problems, including single scatterers of various shapes and materials, a periodic chain of elliptical nanodisks, and a finite cluster of nanospheres arranged in a two-dimensional (2D) lattice. By uniting fast-electron physics with advanced scattering theory, our framework unlocks new possibilities for designing, understanding, and engineering next-generation nanoscale light-matter interactions.

</details>


### [27] [Estimating Full Path Lengths and Kinetics from Partial Path Transition Interface Sampling Simulations](https://arxiv.org/abs/2602.12835)
*Wouter Vervust,Elias Wils,Sina Safaei,Daniel T. Zhang,An Ghysels*

Main category: physics.comp-ph

TL;DR: The paper introduces a Markov state model framework to extract kinetic properties from partial paths generated by REPPTIS, enabling calculation of mean first passage times, fluxes, and rate constants from computationally efficient short path simulations.


<details>
  <summary>Details</summary>
Motivation: REPPTIS was developed to study rare/slow biological processes using partial paths to reduce computational cost, but lacked a formalism to extract time-dependent kinetic properties like mean first passage times, fluxes, and rates from these short paths.

Method: Developed a Markov state model framework to estimate full path lengths and kinetic properties from overlapping partial paths generated by REPPTIS, deriving closed formulas for crossing probability, MFPTs, flux, and rate constants.

Result: Validated on Brownian and Langevin particles on 1D potentials and KCl dissociation, showing REPPTIS accurately reproduces exact kinetics benchmarks. Applied to trypsin-benzamidine complex dissociation rate, though computed rate underestimated experimental value.

Conclusion: The MSM framework provides REPPTIS with a robust theoretical and practical foundation for extracting kinetic information from computationally efficient partial paths, enabling study of rare biological processes beyond accessible MD timescales.

Abstract: Assessing the time scale of biological processes using molecular dynamics (MD) simulations with sufficient statistical accuracy is a challenging task, as processes are often rare and/or slow events, which may extend largely beyond the time scale of what is accessible with modern day high performance computational infrastructure. Recently, the replica exchange partial path transition interface sampling (REPPTIS) algorithm was developed to study rare and slow events involving metastable states along their reactive pathways. REPPTIS is a path sampling method where paths are cut short to reduce the computational cost, while combining this with the efficiency offered by replica exchange between the partial path ensembles. However, REPPTIS still lacks a formalism to extract time-dependent properties, such as mean first passage times, fluxes, and rates, from the short partial paths. In this work, we introduce a Markov state model (MSM) framework to estimate full path lengths and kinetic properties from the overlapping partial paths generated by REPPTIS. The framework results in newly derived closed formulas for the REPPTIS crossing probability, mean first passage times (MFPTs), flux, and rate constant. Our approach is then validated using simulations of Brownian and Langevin particles on a series of one-dimensional potential energy profiles as well as the dissociation of KCl in solution, demonstrating that REPPTIS accurately reproduces the exact kinetics benchmark. The MSM framework is further applied to the trypsin-benzamidine complex to compute the dissociation rate as a test case of a biological system, albeit the computed rate underestimates the experimental value. In conclusion, our MSM framework equips REPPTIS simulations with a robust theoretical and practical foundation for extracting kinetic information from computationally efficient partial paths.

</details>


### [28] [Tensor Network Compression for Fully Spectral Vlasov-Poisson Simulation](https://arxiv.org/abs/2602.13092)
*Erik M. Åsgrim,Luca Pennati,Marco Pasquale,Stefano Markidis*

Main category: physics.comp-ph

TL;DR: A tensor network method for kinetic plasma simulation that represents phase-space distribution functions in compressed form, enabling efficient spectral transforms and time stepping without full grid reconstruction.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for kinetic plasma simulations that can handle the high-dimensional phase-space while maintaining computational efficiency through adaptive compression, avoiding the curse of dimensionality.

Method: Uses low-rank tensor networks to represent phase-space distribution functions, applies Strang splitting with spectral treatment of each substep, performs spectral transforms directly in compressed form, and computes self-consistent electric fields within the tensor formalism.

Result: Validated on standard benchmarks including Landau damping and two-stream instability, and systematically studied compression parameter effects on conservation properties, positivity, filamentation robustness, and computational cost.

Conclusion: The tensor network approach provides an efficient framework for kinetic plasma simulations with adaptive compression, enabling accurate solutions while controlling computational cost through compression parameters.

Abstract: We propose a numerical method for kinetic plasma simulation in which the phase-space distribution function is represented by a low-rank tensor network with an adaptive level of compression. The Vlasov-Poisson system is advanced using Strang splitting, and each substep is treated spectrally in the corresponding variable. By expressing both the distribution function and the Fourier transform as tensor network objects (state and operator representations), spectral transforms are applied directly in compressed form, enabling time stepping without reconstructing the full phase-space grid. The self-consistent electric field is also computed within the tensor formalism. The charge density is obtained by contracting over velocity degrees of freedom and extracting the zero Fourier mode, which provides the source term for a spectral Poisson solver. We validate the approach on standard benchmarks, including Landau damping and the two-stream instability. Finally, we systematically study how compression parameters, including truncation tolerances and internal ranks (bond dimensions), affect momentum and energy conservation, positivity behavior, robustness to filamentation, and computational cost.

</details>


### [29] [An Always-Accepting Algorithm for Transition Path Sampling](https://arxiv.org/abs/2602.13130)
*Magdalena Häupl,Sebastian Falkner,Peter G. Bolhuis,Christoph Dellago,Alessandro Coretti*

Main category: physics.comp-ph

TL;DR: A one-way shooting algorithm for transition path sampling that accepts all proposed trajectories while correctly sampling transition paths for overdamped stochastic systems, eliminating rejection costs.


<details>
  <summary>Details</summary>
Motivation: Conventional transition path sampling algorithms suffer from inefficiency due to high rejection rates when generating trajectories, which is computationally expensive and limits sampling of rare events like crystal formation at challenging conditions.

Method: Two key elements: 1) A procedure to propose trajectories that are always reactive (always connect initial and final states), and 2) A reweighting scheme that corrects for the bias introduced by always accepting proposed paths. This creates a one-way shooting algorithm specifically for systems with overdamped stochastic dynamics.

Result: The algorithm significantly improves efficiency by eliminating rejection costs. Demonstrated by investigating CO₂ clathrate hydrate formation along different reaction mechanisms, showing that the increased efficiency enables proper sampling of crystalline hydrate formation at temperatures and pressures difficult to access with conventional algorithms.

Conclusion: The proposed one-way shooting algorithm provides an efficient alternative to conventional transition path sampling by accepting all proposed trajectories while maintaining correct sampling through reweighting, enabling study of rare events at challenging conditions that were previously inaccessible.

Abstract: We present a one-way shooting algorithm for transition path sampling that accepts every proposed trajectory yet samples the correct transition path ensemble for systems with overdamped stochastic dynamics. The method is based on two key elements: a procedure to propose trajectories that are always reactive, and a reweighting scheme that corrects for the bias introduced by always accepting the proposed paths. This approach significantly improves the efficiency of transition path sampling by eliminating the cost associated with generating trajectories that are then rejected. We demonstrate the algorithm by investigating the formation of CO$_2$ clathrate hydrates along different reaction mechanisms, showing that the increased efficiency allows proper sampling of the formation of crystalline hydrates at temperatures and pressures that are difficult to access with conventional algorithms.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [30] [Influence of finite temperature degeneracy and superthermal ions on dust acoustic solitary structures](https://arxiv.org/abs/2602.12585)
*Rupak Dey,Gadadhar Banerjee*

Main category: physics.plasm-ph

TL;DR: Dust acoustic solitary waves in unmagnetized dusty electron-positron-ion plasma with degenerate electrons/positrons and superthermal ions support only negative potential rarefactive solitons within bounded Mach numbers.


<details>
  <summary>Details</summary>
Motivation: To understand nonlinear dust acoustic dynamics in space/astrophysical dusty plasmas where electrons and positrons are partially degenerate (Fermi-Dirac statistics) and ions follow superthermal kappa distributions.

Method: Used normalized fluid-Poisson model with polylogarithm expressions for degenerate electrons/positrons, cold dust grains for inertia. Linear dispersion analysis for phase speed, Sagdeev pseudopotential method for nonlinear solitary structures, and KdV limit for small amplitude approximation.

Result: System supports only negative potential (rarefactive) DA solitary waves within bounded subsonic Mach numbers. Critical Mach number depends on degeneracy parameters and ion spectral index. Soliton amplitude/width sensitive to degeneracy strength, ion-positron concentration ratios, ion temperature ratio, and ion superthermality.

Conclusion: Combined effects of finite temperature degeneracy and superthermal ions significantly shape nonlinear DA dynamics in space/astrophysical dusty plasmas, with explicit dependencies on plasma parameters revealed through analytical and numerical approaches.

Abstract: We examine dust acoustic (DA) solitary structures in an unmagnetized, collisionless dusty electron positron ion (epi) plasma in which electrons and positrons are described by finite temperature Fermi Dirac statistics and ions obey a superthermal kappa distribution. A normalized fluid Poisson model is formulated using polylogarithm based expressions for the partially degenerate electrons positrons, while cold negatively charged dust grains provide the inertial response. Linear dispersion analysis yields a modified DA phase speed. Nonlinear solitary structures are investigated using the Sagdeev pseudopotential method. The system is found to support only negative potential (rarefactive) DA solitary waves within a bounded subsonic Mach number interval. The critical Mach number, consequently, the corresponding linear DA speed show an explicit dependence on the degeneracy parameters and the ion spectral index. The amplitude and width of the solitary structures are shown to be highly sensitive to the electron (positron) degeneracy strength, the ion positron concentration ratios, the ion temperature ratio, and the superthermality of the ions. A small amplitude approximation of the pseudopotential reduces the system to the Korteweg de Vries limit, providing closed form expressions for the soliton characteristics, in agreement with the Sagdeev predictions. The results clarify the combined roles of finite temperature degeneracy and superthermal ions in shaping nonlinear DA dynamics in space and astrophysical dusty plasmas.

</details>


### [31] [Long-Pulse Fast Ignition in MagLIF](https://arxiv.org/abs/2602.12673)
*Benjamin Wang,Henry Fetsch,Nathaniel J. Fisch*

Main category: physics.plasm-ph

TL;DR: MagLIF's geometry and magnetic fields make fast ignition more practical by relaxing engineering constraints that previously limited its viability.


<details>
  <summary>Details</summary>
Motivation: Fast ignition for inertial confinement fusion requires extremely high-power lasers, creating engineering challenges that question its fundamental practicality. The paper aims to show how magnetized liner inertial fusion (MagLIF) can overcome these limitations.

Method: MagLIF uses large-aspect-ratio cylindrical geometry and strong axial magnetic fields to enable ignition at lower areal densities. The magnetic fields also collimate ignitor electrons, increasing standoff distance and reducing ignitor energy requirements.

Result: MagLIF significantly relaxes engineering constraints on energy deposition and repetition rate while maintaining high yields. The axial magnetic fields improve electron collimation, allowing greater standoff distance and energy savings.

Conclusion: The fast ignition paradigm may be considerably more viable in a MagLIF context due to the tremendous relaxation of engineering constraints that have historically limited fast ignition practicality.

Abstract: The fast ignition paradigm for inertial confinement fusion (ICF) allows for extremely high gains but requires fuel to be heated very quickly to outpace hotspot disassembly and energy losses. This demands lasers with high power and intensity, posing engineering challenges that have called into question the fundamental practicality of fast ignition. Magnetized liner inertial fusion (MagLIF) circumvents these problems through its large-aspect-ratio cylindrical geometry and strong axial magnetic fields that allow for ignition at lower areal densities. Furthermore, MagLIF's large aspect ratio and higher yields relax other constraints on energy deposition and repetition rate while its axial magnetic fields can be used to collimate ignitor electrons and thereby increase allowed standoff distance and save on ignitor energy. This tremendous overall relaxation of the engineering constraints that have historically limited the practicality of fast ignition suggests that the paradigm may be considerably more viable in a MagLIF context.

</details>


### [32] [Resonant Excitation of Surface Plasmon for Wakefield Acceleration by Beating GW Lasers on Smooth Cylindrical Surface](https://arxiv.org/abs/2602.12789)
*Bifeng Lei,Hao Zhang,Alexandre Bonatto,Bin Liu,Javier Resta-Lopez,Matt Zepf,Guoxing Xia,Carsten Welsch*

Main category: physics.plasm-ph

TL;DR: Resonant surface plasmon excitation on cylindrical plasma-vacuum interface using two co-propagating laser pulses enables efficient wakefield acceleration with accessible laser power.


<details>
  <summary>Details</summary>
Motivation: To develop portable laser-driven plasma wakefield accelerators by overcoming limitations of planar geometries and single lasers through resonant surface plasmon excitation.

Method: Theoretical derivation of SP dispersion relation, field amplitude, geometric coupling factor, and resonance conditions, validated by 3D particle-in-cell simulations of two co-propagating laser pulses on cylindrical plasma-vacuum interface.

Result: Curvature-induced geometric effects modify SP dispersion enabling resonant matching by laser beat waves, generating high-amplitude SP-based wakefields with few gigawatt lasers within reach of state-of-the-art fibre lasers.

Conclusion: This mechanism opens a route toward portable laser-driven plasma wakefield accelerators by enabling efficient resonant surface plasmon excitation on curved interfaces with accessible laser technology.

Abstract: We present a theoretical and numerical study of resonant surface-plasmon (SP) excitation driven by the beating of two co-propagating laser pulses on a smooth cylindrical plasma-vacuum interface. Analytical expressions for the SP dispersion relation, field amplitude, geometric coupling factor, and resonance conditions are derived and validated by fully three-dimensional particle-in-cell simulations. We reveal that curvature-induced geometric effects can substantially modify the SP dispersion and enable resonant matching by laser beat waves. This is inaccessible in planar geometries or with a single laser. Under matched resonance conditions, a high-amplitude SP-based wakefield can be generated by a few gigawatt lasers, placing this mechanism within reach of state-of-the-art fibre lasers. It therefore opens a route toward portable laser-driven plasma wakefield accelerators.

</details>


### [33] [Quantitative 3D non-linear simulations of shattered pellet injection in ASDEX Upgrade using JOREK](https://arxiv.org/abs/2602.12813)
*W. Tang,M. Hoelzl,P. Heinrich,D. Hu,F. J. Artola,P. de Marne,M. Dibon,M. Dunne,O. Ficker,P. Halldestam,S. Jachmich,M. Lehnen,E. Nardon,G. Papp,A. Patel,U. Sheikh,the ASDEX Upgrade Team,the EUROfusion Tokamak Exploitation Team,the JOREK Team*

Main category: physics.plasm-ph

TL;DR: Researchers improved 3D MHD modeling of shattered pellet injection for ITER disruption mitigation by adding parallel heat-flux limiting, enabling quantitative predictions of thermal quench duration and radiation fraction.


<details>
  <summary>Details</summary>
Motivation: SPI has many optimization parameters for ITER disruption mitigation, but current models only provide qualitative comparisons. There's a need for quantitative validation to optimize SPI for minimizing heat loads, electromagnetic forces, and relativistic electron formation.

Method: Applied 3D non-linear magnetohydrodynamic modeling to SPI experiments in ASDEX Upgrade, incorporating a simplified treatment of parallel heat-flux limiting to resolve previous discrepancies between simulations and experiments.

Result: The addition of parallel heat-flux limiting enabled transition from qualitative to quantitative predictions of thermal quench duration and radiation fraction, improving model reliability for matching key disruption mitigation processes.

Conclusion: The improved modeling increases confidence in using high-fidelity simulations for predictive studies of SPI optimization in ITER, moving toward quantitative validation and experiment interpretation.

Abstract: Shattered pellet injection (SPI) as primary mitigation method for major disruptions in ITER has a large parameter space available for optimization including the total amount of injected material, the size of the individual pellet fragments, the material composition, and the timing of multiple injections. This flexibility needs to be exploited to simultaneously minimize thermal heat loads, electromagnetic vessel forces, and formation of relativistic electrons and their impacts on plasma facing components. In this article, we apply 3D non-linear magnetohydrodynamic modelling to SPI experiments in the ASDEX Upgrade tokamak, going beyond our previous work [Tang et al Nucl. Fusion 65 116003 (2025)] by resolving some discrepancies between simulations and experiment and thus opening the path to quantitative model validation and experiment interpretation. The key element that enables the transition from merely qualitative comparisons to quantitatively reliable predictions of the thermal quench duration and the radiation fraction is the incorporation of a simplified treatment of parallel heat-flux limiting. The work increases the confidence of matching the key processes of disruption mitigation with this high fidelity modelling in view of predictive studies for ITER.

</details>


### [34] [Role of the radial electric field in the confinement of energetic ions in the Wendelstein 7-X stellarator](https://arxiv.org/abs/2602.12969)
*M. Arranz,J. L. Velasco,I. Calvo,D. Carralero*

Main category: physics.plasm-ph

TL;DR: W7-X stellarator's fast-ion confinement optimization is validated numerically using ASCOT5, showing radial electric field effects are equivalent to β effects, enabling experimental validation strategies.


<details>
  <summary>Details</summary>
Motivation: Good fast-ion confinement is essential for fusion reactors. W7-X was optimized for this in reactor-relevant high-β scenarios, but experimental validation is challenging due to power limitations and inevitable radial electric field effects.

Method: Numerical study using ASCOT5 code to analyze fast-ion confinement in various W7-X scenarios. Performed parameter scans on both β and radial electric field to characterize their effects.

Result: Confirmed that radial electric field effects on fast-ion losses are equivalent to those produced by β. Identified viable experimental scenarios using experimentally-based profiles that leverage this equivalence.

Conclusion: The equivalence between radial electric field and β effects enables experimental validation of W7-X's fast-ion confinement optimization strategy, overcoming previous experimental challenges.

Abstract: Good fast-ion confinement is an essential requirement for a fusion reactor. The magnetic configuration of the Wendelstein 7-X (W7-X) stellarator is partially optimized in this regard in a reactor-relevant scenario: it is expected to show improved fast-ion confinement when $β$ is high and the effect of the radial electric field is negligible. The experimental validation of this optimization is difficult since, with the available power, achieving high $β$ under appropriate conditions for the validation is challenging and the effect of the radial electric field is inevitable. In this work, the confinement of fast ions in W7-X has been studied numerically for a variety of scenarios via the ASCOT5 code. The effect of the radial electric field on fast-ion losses is confirmed to be equivalent to the one produced by $β$, and this is characterized by means of scans on both parameters. Through a preliminary study with experimentally-based profiles, a viable scenario is identified that takes advantage of this effect for the experimental validation of the optimization strategy of W7-X.

</details>


### [35] [Structure preservation using discrete gradients in the Vlasov-Poisson-Landau system](https://arxiv.org/abs/2602.13068)
*Daniel S. Finn,Joseph V. Pusztay,Matthew G. Knepley,Mark F. Adams*

Main category: physics.plasm-ph

TL;DR: Structure-preserving PIC method for Vlasov-Poisson-Landau system with discrete gradient time integrators, ensuring conservation laws and entropy monotonicity.


<details>
  <summary>Details</summary>
Motivation: The Vlasov-Poisson-Landau system accurately models hot plasma dynamics at kinetic scales dominated by small-angle Coulomb collisions. There's a need for numerical methods that preserve the fundamental physical structure of the system.

Method: Combines particle-in-cell (PIC) discretization with discrete gradient time integrators. Uses conservative integrator for both Hamiltonian Vlasov-Poisson equations and dissipative Landau equation. Implemented using PETSc library.

Result: The scheme guarantees conservation of mass, momentum, and energy, and preserves monotonicity of entropy production in both time-continuous and discrete systems.

Conclusion: The framework provides a structure-preserving numerical method for the Vlasov-Poisson-Landau system that maintains fundamental physical properties through discrete gradient time integration combined with PIC discretization.

Abstract: We present a novel structure-preserving framework for solving the Vlasov-Poisson-Landau system of equations using a particle in cell (PIC) discretization combined with discrete gradient time integrators. The Vlasov-Poisson-Landau system is an accurate model for studying hot plasma dynamics at a kinetic scale where small-angle Coulomb collisions dominate. Our scheme guarantees conservation of mass, momentum and energy as well as preservation of the monotonicity of entropy production in both the time-continuous and discrete systems. We employ the conservative integrator for both the Hamiltonian Vlasov-Poisson equations and the dissipative Landau equation using the PETSc library (www.mcs.anl.gov/petsc) to showcase structure-preserving properties.

</details>


### [36] [Exact moment models for conservation laws in phase space](https://arxiv.org/abs/2602.13180)
*Tileuzhan Mukhamet,Katharina Kormann*

Main category: physics.plasm-ph

TL;DR: Moment equations provide a reduced-order alternative to kinetic models for plasma/gas/liquid simulation, using centered moments parameterization to exactly solve hyperbolic conservation laws.


<details>
  <summary>Details</summary>
Motivation: To develop efficient simulation methods for plasmas, gases, and liquids that require fewer degrees of freedom than full phase space models while still capturing kinetic effects.

Method: Uses Burby's centered moments parameterization of distribution function to derive moment equations that exactly solve hyperbolic conservation laws; also derives particle model using phase space moments parameterization.

Result: Develops moment equations and particle models based on moment parameterizations; applies the method to both non-relativistic and relativistic Vlasov-Maxwell equations.

Conclusion: Moment equations offer an efficient alternative to kinetic models with reduced computational cost while maintaining kinetic accuracy through proper moment parameterization.

Abstract: Moment equations offer a compelling alternative to the kinetic description of plasmas, gases, and liquids. Their simulation requires fewer degrees of freedom than phase space models, yet it can still incorporate kinetic effects to a certain extent. To derive moment equations, we use a parameterization of the distribution function using centered moments, as proposed by Burby. This yields moment equations for which the parameterized distribution function exactly solves the hyperbolic conservation law. Similarly, a particle model is derived based on a parametrization of the distribution function using phase space moments. Finally, we present the application of the method to the non-relativistic and relativistic Vlasov--Maxwell equations.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [37] [Optimal bounds for the cost of fast controls of a KdV system](https://arxiv.org/abs/2602.12698)
*Hoai-Minh Nguyen*

Main category: math.OC

TL;DR: Optimal cost bounds for fast controls of linearized and nonlinear KdV systems using Neumann boundary control for non-critical lengths.


<details>
  <summary>Details</summary>
Motivation: The operator for linearized KdV system is neither self-adjoint nor skew-adjoint, making its spectral properties unsuitable for moment method, leaving optimal cost bounds as an open problem.

Method: Shift attention to a related KdV system and derive optimal bounds from this new system instead of directly analyzing the original linearized system.

Result: Successfully obtained optimal cost bounds for fast controls of both linearized and nonlinear KdV systems with Neumann boundary control for non-critical lengths.

Conclusion: By analyzing a related KdV system, the paper resolves the open problem of determining optimal cost bounds for fast controls in KdV systems with Neumann boundary control.

Abstract: We study the cost of fast controls for a linearized KdV system and a nonlinear KdV system locally, using right Neumann boundary control for non-critical lengths. Since the operator associated with the linearized system is neither self-adjoint nor skew-adjoint, its (known) spectral properties are not directly amenable to the moment method, leaving optimal cost bounds an open problem. We address this difficulty by shifting attention to a related KdV system and deriving the optimal bounds from the new one.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [38] [Ballistic Surfing Acceleration as a Coherent Mechanism for Electron Acceleration in Galaxy Cluster Shocks](https://arxiv.org/abs/2602.12647)
*Ji-Hoon Ha,Krzysztof Stasiewicz*

Main category: astro-ph.HE

TL;DR: Ballistic surfing acceleration (BSA) may explain radio relics in galaxy clusters better than diffusive shock acceleration, requiring only tiny efficiency to match observations.


<details>
  <summary>Details</summary>
Motivation: Diffusive shock acceleration (DSA) faces challenges in explaining radio relics in galaxy clusters due to low-Mach-number, weakly turbulent environments where DSA efficiency is reduced. Recent results question DSA's viability, motivating exploration of alternative acceleration mechanisms.

Method: The authors investigate ballistic surfing acceleration (BSA) as an electrodynamically grounded alternative. They formulate BSA under typical cluster shock conditions, derive balance between acceleration by shock convection electric field and radiative losses, determine maximum electron energy and steady-state spectrum, and forward-model synchrotron emission.

Result: BSA can reproduce observed spectral curvature and high-frequency steepening of Sausage and Toothbrush relics with extremely small efficiency (~10⁻⁹ to 10⁻⁸). Despite this tiny efficiency, electrons can be accelerated to Lorentz factors γ~10⁴-10⁵ under cluster conditions.

Conclusion: Radio relics provide a promising astrophysical laboratory for probing coherent acceleration, and the BSA framework may account for relativistic electron production in cluster shocks, offering a viable alternative to DSA.

Abstract: Radio relics in merging galaxy clusters are widely interpreted as synchrotron emission from relativistic electrons accelerated at large-scale shocks. However, the efficiency of diffusive shock acceleration (DSA) is expected to be reduced in the low-Mach-number, weakly turbulent environments characteristic of cluster merger shocks, and recent results suggest that DSA itself may not constitute a viable physical mechanism. In this work, we investigate ballistic surfing acceleration (BSA) as an electrodynamically grounded mechanism for electron energization that does not rely on prescribed diffusion coefficients. We formulate BSA under typical cluster shock conditions and derive the balance between coherent acceleration by the shock convection electric field and radiative losses due to synchrotron and inverse-Compton cooling. This balance determines both the maximum electron energy and the resulting steady-state spectrum. By forward-modeling the associated synchrotron emission and comparing it with integrated radio observations of the Sausage and Toothbrush relics, we find that the observed spectral curvature and high-frequency steepening can be reproduced when only a very small fraction ($\sim 10^{-9} - 10^{-8}$) of the available BSA acceleration capacity contributes to systematic electron energization. Despite this extremely small efficiency, it is sufficient to accelerate electrons to Lorentz factors $γ\sim 10^4 - 10^5$ under cluster conditions. These results suggest that radio relics provide a promising astrophysical laboratory for probing coherent acceleration, and that the BSA framework may account for the production of relativistic electrons in cluster shocks.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [39] [Odd Radio Circles Modeled by Shock-Bubble Interactions](https://arxiv.org/abs/2602.12479)
*Yiting Wang,Sebastian Heinz*

Main category: astro-ph.GA

TL;DR: ORCs may be synchrotron-emitting vortex rings formed by shock interaction with fossil radio lobes via Richtmyer-Meshkov instability, matching observed properties without requiring central galaxy alignment.


<details>
  <summary>Details</summary>
Motivation: To explain the physical nature and origins of Odd Radio Circles (ORCs), a newly discovered class of astronomical objects whose formation mechanisms remain unclear.

Method: 3D magnetohydrodynamic simulations with a new scale-free method to model Inverse-Compton and synchrotron cooling at high frequencies, testing various model parameters against observational constraints.

Result: Shock strengths of Mach 2-4 match data; initial bubble sizes of 140-250 kpc with energy 10^57-10^59 erg; ORCs located in low-density outskirts of galaxy groups with ages 70-200 Myr; synthetic maps match ORC1 polarization.

Conclusion: RMI-driven vortex ring model explains ORC properties without requiring central galaxy alignment, making it redshift-agnostic and consistent with fossil lobes from moderately powerful radio galaxies.

Abstract: The physical nature and origins of the newly discovered class of Odd Radio Circles (ORCs) remain unclear. We investigate a model whereby ORCs are synchrotron-emitting vortex rings formed by the Richtmyer-Meshkov instability (RMI) when a shock interacts with a low-density fossil radio lobe in the intergalactic medium using 3D magnetohydrodynamic simulations. These rings initially exhibit oscillatory behavior that damps over time. We implement a new method to model Inverse-Compton cooling and synchrotron cooling at high frequencies in a scale-free manner, enabling us to test a wide range of model parameters against the observational constraints. We find that shock strengths of Mach 2-4 are consistent with the data, as expected in accretion, merger-driven, or active galactic nuclei-driven shocks. We find that the initial size of the bubbles required to explain the rings ranges from 140 to 250 kpc, with initial energy in the bubble of order $10^{57}-10^{59}$ erg, consistent with fossil lobes inflated by moderately powerful radio galaxies. Derived ambient pressures and densities place ORCs in low density environments, such as the outskirts of galaxy groups with ages of order 70-200 Myr. Our synthetic radio maps match the polarization properties of ORC1 and predict a dependency of the tangential magnetic field angle on the aspect ratio of ORCs. A key distinguishing trait of the RMI-driven vortex ring model is that it does not require the ORC to be centered on its host galaxy and is therefore redshift agnostic.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [40] [Microlocal analysis of Radon transforms over quadric surfaces](https://arxiv.org/abs/2602.12453)
*Gaik Ambartsoumian,Raluca Felea,Venkateswaran P. Krishnan,Clifford J. Nolan,Eric Todd Quinto*

Main category: math.CA

TL;DR: The paper analyzes microlocal properties of generalized Radon transforms over quadric hypersurfaces, classifying singularity types based on matrix signature and hypersurface geometry.


<details>
  <summary>Details</summary>
Motivation: To understand the microlocal properties and singularity structures of generalized Radon transforms defined over families of quadric hypersurfaces, which is important for applications in integral geometry and tomography.

Method: Study the singularities of right and left projections of the canonical relation associated with these operators. Analyze how these singularities are determined by the signature of the symmetric, invertible matrix A defining the quadric surfaces and the geometry of the hypersurface S where centers lie.

Result: 1) For positive/negative definite matrices (ellipsoids) with strictly convex S: singularities are folds. 2) For indefinite matrices (hyperboloid-type quadrics) with strictly convex or cylindrical S: cusp, fold, or blowdown singularities occur. 3) For paraboloid surfaces: the Bolker condition is satisfied.

Conclusion: The singularity structure of generalized Radon transforms over quadric hypersurfaces is systematically classified based on matrix signature and hypersurface geometry, providing a complete picture of microlocal properties for these important integral operators.

Abstract: We study the microlocal properties of generalized Radon transforms over a family of quadric hypersurfaces whose centers lie on an orientable hypersurface $S$. The quadric surfaces we consider are level sets of the quadratic form associated to a symmetric, invertible matrix $A$, with real entries. We study the singularities of the right and left projections of the canonical relation associated with these operators and show that they are determined by the signature of the matrix $A$ and the hypersurface $S$. If the matrix is positive/negative definite (i.e., the surface of integration is an ellipsoid) and $S$ is strictly convex, we prove that the singularities are folds. If the matrix is indefinite (i.e., the surface of integration is a hyperboloid-type quadric) and $S$ is either strictly convex or a cylinder, then cusp, fold, or blowdown singularities are present. We also study the case when the surface of integration is a paraboloid and show that the Bolker condition is satisfied.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [41] [The condition number of a random banded Toeplitz matrix is typically large](https://arxiv.org/abs/2602.12581)
*Paulo Manrique*

Main category: math.PR

TL;DR: Banded Toeplitz matrices with symmetric bandwidth (r=s) are well-conditioned with high probability, while asymmetric bandwidth (r≠s) leads to ill-conditioning.


<details>
  <summary>Details</summary>
Motivation: To understand how structural constraints in random matrices (beyond iid entries) affect conditioning, specifically examining Toeplitz matrices with banded structure.

Method: Analyzing banded Toeplitz matrices with r diagonals below and s diagonals above the main diagonal, studying the impact of bandwidth symmetry/asymmetry on condition numbers.

Result: Bandwidth symmetry is crucial: when r=s, banded Toeplitz matrices are well-conditioned with high probability; when r≠s, they are typically ill-conditioned.

Conclusion: Structural constraints significantly impact numerical behavior of random matrices, with bandwidth asymmetry playing a decisive role in conditioning of banded Toeplitz matrices.

Abstract: It is well known that square matrices with independent and identically distributed (iid) random entries are typically well conditioned. A natural question is whether this favorable behavior persists for random matrices whose entries obey additional structure, i.e., their position inside of the matrix. A prominent class of structured matrices is given by {\it Toeplitz matrices}, characterized by constant diagonals. A particular tractable subclass is that of circulant matrices, whose additional characteristic (its entries {\it circulate} row by row) allows one to express their conditioning in terms of the localization of the zeros of a associated polynomial. When the entries of a circulant matrix are iid, the matrix is well conditioned precisely when the corresponding random polynomial has no zeros on the unit circle. This connection is especially relevant because, as the degree of a random polynomial increases, its zeros tend to concentrate near the unit circle, making it a delicate problem to quantify how close the closest zeros lie to the unit circle. Another notable family within the Toeplitz class is that of {\it banded Toeplitz matrices}, namely matrices for which only finitely many diagonals around the main diagonal may be nonzero. These matrices have been extensively studied in Operator Theory, nevertheless, despite their apparent simplicity, they raise subtle questions regarding the behavior of their condition numbers. In present work we show that the bandwidth asymmetry plays a decisive role: if the band contains $r$ diagonals below and $s$ diagonal above the main diagonal, then if $r=s$ the banded Toeplitz matriz is well conditioned with high probability, whereas if $r\neq s$ it is typically ill conditioned. This highlights that structural constraints can have a impact on the numerical behavior of random matrices.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [42] [Neural Quantum States Based on Selected Configurations](https://arxiv.org/abs/2602.12993)
*Marco Julian Solanki,Lexin Ding,Markus Reiher*

Main category: physics.chem-ph

TL;DR: NQS-SC outperforms NQS-VMC for electronic structure calculations, especially for statically correlated systems, and should be the new default approach.


<details>
  <summary>Details</summary>
Motivation: While neural quantum states (NQS) offer flexible wave function parameterization for quantum chemistry, variational Monte Carlo (VMC) has limitations for electronic Hamiltonians including sharply peaked distributions, stochastic gradient noise, and slow convergence. The paper aims to systematically compare NQS-VMC with the newer NQS-SC approach to determine which is better for capturing electronic correlation.

Method: The study conducts a systematic comparison of ground-state optimizations using both NQS-VMC and NQS-SC approaches for molecular systems dominated by either static or dynamical correlation. The comparison evaluates energy accuracy and wave-function coefficients across different correlation regimes.

Result: NQS-SC demonstrates clear advantages over NQS-VMC in both energy accuracy and wave-function coefficients, particularly for statically correlated molecules. NQS-SC shows robust systematic improvability while NQS-VMC does not. However, neither method efficiently captures dynamical correlation.

Conclusion: NQS-SC should replace NQS-VMC as the default approach for electronic structure calculations. Future work should focus on hybrid methods like multiconfigurational perturbation theories built on NQS solutions to address dynamical correlation limitations.

Abstract: Neural quantum states (NQS) provide a flexible and highly expressive parameterization of wave functions for strongly correlated problems in quantum chemistry. Despite rapid advances in network architectures, the evaluation of electronic energies remains almost exclusively based on variational Monte Carlo (VMC). While VMC is effective for structured systems such as spin chains, its accuracy and efficiency for electronic Hamiltonians are hindered by sharply peaked distributions, stochastic gradient noise, and slow convergence with sample size. In this letter, we assess the capability of NQS-VMC to efficiently capture correlation in electronic ground states by comparing it to a recently developed NQS-based selected configuration (NQS-SC) approach. We set up a systematic comparison of the ground-state optimizations obtained with NQS-VMC and NQS-SC for molecular systems dominated by either static or dynamical correlation. The comparison demonstrates a clear advantage of NQS-SC over NQS-VMC in both energy accuracy and wave-function coefficients, particularly for statically correlated molecules. Moreover, NQS-SC exhibits robust systematic improvability, whereas NQS-VMC does not. These findings position NQS-SC as the new default approach over NQS-VMC for electronic structure calculations. We further observe that neither NQS-SC nor NQS-VMC can efficiently capture dynamical correlation, highlighting the need for future hybrid methods, such as multiconfigurational perturbation theories built on top of NQS solutions.

</details>


<div id='nlin.SI'></div>

# nlin.SI [[Back]](#toc)

### [43] [Effective dynamics and defect expansions for polynomial PDEs on thin annuli](https://arxiv.org/abs/2602.12308)
*Jean-Pierre Magnot*

Main category: nlin.SI

TL;DR: Develops geometric framework for PDEs on thin annuli using Sobolev orthogonal polynomials, proves dimension reduction to 1D dynamics, applies to integrable and non-integrable systems.


<details>
  <summary>Details</summary>
Motivation: To analyze polynomial PDEs on thin annuli with unified geometric perspective on dimension reduction, homogenization, and integrability in thin geometries.

Method: Constructs Sobolev orthogonal polynomial bases using renormalized inner products, develops stable Galerkin approximations, proves dimension-reduction theorem for polynomial Hamiltonian and dissipative PDEs.

Result: Shows solutions converge to effective 1D dynamics on limiting circle, identifies transverse defect correctors, derives cell problems for anisotropic dispersive/homogenized effects, establishes stability under Sobolev order changes.

Conclusion: Provides unified geometric framework for thin geometry PDEs, introduces Sobolev orthogonal polynomials as constructive tool for multiscale analysis, applicable to wide range of integrable and non-integrable systems.

Abstract: We develop a geometric and analytic framework for polynomial partial differential equations posed on thin annuli in the plane. Using renormalized Sobolev inner products, we construct Sobolev orthogonal polynomial bases adapted to the thin geometry and use them to define stable Galerkin approximations.
  We prove a general dimension-reduction theorem for polynomial Hamiltonian and dissipative PDEs, showing that solutions converge to effective one-dimensional dynamics on the limiting circle. Beyond the leading-order limit, we identify transverse defect correctors and derive cell problems describing anisotropic dispersive and homogenized effects.
  Our framework applies uniformly to integrable models (KdV, modified KdV, nonlinear Schrödinger, sine--Gordon), anisotropic dispersive systems such as Zakharov--Kuznetsov, and non-integrable perturbations including dissipation, forcing, and rapidly oscillating coefficients. We establish stability of the effective dynamics under changes of Sobolev order and of polynomial Hilbert geometry, and show robustness of the associated Galerkin schemes.
  The results provide a unified geometric perspective on dimension reduction, homogenization, and integrability in thin geometries, and introduce Sobolev orthogonal polynomial methods as a constructive tool for multiscale PDE analysis.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [44] [Enhanced numerical approaches for modeling insoluble surfactants in two-phase flows with the diffuse-interface method](https://arxiv.org/abs/2602.13076)
*Shu Yamashita,Shintaro Matsushita,Tetsuya Suekane*

Main category: physics.flu-dyn

TL;DR: The paper proposes two simple, practical approaches to improve accuracy of diffuse-interface methods for simulating insoluble surfactant transport at fluid interfaces, without significant computational cost increases.


<details>
  <summary>Details</summary>
Motivation: Surfactants significantly influence two-phase flow dynamics at interfaces, but accurate numerical simulation of surfactant transport requires improved methods. Current diffuse-interface approaches for insoluble surfactants need better accuracy.

Method: Two approaches: (1) adopting a formulation that avoids spatial derivatives of variables with sharp gradients, and (2) allowing the delta function width to be specified independently of the interface width. Both maintain low computational cost and implementation simplicity.

Result: Numerical tests demonstrate the effectiveness of the proposed approaches. The paper also presents a challenging test case that hasn't been previously discussed, intended as a benchmark for comparing various methods in the literature.

Conclusion: The proposed approaches provide simple, practical improvements to diffuse-interface methods for surfactant-laden flows, offering better accuracy without significant drawbacks. The challenging test case serves as a valuable benchmark for future method evaluations.

Abstract: Surfactants reside at the interface of two-phase flows and significantly influence the flow dynamics. Numerical simulations are essential for a comprehensive understanding of such surfactant-laden flows and require a method that can accurately simulate surfactant transport along the interface. In this study, we focus on interfacial transport models for insoluble surfactants based on the diffuse-interface method and propose two approaches to improve their accuracy: (a) adopting a formulation that avoids the spatial derivatives of variables with sharp gradients and (b) allowing the width of the delta function to be specified independently of the interface width. These approaches are simple and practical in that they do not lead to significant increases in computational cost, implementation complexity, or degradation of interface-capturing accuracy. We conduct a series of numerical tests to demonstrate the effectiveness of the proposed approaches. Finally, we present a challenging test case that is difficult to solve accurately and has not been previously discussed. We expect this case to serve as a valuable benchmark for evaluating and comparing the performances of various methods proposed in the literature.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [A Machine Learning Approach to the Nirenberg Problem](https://arxiv.org/abs/2602.12368)
*Gianfranco Cortés,Maria Esteban-Casadevall,Yueqing Feng,Jonas Henkel,Edward Hirst,Tancredi Schettini Gherardini,Alexander G. Stapleton*

Main category: cs.LG

TL;DR: A neural network approach (Nirenberg Neural Network) solves the Nirenberg problem of prescribing Gaussian curvature on S² using PINNs, achieving low losses for realizable curvatures and high losses for unrealizable ones.


<details>
  <summary>Details</summary>
Motivation: To develop a computational tool for exploring the longstanding geometric analysis problem of prescribing Gaussian curvature on the 2-sphere, providing quantitative insights into existence questions.

Method: Mesh-free physics-informed neural network (PINN) that directly parametrizes the conformal factor globally, trained with geometry-aware loss enforcing the curvature equation, with validation via Gauss-Bonnet theorem and spherical-harmonic expansions.

Result: The network achieves very low losses (10⁻⁷ - 10⁻¹⁰) for realizable prescribed curvatures, while unrealizable curvatures yield significantly higher losses, enabling assessment of unknown cases.

Conclusion: Neural solvers can serve as exploratory tools in geometric analysis, offering computational perspectives on longstanding existence questions like the Nirenberg problem.

Abstract: This work introduces the Nirenberg Neural Network: a numerical approach to the Nirenberg problem of prescribing Gaussian curvature on $S^2$ for metrics that are pointwise conformal to the round metric. Our mesh-free physics-informed neural network (PINN) approach directly parametrises the conformal factor globally and is trained with a geometry-aware loss enforcing the curvature equation. Additional consistency checks were performed via the Gauss-Bonnet theorem, and spherical-harmonic expansions were fit to the learnt models to provide interpretability.
  For prescribed curvatures with known realisability, the neural network achieves very low losses ($10^{-7} - 10^{-10}$), while unrealisable curvatures yield significantly higher losses. This distinction enables the assessment of unknown cases, separating likely realisable functions from non-realisable ones. The current capabilities of the Nirenberg Neural Network demonstrate that neural solvers can serve as exploratory tools in geometric analysis, offering a quantitative computational perspective on longstanding existence questions.

</details>


### [46] [Learning functional components of PDEs from data using neural networks](https://arxiv.org/abs/2602.13174)
*Torkel E. Loman,Yurij Salmaniw,Antonio Leon Villares,Jose A. Carrillo,Ruth E. Baker*

Main category: cs.LG

TL;DR: Neural networks embedded in PDEs can recover unknown functions from data, demonstrated with nonlocal aggregation-diffusion equations to recover interaction kernels and external potentials from steady state data.


<details>
  <summary>Details</summary>
Motivation: Partial differential equations often contain unknown functions that are difficult or impossible to measure directly, which hampers our ability to derive predictions from the model. While workflows for recovering scalar PDE parameters from data are well studied, there's a need to extend similar approaches to recover functions from data.

Method: Embed neural networks into PDEs and train them on data to approximate unknown functions with arbitrary accuracy. Using nonlocal aggregation-diffusion equations as a case study, recover interaction kernels and external potentials from steady state data. Investigate factors affecting recovery success including number of available solutions, their properties, sampling density, and measurement noise.

Result: The approach successfully recovers unknown functions from PDE data. The method can utilize standard parameter-fitting workflows, and the trained PDE can be treated as a normal PDE for purposes such as generating system predictions.

Conclusion: Neural networks embedded in PDEs provide an effective approach for recovering unknown functions from data, extending existing parameter-fitting workflows to functional recovery problems. The method is particularly useful for nonlocal aggregation-diffusion equations and can handle various practical factors affecting data quality.

Abstract: Partial differential equations often contain unknown functions that are difficult or impossible to measure directly, hampering our ability to derive predictions from the model. Workflows for recovering scalar PDE parameters from data are well studied: here we show how similar workflows can be used to recover functions from data. Specifically, we embed neural networks into the PDE and show how, as they are trained on data, they can approximate unknown functions with arbitrary accuracy. Using nonlocal aggregation-diffusion equations as a case study, we recover interaction kernels and external potentials from steady state data. Specifically, we investigate how a wide range of factors, such as the number of available solutions, their properties, sampling density, and measurement noise, affect our ability to successfully recover functions. Our approach is advantageous because it can utilise standard parameter-fitting workflows, and in that the trained PDE can be treated as a normal PDE for purposes such as generating system predictions.

</details>


### [47] [Rational Neural Networks have Expressivity Advantages](https://arxiv.org/abs/2602.12390)
*Maosen Tang,Alex Townsend*

Main category: cs.LG

TL;DR: Rational activation functions outperform standard activations in expressiveness and parameter efficiency, with exponential separation in approximation complexity.


<details>
  <summary>Details</summary>
Motivation: Standard activation functions (piecewise-linear and smooth) have limitations in expressiveness and parameter efficiency. The paper aims to demonstrate that trainable low-degree rational activations can overcome these limitations while maintaining practical usability.

Method: Theoretical analysis of neural networks with trainable low-degree rational activation functions. Establishes approximation-theoretic separations by comparing rational activations against standard fixed activations. Also includes practical integration into standard architectures and training pipelines.

Result: Exponential gap in parameter efficiency: rational networks need only poly(log log(1/ε)) overhead to approximate standard networks, while the converse requires Ω(log(1/ε)) parameters. Rational activations match or outperform fixed activations in practice under identical conditions.

Conclusion: Rational activation functions are more expressive and parameter-efficient than standard activations, with provable exponential separation in approximation complexity, while being practically viable for integration into existing neural network architectures.

Abstract: We study neural networks with trainable low-degree rational activation functions and show that they are more expressive and parameter-efficient than modern piecewise-linear and smooth activations such as ELU, LeakyReLU, LogSigmoid, PReLU, ReLU, SELU, CELU, Sigmoid, SiLU, Mish, Softplus, Tanh, Softmin, Softmax, and LogSoftmax. For an error target of $\varepsilon>0$, we establish approximation-theoretic separations: Any network built from standard fixed activations can be uniformly approximated on compact domains by a rational-activation network with only $\mathrm{poly}(\log\log(1/\varepsilon))$ overhead in size, while the converse provably requires $Ω(\log(1/\varepsilon))$ parameters in the worst case. This exponential gap persists at the level of full networks and extends to gated activations and transformer-style nonlinearities. In practice, rational activations integrate seamlessly into standard architectures and training pipelines, allowing rationals to match or outperform fixed activations under identical architectures and optimizers.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [48] [Hyb-Adam-UM: hybrid ultrametric-aware mtDNA phylogeny reconstruction](https://arxiv.org/abs/2602.12854)
*Dmitrii Chaikovskii,Weilai Qu,Boris Melnikov,Ye Zhang,Yuehong Zhao*

Main category: q-bio.PE

TL;DR: Hyb-Adam-UM: A method for completing missing entries in mitochondrial DNA distance matrices while preserving ultrametric structure for phylogeny inference.


<details>
  <summary>Details</summary>
Motivation: Computing all pairwise alignments for mtDNA distance matrices is computationally expensive. Missing entries degrade phylogenetic tree inference, and generic matrix completion methods don't preserve the ultrametric (tree-like) structure needed for accurate phylogeny reconstruction.

Method: Hybrid approach starting from Needleman-Wunsch distance backbone, completing missing entries by minimizing a robust triplet ultrametric-violation functional using Adam-style finite-difference optimizer. Enforces symmetry, non-negativity, and zero diagonal constraints while updating only missing entries.

Result: Consistently reduces ultrametric violations and achieves competitive reconstruction error. Shows improved topological accuracy and branch-length agreement compared to baseline methods (MW*/NJ* projection, Soft-Impute), with most significant gains at 85% missingness.

Conclusion: Hyb-Adam-UM effectively completes mitochondrial DNA distance matrices while preserving tree-like structure, offering improved phylogenetic inference especially under high missingness conditions.

Abstract: Motivation: mtDNA distance matrices are standard inputs for distance-based phylogeny, but computing all pairwise alignments is costly. Missing entries can degrade inferred topology and branch lengths, and generic matrix-completion methods may disrupt tree-like (ultrametric) structure.
  Results: We propose Hyb-Adam-UM, which starts from an alignment-limited Needleman-Wunsch distance backbone and completes the matrix by minimizing a robust triplet ultrametric-violation functional. An Adam-style finite-difference optimizer updates only missing entries while enforcing symmetry, non-negativity, and a zero diagonal. From one complete reference matrix, we generate 20 masked instances at 30%, 50%, 65%, and 85% missingness. Hyb-Adam-UM consistently reduces ultrametric violations and achieves competitive reconstruction error, with improved topological accuracy and branch-length agreement relative to MW*/NJ* projection baselines (which exactly preserve observed distances) and Soft-Impute; gains are most pronounced at 85% missingness.
  Availability and implementation: https://github.com/mitichya/hyb-adam-um/; Zenodo: https://doi.org/10.5281/zenodo.18609748
  Supplementary information: Supplementary data available online.

</details>
