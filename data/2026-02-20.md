<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 11]
- [math.AP](#math.AP) [Total: 17]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 10]
- [cs.LG](#cs.LG) [Total: 1]
- [math.FA](#math.FA) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 4]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [An entropy-stable oscillation-eliminating dgsem for the euler equations on curvilinear meshes](https://arxiv.org/abs/2602.16732)
*Jielin Yang,Guosheng Fu*

Main category: math.NA

TL;DR: Developed an entropy-stable high-order numerical method for 2D compressible Euler equations on curvilinear meshes using DG spectral elements with SBP property and oscillation control.


<details>
  <summary>Details</summary>
Motivation: To create robust, high-order numerical methods for compressible flows on complex geometries that maintain physical entropy principles while controlling nonphysical oscillations near discontinuities.

Method: Combines nodal discontinuous Galerkin spectral element method (DGSEM) with summation-by-parts (SBP) property for entropy stability, then integrates with modified oscillation-eliminating DG (OEDG) method using projection operators for general curvilinear meshes.

Result: The method achieves global discrete entropy inequality, effectively suppresses spurious oscillations, reduces computational cost through localized oscillation control, and works on both Cartesian and curvilinear meshes.

Conclusion: The proposed entropy-stable OEDG method provides accurate, robust, and effective high-order simulation of compressible flows on complex geometries while preserving conservation and entropy stability.

Abstract: We develop an entropy-stable high-order numerical method for the two-dimensional compressible Euler equations on general curvilinear meshes. The proposed approach is based on a nodal discontinuous Galerkin spectral element method (DGSEM) that satisfies the summation-by-parts (SBP) property. At the semidiscrete level, entropy stability is established through the SBP structure and the discrete metric identities associated with curvilinear coordinate mappings. By incorporating entropy-stable numerical fluxes at element interfaces, a global discrete entropy inequality is obtained. To further control nonphysical oscillations near strong discontinuities, the entropy-stable DG formulation is combined with a modified oscillation-eliminating discontinuous Galerkin (OEDG) method, which was originally proposed in [59]. We observe that the zero-order damping coefficient in the original OEDG method naturally serves as an effective shock indicator, which enables localization of the oscillation control mechanism and significantly reduces computational cost. Moreover, while the original OEDG formulation relies on local orthogonal modal bases and is primarily restricted to simplicial meshes, we reformulate the OE procedure using projection operators, allowing for a systematic extension to general curvilinear meshes. The resulting method preserves conservation and entropy stability while effectively suppressing spurious oscillations. A series of challenging numerical experiments is presented to demonstrate the accuracy, robustness, and effectiveness of the proposed entropy-stable OEDG method on both Cartesian and curvilinear meshes.

</details>


### [2] [Fast, High-Accuracy, Randomized Nullspace Computations for Tall Matrices](https://arxiv.org/abs/2602.16797)
*Ethan N. Epperly,Taejun Park,Yuji Nakatsukasa*

Main category: math.NA

TL;DR: RLobPCG is a randomized preconditioned eigensolver for computing smallest singular values of large tall matrices, combining sketching with LOBPCG for 12× speedup over classical methods.


<details>
  <summary>Details</summary>
Motivation: Need efficient methods to compute smallest singular triplets of large, tall matrices where classical methods like LOBPCG and Lanczos are slow or fail to converge.

Method: Combines randomized sketching (sketch-and-precondition) with LOBPCG eigensolver: uses small sketch to construct high-quality preconditioner, then runs LOBPCG on Gram matrix to refine singular vectors.

Result: Proves geometric convergence under subspace embedding assumption and modest singular value gap. Achieves near-optimal accuracy on matrices up to 10^6 rows, with 12× speedup over classical methods and robustness where others fail.

Conclusion: RLobPCG is an efficient, robust method for computing smallest singular triplets of large matrices, significantly outperforming existing iterative methods while maintaining theoretical guarantees.

Abstract: In this paper, we develop RLOBPCG, an efficient method for computing a small number of singular triplets corresponding to the smallest singular values of large, tall matrices. The algorithm combines randomized preconditioner from the sketch-and-precondition techniques with the LOBPCG eigensolver: a small sketch is used to construct a high-quality preconditioner, and LOBPCG is run on the Gram matrix to refine the singular vector. Under the standard subspace embedding assumption and a modest singular value gap between the two smallest singular values, we prove that RLOBPCG converges geometrically to the minimum singular vector. In numerical experiments, RLOBPCG achieves near-optimal accuracy on matrices with up to $10^6$ rows, outperforming classical LOBPCG and Lanczos methods by a speedup of up to $12\times$ and maintaining robustness when other iterative methods fail to converge.

</details>


### [3] [Domain Decomposition for Mean Curvature Flow of Surface Polygonal Meshes](https://arxiv.org/abs/2602.16874)
*Lenka Ptackova,Michal Outrata*

Main category: math.NA

TL;DR: Domain decomposition enables more efficient mean curvature flow for polygonal surface meshes by solving smaller parallel problems with optimized transmission conditions.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency of mean curvature flow for surface meshes with arbitrary polygonal faces by leveraging parallel processing capabilities through domain decomposition.

Method: Apply domain decomposition (with/without overlap) to split mesh into sub-meshes, use adapted Robin transmission conditions from optimized Schwarz method, solve smaller boundary value problems in parallel instead of one large problem.

Result: Achieved parallel processing of decomposed sub-problems while analyzing smoothing effects on shape quality and texture deformation.

Conclusion: Domain decomposition with optimized transmission conditions provides an effective approach for efficient mean curvature flow computation on polygonal surface meshes through parallelizable smaller problems.

Abstract: We examine the use of domain decomposition for potentially more efficient mean curvature flow of surface meshes, whose faces are arbitrary simple polygons. We first test traditional domain decomposition methods with and without overlap of deconstructed domains. And we present adapted Robin transmission conditions of optimized Schwarz method. We then analyze the resulting smoothing from the point of view of shape quality and texture deformation. By decomposing the initial mesh into two sub-meshes, we solve two smaller boundary value problems instead of one big problem, and we can process these two tasks almost entirely in parallel.

</details>


### [4] [ARCANE: Scalable high-degree cubature formulae for simulating SDEs without Monte Carlo error](https://arxiv.org/abs/2602.17151)
*Peter Koepernik,Thomas Coxon,James Foster*

Main category: math.NA

TL;DR: ARCANE algorithm automatically constructs high-degree cubature formulae for SDEs, achieving D=19 vs previous D=7, with orders of magnitude better accuracy than Monte Carlo.


<details>
  <summary>Details</summary>
Motivation: Monte Carlo sampling for SDEs requires huge sample sizes for accuracy. Existing cubature methods (deterministic weighted path sets) are theoretically promising but limited to low degree D=7, making them impractical.

Method: ARCANE algorithm efficiently and automatically constructs cubature formulae of arbitrary degree by generating deterministic weighted sets of paths that match Brownian signature moments up to degree D.

Result: ARCANE reproduces state-of-the-art (D=7) in seconds and reaches D=19 within hours on modest hardware. In simulations across multiple SDEs and error metrics, cubature formulae achieve orders of magnitude smaller error than Monte Carlo with same number of paths.

Conclusion: ARCANE enables practical high-degree cubature for SDEs, offering dramatic accuracy improvements over Monte Carlo sampling with the same computational cost.

Abstract: Monte Carlo sampling is the standard approach for estimating properties of solutions to stochastic differential equations (SDEs), but accurate estimates require huge sample sizes. Lyons and Victoir (2004) proposed replacing independently sampled Brownian driving paths with "cubature formulae", deterministic weighted sets of paths that match Brownian "signature moments" up to some degree $D$. They prove that cubature formulae exist for arbitrary $D$, but explicit constructions are difficult and have only reached $D=7$, too small for practical use. We present ARCANE, an algorithm that efficiently and automatically constructs cubature formulae of arbitrary degree. It reproduces the state of the art in seconds and reaches $\boldsymbol{D=19}$ within hours on modest hardware. In simulations across multiple different SDEs and error metrics, our cubature formulae robustly achieve an error orders of magnitude smaller than Monte Carlo with the same number of paths.

</details>


### [5] [Invertibility of the Fourier Diffraction Relation in Raster Scan Diffraction Tomography](https://arxiv.org/abs/2602.17344)
*Peter Elbau,Noemi Naujoks*

Main category: math.NA

TL;DR: Focused beam diffraction tomography has different Fourier recovery properties than plane-wave tomography - in 2D, some Fourier coefficients are not uniquely recoverable, while in higher dimensions they typically are.


<details>
  <summary>Details</summary>
Motivation: Many practical imaging systems use focused beams instead of plane waves for diffraction tomography, but the Fourier diffraction theorem differs for focused beams, creating ambiguity in Fourier coefficient recovery.

Method: Analyzed the Fourier diffraction relations for focused beam illumination, investigating which Fourier coefficients of the scattering potential can be uniquely recovered from the measurement relations.

Result: In dimensions higher than two, all Fourier coefficients appearing in the equations are typically uniquely determined. In two dimensions, only part of the Fourier coverage is uniquely recoverable, while distinct coefficients can produce identical data on the remaining subset.

Conclusion: The dimensionality of the imaging system critically affects the unique recoverability of Fourier coefficients in focused beam diffraction tomography, with fundamental limitations in 2D that don't exist in higher dimensions.

Abstract: Diffraction tomography aims to recover an object's scattering potential from measured wave fields. In the classical setting, the object is illuminated by plane waves from many directions, and the Fourier diffraction theorem gives a direct relation between the Fourier transform of the scattering potential of the object and the Fourier transformed measurements.
  In many practical imaging systems, however, focused beams are used instead of plane waves. These beams are then translated across the object to bring different regions of interest into focus. The Fourier diffraction relation adapted to this setting differs in one crucial point from the plane-wave case: while certain Fourier coefficients of the measurements still directly correspond to individual Fourier coefficients of the scattering potential, others are given by linear combinations of two Fourier coefficients of the scattering potential.
  This article investigates which Fourier coefficients of the scattering potential can be uniquely recovered from these relations. We show that in dimensions higher than two, all coefficients appearing in the equations are typically uniquely determined. In two dimensions, however, only part of the Fourier coverage is uniquely recoverable, while on the remaining subset, distinct coefficients can produce identical data.

</details>


### [6] [Analysis of an exponential integrator for stochastic PDEs driven by Riesz noise](https://arxiv.org/abs/2602.17348)
*Charles-Edouard Bréhier,David Cohen,Lluís Quer-Sardanyons,Johan Ulander*

Main category: math.NA

TL;DR: Explicit exponential integrator for parabolic SPDEs with Riesz kernel spatial correlation, analyzed for strong error bounds and convergence rates dependent on Riesz exponent.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze numerical methods for parabolic stochastic partial differential equations (SPDEs) driven by Gaussian noise that is white in time but spatially correlated via Riesz kernels, which is relevant for modeling various physical phenomena with long-range spatial dependencies.

Method: An explicit exponential integrator scheme for solving parabolic SPDEs in any spatial dimension, where the noise has temporal whiteness and spatial correlation described by Riesz kernels. The method is analyzed theoretically under coefficient assumptions.

Result: Strong error bounds are proven for the explicit exponential integrator, with convergence rates explicitly depending on the exponent in the Riesz kernel. Numerical experiments in 1D and 2D confirm the theoretical convergence results.

Conclusion: The proposed explicit exponential integrator is effective for parabolic SPDEs with Riesz kernel spatial correlations, with convergence rates that depend on the Riesz exponent, as validated by both theoretical analysis and numerical experiments.

Abstract: We present and study an explicit exponential integrator for parabolic SPDEs in any dimension driven by a Gaussian noise which is white in time and with spatial correlation given by a Riesz kernel. Under assumptions on the coefficients of the SPDE, we prove strong error bounds and exhibit how the rate of convergence depends on the exponent in the Riesz kernel. Finally, numerical experiments in spatial dimensions $1$ and $2$ are provided in order to confirm our convergence results.

</details>


### [7] [Raster Scan Diffraction Tomography](https://arxiv.org/abs/2602.17351)
*Peter Elbau,Noemi Naujoks,Otmar Scherzer*

Main category: math.NA

TL;DR: Extends diffraction tomography to incorporate focused beam illumination and scanning geometries, deriving new Fourier diffraction relation for quantitative reconstruction.


<details>
  <summary>Details</summary>
Motivation: Conventional diffraction tomography assumes monochromatic plane wave illumination, which doesn't reflect practical imaging systems like medical ultrasound that use focused beams and scanning. Current theory hasn't incorporated these practical measurement setups.

Method: Extends diffraction tomography by modeling incident fields as Herglotz waves to incorporate focused beams, derives new Fourier diffraction relation for scanning data, and analyzes different scan geometries.

Result: Develops theoretical framework for quantitative tomographic reconstruction from scanning data with focused illumination, providing systematic analysis of how different scan geometries influence reconstruction.

Conclusion: Bridges gap between conventional diffraction tomography theory and practical imaging systems by incorporating focused beam illumination and scanning geometries into the framework.

Abstract: Diffraction tomography is a widely used inverse scattering technique for quantitative imaging of weakly scattering media. In its conventional formulation, diffraction tomography assumes monochromatic plane wave illumination. This assumption, however, represents a simplification that often fails to reflect practical imaging systems such as medical ultrasound, where focused beams are used to scan a region of interest of the human body. Such measurement setups, combining focused illumination with scanning, have not yet been incorporated into the diffraction tomography framework. To bridge this gap, we extend diffraction tomography by modeling incident fields as Herglotz waves, thereby incorporating focused beams into the theory. Within this setting, we derive a new Fourier diffraction relation, which forms the basis for quantitative tomographic reconstruction from scanning data. Using this result, we systematically analyze how different scan geometries influence the reconstruction.

</details>


### [8] [Application and Evaluation of the Common Circles Method](https://arxiv.org/abs/2602.17353)
*Michael Quellmalz,Mia Kvåle Løvmo,Simon Moser,Franziska Strasser,Monika Ritsch-Marte*

Main category: math.NA

TL;DR: The paper presents a practical implementation of the common circle method for estimating sample motion in optical diffraction tomography, using temporal consistency constraints for stable reconstructions.


<details>
  <summary>Details</summary>
Motivation: In optical diffraction tomography of biological tissue, when samples are confined via contact-free acoustical force fields, their motion must be estimated from captured images for accurate reconstruction.

Method: The common circle method identifies intersections of Ewald spheres in Fourier space to determine rotational motion, with practical implementation incorporating temporal consistency constraints.

Result: Results on both simulated and real-world data demonstrate that the common circle method provides a computationally efficient alternative to full optimization methods for motion detection.

Conclusion: The common circle method offers a practical and efficient approach for motion estimation in optical diffraction tomography, enabling stable reconstructions of moving biological samples.

Abstract: We investigate the application of the common circle method for estimating sample motion in optical diffraction tomography (ODT) of sub-millimeter sized biological tissue. When samples are confined via contact-free acoustical force fields, their motion must be estimated from the captured images. The common circle method identifies intersections of Ewald spheres in Fourier space to determine rotational motion. This paper presents a practical implementation, incorporating temporal consistency constraints to achieve stable reconstructions. Our results on both simulated and real-world data demonstrate that the common circle method provides a computationally efficient alternative to full optimization methods for motion detection.

</details>


### [9] [Functional Analysis and Parallel Domain Decomposition for the TV-Stokes Model](https://arxiv.org/abs/2602.17494)
*Andreas Langer,Marc Runft,Talal Rahman,Xue-Cheng Tai,Bin Wu*

Main category: math.NA

TL;DR: This paper provides a rigorous mathematical foundation for the TV-Stokes image denoising model, identifies and fixes analytical inconsistencies in the original formulation, and develops the first parallelizable domain decomposition method for large-scale image processing.


<details>
  <summary>Details</summary>
Motivation: The TV-Stokes model is effective for image denoising but lacks proper mathematical analysis and parallelization capabilities. The authors aim to establish a rigorous functional-analytic foundation, resolve mathematical inconsistencies, and develop scalable parallel algorithms for large-scale image processing.

Method: 1) Formulate both steps of TV-Stokes in appropriate infinite-dimensional function spaces with dual formulations; 2) Analyze compatibility and mathematical consistency of the coupled system; 3) Identify and resolve analytical inconsistencies; 4) Examine orthogonal projection onto divergence-free subspace; 5) Develop domain decomposition method using overlapping Schwarz-type iterations on dual formulations; 6) Show local computability of divergence-free constraint in discrete setting.

Result: Established rigorous functional-analytic foundation for TV-Stokes, resolved mathematical inconsistencies in original formulation, proved existence of orthogonal projection in continuous setting, developed first domain decomposition method for TV-Stokes, and demonstrated fully parallelizable algorithm suitable for memory-constrained environments.

Conclusion: The paper successfully provides a solid mathematical foundation for TV-Stokes, resolves its analytical inconsistencies, and enables practical parallel implementation through domain decomposition, making the model suitable for large-scale image processing applications.

Abstract: The TV-Stokes model is a two-step variational method for image denoising that combines the estimation of a divergence-free tangent field with total variation regularization in the first step and then uses that to reconstruct the image in the second step. Although effective in practice, its mathematical structure and potential for parallelization have remained unexplored. In this work, we establish a rigorous functional-analytic foundation for the TV-Stokes model. We formulate both steps in appropriate infinite-dimensional function spaces, derive their dual formulations, and analyze the compatibility and mathematical consistency of the coupled system. In particular, we identify analytical inconsistencies in the original formulation and demonstrate how an alternative model resolves them. We also examine the orthogonal projection onto the divergence-free subspace, proving its existence in a continuous setting and establishing consistency with its discrete counterpart.
  Building on this theoretical framework, we develop the first domain decomposition method for TV-Stokes by applying overlapping Schwarz-type iterations to the duals of both steps. Although the divergence-free constraint gives rise to a global projection operator in the continuous model, we show that it becomes locally computable in the discrete setting. This insight enables a fully parallelizable algorithm suitable for large-scale image processing in memory-constrained environments. Numerical experiments demonstrate the correctness of the domain decomposition approach and its usability in parallel image reconstruction.

</details>


### [10] [High Order semi-implicit Rosenbrock type and Multistep methods for evolutionary partial differential equations with higher order derivatives](https://arxiv.org/abs/2602.17507)
*Boscarino Sebastiano,Giuseppe Izzo*

Main category: math.NA

TL;DR: Semi-implicit Rosenbrock/IMEX methods for high-order PDEs avoid Newton iterations and severe explicit time-step restrictions while achieving high-order accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for time-dependent PDEs with high-order spatial derivatives that avoid the severe time-step restrictions of explicit methods and the complexity of Newton iterations in fully implicit schemes.

Method: Combines semi-implicit strategy with Rosenbrock-type and IMEX linear multistep methods, using finite difference for spatial discretization. Constructs linearly implicit schemes without Newton iterations.

Result: Developed stable schemes up to order p=4 that avoid Δt = O(Δx^k) restrictions for k-th order PDEs. Numerical experiments confirm stability and expected accuracy orders for dissipative, dispersive, and biharmonic equations.

Conclusion: The semi-implicit approach provides flexible, efficient numerical methods for high-order PDEs that combine the stability advantages of implicit methods with the simplicity of explicit schemes, without requiring Newton iterations.

Abstract: The aim of this work is to apply a semi-implicit (SI) strategy within a Rosenbrock-type and IMEX linear multistep (LM) framework to a sequence of 1D time-dependent partial differential equations (PDEs) with high order spatial derivatives. This strategy provides great flexibility to treat these equations, and allows the construction of simple lienarly implicit schemes without any Newton iteration. Furthermore, the SI schemes so designed do not require the severe time-step restrictions typically encountered when using explicit methods for stability, i.e., $Δt = \mathcal{O}(Δx^k)$ for the $k$-th order PDEs with $k\ge 2$. For space discrertization, this strategy is combined with finite difference schemes. We provide example of methods up to order $p = 4$, and we illustrate the effectiveness of the schemes with appllications to dissipative, dispersive, and biharmonic-type equations. Numerical experiments show that the proposed schemes are stable and achieve the expected orders of accuracy.

</details>


### [11] [Computing the action of a matrix exponential on an interval via the $\star$-product approach](https://arxiv.org/abs/2602.17516)
*Stefano Pozza,Shazma Zahid*

Main category: math.NA

TL;DR: New method for computing e^{At}v using orthogonal polynomial series expansion over bounded intervals, based on matrix exponential representation in star-algebra of bivariate distributions.


<details>
  <summary>Details</summary>
Motivation: Need efficient method to compute matrix exponential action e^{At}v for all t in bounded intervals, improving upon existing state-of-the-art techniques.

Method: Expand solution into orthogonal polynomial series, using new representation of matrix exponential in star-algebra (bivariate distributions algebra), leading to linear system equivalent to Stein-type matrix equation solvable by direct or Krylov methods.

Result: Numerical experiments show proposed approach achieves accuracy and efficiency comparable to or better than state-of-the-art techniques.

Conclusion: The star-algebra based orthogonal polynomial expansion method provides effective solution for computing matrix exponential action over bounded intervals.

Abstract: We present a new method for computing the action of the matrix exponential on a vector, $e^{At}v$. The proposed approach efficiently evaluates the solution for all $t$ within a prescribed bounded interval by expanding it into an orthogonal polynomial series. This method is derived from a new representation of the matrix exponential in the so-called $\star$-algebra, an algebra of bivariate distributions. The resulting formulation leads to a linear system equivalent to a matrix equation of Stein type, which can be solved by either direct or Krylov subspace methods. Numerical experiments demonstrate the accuracy and efficiency of the proposed approach in comparison with state-of-the-art techniques.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [12] [Inverse problems in electrolysers](https://arxiv.org/abs/2602.16906)
*Giovanni S. Alberti,Wadim Gerner,Matteo Santacesaria*

Main category: math.AP

TL;DR: Paper investigates relationship between electric potential in electrolyser cells and temperature/particle concentrations, identifying necessary measurements for unique potential reconstruction.


<details>
  <summary>Details</summary>
Motivation: Understanding the complex relationship between electric potential, temperature, and particle concentrations in electrolyser cells is crucial for optimizing electrolysis processes and improving cell efficiency and performance.

Method: The authors analyze the mathematical relationship between electric potential (φ) and variables (temperature and concentrations), identifying the specific measurements needed to uniquely reconstruct the potential function from experimental data.

Result: The study determines the necessary measurement requirements for uniquely reconstructing the electric potential as a function of temperature and particle concentrations in electrolyser cells.

Conclusion: By identifying the specific measurements needed, this work provides guidance for experimental design in electrolyser research and enables more accurate modeling of potential-temperature-concentration relationships.

Abstract: We investigate the relationship between the electric potential within an electrolyser cell and its temperature and particle concentrations. Specifically, we identify the measurements required to uniquely reconstruct the potential $φ$ as a function of temperature and concentration.

</details>


### [13] [Self-similar extinction for a fast diffusion equation with weighted absorption](https://arxiv.org/abs/2602.16940)
*Razvan Gabriel Iagar,Diana-Rodica Munteanu*

Main category: math.AP

TL;DR: Finite time extinction proven for bounded solutions to fast diffusion equation with spatially inhomogeneous absorption, contrasting with standard case where solutions remain positive forever.


<details>
  <summary>Details</summary>
Motivation: To study the extinction behavior of solutions to fast diffusion equations with spatially inhomogeneous absorption, where the absorption term depends on spatial position via |x|^σ, and understand how this affects solution behavior compared to the standard case (σ=0).

Method: Mathematical analysis of the PDE ∂_t u = Δu^m - |x|^σ u^p, establishing finite time extinction for bounded solutions under specific parameter conditions, and constructing self-similar solutions of the form U(x,t) = (T-t)^α f(|x|(T-t)^β).

Result: Proves finite time extinction for any bounded solution when p>1, m_c < m < 1, and σ > σ_*. Also establishes existence of self-similar solutions with specific asymptotic behavior, including some unbounded ones.

Conclusion: The spatially inhomogeneous absorption (σ>0) fundamentally changes solution behavior from the standard case (σ=0), causing finite time extinction rather than eternal positivity, with explicit self-similar solutions demonstrating this phenomenon.

Abstract: Finite time extinction of any bounded solution to the fast diffusion equation with spatially inhomogeneous absorption $$ \partial_tu=Δu^m-|x|^σu^p, \quad (x,t)\in\mathbb{R}^N\times(0,\infty), $$ with $N\geq1$ and exponents $$ p>1, \quad m_c=\frac{(N-2)_+}{N}<m<1, \quad σ>σ_*:=\frac{2(p-1)}{1-m}, $$ is established. Moreover, the existence of self-similar solutions of the form $$ U(x,t)=(T-t)^αf(|x|(T-t)^β), \quad α=\frac{σ+2}{(1-m)(σ-σ_*)}, \ β=\frac{p-m}{(1-m)(σ-σ_*)}, $$ with $f(0)>0$, $f'(0)=0$ and $$ \lim\limits_{ξ\to\infty}ξ^{(σ+2)/(p-m)}f(ξ)=L\in(0,\infty). $$ is proved, together with some unbounded self-similar solutions as well. The property of finite time extinction is in striking contrast to the standard fast diffusion equation with absorption (that is, $σ=0$), where the strict positivity of solutions for any $t\in(0,\infty)$ is well-known.

</details>


### [14] [Divergence-form equations admitting nowhere $C^1$ Lipschitz weak solutions](https://arxiv.org/abs/2602.17012)
*Menglan Liao,Baisheng Yan*

Main category: math.AP

TL;DR: The paper shows that divergence-form PDEs can have highly irregular Lipschitz weak solutions that are nowhere differentiable, using convex integration techniques adapted from diffusion equations.


<details>
  <summary>Details</summary>
Motivation: To understand the existence of highly irregular weak solutions for divergence-form partial differential equations, particularly showing that even smooth strongly polyconvex functions can have Euler-Lagrange equations with nowhere differentiable Lipschitz solutions.

Method: Reformulate divergence-form equations as first-order PDE relations, adapt convex integration scheme from previous work on irregular diffusion equations, use structural Condition O_N, and construct new building blocks from wave cones and T_N-configurations.

Result: Prove existence of Lipschitz weak solutions that are nowhere C^1 for time-independent divergence-form equations in bounded domains with suitable boundary data. Specifically, for smooth strongly polyconvex functions on R^{2×n}, the associated Euler-Lagrange equations admit such irregular solutions with zero boundary conditions.

Conclusion: The same structural condition that ensures irregular solutions for diffusion equations also works for divergence-form PDEs, demonstrating that smooth strongly polyconvex variational problems can have highly irregular minimizers.

Abstract: We study a class of partial differential equations in divergence form that admit highly irregular Lipschitz weak solutions. By reformulating these divergence-form equations as a first-order partial differential relation and adapting the convex integration scheme recently developed in \cite{GKY26} for irregular diffusion equations, we show that the same structural Condition~$O_N$ introduced there also ensures the existence of Lipschitz weak solutions that are nowhere $C^1$ for the corresponding time-independent equations in bounded domains, under suitable boundary data. In particular, for the smooth strongly polyconvex functions on $\mathbb{R}^{2\times n}$ constructed in that paper for all $n \ge 2$, the associated Euler--Lagrange equations admit Lipschitz weak solutions that are nowhere $C^1$ and satisfy zero boundary conditions in any bounded domain of $\mathbb{R}^n$. Our approach relies on new building blocks constructed from the same wave cone and $\mathcal{T}_N$-configurations employed in the analysis of diffusion equations.

</details>


### [15] [Asymptotic stability of symmetric flows with viscous inflow boundary condition](https://arxiv.org/abs/2602.17059)
*Yan Guo,Zhuolun Yang*

Main category: math.AP

TL;DR: The paper studies 2D Navier-Stokes equations in a channel with small viscosity, constructs exact steady solutions near shear flows, and develops a weighted vorticity energy method to prove uniform linear stability, enhanced dissipation, and nonlinear stability in different channel length regimes.


<details>
  <summary>Details</summary>
Motivation: To understand the stability properties of shear flows in channels with small viscosity and Navier slip conditions, particularly how perturbations behave near symmetric base profiles that vanish on walls, and to establish rigorous stability results with explicit thresholds.

Method: Construct exact steady solutions close to shear flows using perturbation methods, then develop a new weighted vorticity energy method to analyze stability. The method introduces a weighted L^2 norm and uses Rayleigh vorticity to control unfavorable terms in long channels.

Result: For symmetric base profiles vanishing on walls: 1) Constructed exact steady solutions O(ε^{1/3})-close to shear flows; 2) Proved uniform linear stability and enhanced dissipation with exponential decay on O(ε^{-1/3}) time scale; 3) Established nonlinear asymptotic stability with threshold O(ε^{2/3}) for short channels; 4) For long channels with concavity and spectral condition, proved nonlinear stability with threshold O(ε^{5/6+}).

Conclusion: The weighted vorticity energy method successfully establishes rigorous stability results for 2D Navier-Stokes equations with small viscosity in channels, providing explicit stability thresholds that depend on channel length and base flow properties, with applications to both short and long channel regimes.

Abstract: We study the two-dimensional incompressible Navier-Stokes equations in a channel $Ω=(0,L)\times(0,H)$ with small viscosity $\varepsilon\ll1$, an $\varepsilon$-Navier slip condition on the horizontal walls, and a viscous inflow condition for the perturbation stream function. For a broad class of symmetric base profiles $u_0(y)$ vanishing on the walls, we construct an exact steady solution $(u_s,v_s)$ that is $O(\varepsilon^{1/3})$-close to the shear $(u_0,0)$. We then develop a new weighted vorticity energy method to prove uniform linear stability and enhanced dissipation: perturbations decay exponentially in a weighted $L^2$ norm on the time scale $O(\varepsilon^{-1/3})$. In the short-channel regime $L\ll1$, the method yields nonlinear asymptotic stability with threshold $O(\varepsilon^{2/3})$. In the long-channel regime, assuming concavity together with a spectral condition, we introduce a quantity \textit{Rayleigh vorticity} to control the non-favorable terms and obtain nonlinear stability with threshold $O(\varepsilon^{5/6+})$.

</details>


### [16] [On sliding methods for mixed local and nonlocal equations and Gibbons' conjecture](https://arxiv.org/abs/2602.17069)
*Yinbin Deng,Pengyan Wang,Zhihao Wang,Leyun Wu*

Main category: math.AP

TL;DR: Develops refined sliding method for mixed local-nonlocal operators, establishes new technical tools, proves monotonicity/symmetry results, resolves Gibbons' conjecture for mixed fractional equations.


<details>
  <summary>Details</summary>
Motivation: Address coexistence of operators with different nonlocal structures and incompatible scaling properties that obstruct classical sliding methods for mixed local-nonlocal operators.

Method: Develop refined sliding method for mixed local-nonlocal operators; establish generalized weighted average inequalities, narrow region principles, maximum principles in bounded/unbounded domains.

Result: Derive monotonicity and one-dimensional symmetry results for mixed elliptic equations in various domains; extend analysis to parabolic equations with mixed time derivatives; resolve Gibbons' conjecture for mixed fractional equations.

Conclusion: Successfully develops new analytical framework for mixed local-nonlocal operators, overcoming scaling incompatibility issues and providing powerful tools for studying symmetry and monotonicity properties.

Abstract: We investigate elliptic and parabolic equations involving mixed local and nonlocal operators of the form $(-Δ)^s-Δ$, as well as their parabolic counterparts with both the Marchaud fractional time derivative and the classical first-order derivative.
  A major difficulty in this setting stems from the coexistence of operators with different nonlocal structures and incompatible scaling properties, which obstruct the direct use of classical sliding methods. To address this issue, we develop a refined sliding method suited to mixed local-nonlocal operators. As key technical ingredients, we establish new generalized weighted average inequalities, narrow region principles, and maximum principles in bounded and unbounded domains.
  These tools enable us to derive monotonicity and one-dimensional symmetry results for mixed elliptic equations in bounded domains, half-spaces, and the whole space, and to extend the analysis to parabolic equations with mixed time derivatives. As an application, we resolve the Gibbons' conjecture for a class of mixed fractional equations.

</details>


### [17] [Landau-de Gennes Energy with Weak Planar Anchoring](https://arxiv.org/abs/2602.17233)
*Ho Man Tai,Yong Yu*

Main category: math.AP

TL;DR: Analysis of Landau-de Gennes energy minimizers for nematic liquid crystals in 3D domains with weak planar anchoring, showing singular set is finite and studying boundary singularities.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of nematic liquid crystal configurations in 3D domains with degenerate boundary conditions, particularly the structure of singularities that arise when minimizing the Landau-de Gennes energy in the small elastic constant regime below the nematic-isotropic transition temperature.

Method: Study global minimizers of Landau-de Gennes energy in simply connected 3D domains with weak planar anchoring. Analyze the regime where elastic constant tends to zero and temperature is below transition threshold. Use analysis of associated harmonic maps with strong tangential boundary constraints to study singular sets.

Result: Prove that the singular set of the limiting minimizer within the domain closure is a finite set. Investigate the structure of singularities located on the boundary, showing that minimizers become uniaxial with director fields lying tangent to the boundary in trace sense.

Conclusion: The singularities in nematic liquid crystal configurations under weak planar anchoring are finite in number and their structure, particularly those on boundaries, can be systematically analyzed through harmonic map theory with tangential constraints.

Abstract: We study global minimizers of the Landau-de Gennes energy for the nematic liquid crystals in simply connected, bounded, smooth domains of dimension 3, subject to a weak planar anchoring. The boundary condition is degenerate. In the regime where the elastic constant tends to zero and the temperature is below the nematic-isotropic transition threshold, the bulk and surface energy enforce the energy minimizers to be uniaxial, with director fields lying in the tangent plane to the boundary in the sense of trace. We establish that the singular set of the limiting minimizer within the closure of the domain is a finite set by studying the associated harmonic map with a strong tangential boundary constraint. The structure of the singularities located on the boundary is also investigated.

</details>


### [18] [Derivation of variational membrane models in the context of anisotropic nonlocal hyperelasticity](https://arxiv.org/abs/2602.17278)
*Dominik Engl,Anastasia Molchanova,Hidde Schönberger*

Main category: math.AP

TL;DR: Nonlocal gradient theory for thin structures leads to anisotropic interaction ranges, enabling interpolation between nonlocal and local membrane models via Γ-convergence.


<details>
  <summary>Details</summary>
Motivation: To analyze thin structures and develop a variational dimension reduction framework for hyperelastic energies with nonlocal gradients, addressing the natural anisotropy that arises when rescaling thin domains.

Method: Develop theory for anisotropic nonlocal gradients with ellipsoidal interaction regions (principal radii may vanish independently), creating unified framework interpolating between nonlocal and local models. Apply Γ-convergence analysis to nonlocal thin-film energies.

Result: Limit functional retains structural form of classical membrane energy. Classical local model recovered precisely when all interaction radii vanish. Framework enables interpolation between fully nonlocal, partially nonlocal, and purely local models.

Conclusion: Anisotropic nonlocal gradient theory provides unified framework for dimension reduction of thin structures, connecting nonlocal and local membrane models through Γ-convergence analysis.

Abstract: Motivated by the analysis of thin structures, we study the variational dimension reduction of hyperelastic energies involving nonlocal gradients to an effective membrane model. When rescaling the thin domain, isotropic interaction ranges naturally become anisotropic, leading to the development of a theory for anisotropic nonlocal gradients with direction-dependent interaction ranges. Unlike existing nonlocal derivatives with finite horizon, which are defined via interaction kernels supported on balls of positive radius, our formulation is based on ellipsoidal interaction regions whose principal radii may vanish independently. This yields a unified framework that interpolates between fully nonlocal, partially nonlocal, and purely local models. Employing these tools, we present a $Γ$-convergence analysis for the nonlocal thin-film energies. The limit functional retains the structural form of the classical membrane energy, and the classical local model is recovered precisely when all interaction radii vanish.

</details>


### [19] [Singular convergence for semilinear wave equations with steep potential well](https://arxiv.org/abs/2602.17279)
*Martino Prizzi*

Main category: math.AP

TL;DR: Semilinear wave equation with deep potential well converges to wave equation on well bottom with Dirichlet boundary as well depth → ∞


<details>
  <summary>Details</summary>
Motivation: Study the limiting behavior of semilinear wave equations with deep potential wells, connecting solutions in the whole space to solutions on a restricted domain with boundary conditions

Method: Analyze semilinear wave equation in whole space with potential well, prove convergence as well depth approaches infinity to wave equation defined on bottom of well with Dirichlet boundary condition

Result: Proved convergence of solutions to wave equation on well bottom with Dirichlet boundary condition as potential well depth tends to infinity

Conclusion: Deep potential wells effectively create Dirichlet boundary conditions, connecting whole-space problems to bounded domain problems in the infinite depth limit

Abstract: We consider a semilinear wave equation in the whole space with a deep potential well. We prove that as the depth of the well tends to infinity, the solutions of the equation converge to the solutions of a wave equation defined on the bottom of the well, with Dirichlet condition on the boundary.

</details>


### [20] [Instability of two-pulse periodic waves with long wavelength in some Hamiltonian PDEs](https://arxiv.org/abs/2602.17317)
*Thomas Courant*

Main category: math.AP

TL;DR: Periodic waves with bright and dark soliton components in quasilinear KdV-type equations are spectrally unstable for sufficiently large periods.


<details>
  <summary>Details</summary>
Motivation: To analyze spectral stability of periodic waves in quasilinear generalizations of KdV equations and dispersive perturbations of Euler equations, particularly focusing on waves that exhibit both bright and dark soliton characteristics in the long-wavelength limit.

Method: Two complementary approaches: 1) Calculating asymptotic expansion of Hessian matrix of action integral to determine instability when both limiting solitary waves are stable, 2) Studying convergence of spectrum when period goes to infinity for cases where one solitary wave is unstable, using renormalization of periodic Evans function.

Result: Periodic waves with two pulses (one converging to bright soliton, one to dark soliton) are spectrally unstable for sufficiently large periods.

Conclusion: The combination of two analytical approaches provides a comprehensive framework for establishing spectral instability of periodic waves in quasilinear dispersive systems, covering all cases regardless of the stability properties of the limiting solitary waves.

Abstract: We consider quasilinear generalizations of the Korteweg-de Vries equation and dispersive perturbations of the Euler equations for compressible fluids, either in Lagrangian or in Eulerian coordinates. In particular, our framework includes hydrodynamic formulation of the nonlinear Schrödinger equations. The periodic waves we study exhibit on each period two pulses, one converging to a bright soliton and one converging to a dark soliton, when wavelength goes to infinity. We show that such waves, for sufficiently large periods, are spectrally unstable. To do so, we combine two approaches. The first one is to calculate the asymptotic expansion of the Hessian matrix of the action integral and concludes using arXiv:1505.01382 as in arXiv:1710.03936 . This shows instability when both limiting solitary waves are stable. The second approach studies the convergence of the spectrum when the period goes to infinity and is applied in remaining cases, when one of the solitary waves is unstable. To carry out the latter, we prove the convergence of an appropriate renormalization of the periodic Evans function as in arXiv:1802.02830 .

</details>


### [21] [On the classical Reinforcement problem and Optimisation](https://arxiv.org/abs/2602.17331)
*Emanuele Cristoforoni,Carlo Nitsch,Cristina Trombetti*

Main category: math.AP

TL;DR: Survey paper reviewing classical reinforcement problems for elliptic boundary value problems, focusing on key works by Sanchez-Palencia, Brezis-Caffarelli-Friedman, and Acerbi-Buttazzo, along with related optimization problems.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive survey of the classical reinforcement problem for elliptic boundary value problems, tracing its development from Sanchez-Palencia's 1969 work through seminal contributions by other researchers.

Method: Survey methodology - reviewing and analyzing key papers in the field, focusing on the works of Sanchez-Palencia, Brezis-Caffarelli-Friedman, and Acerbi-Buttazzo, and discussing related optimization problems proposed by Friedman and Buttazzo.

Result: A systematic overview of the classical reinforcement problem for elliptic boundary value problems, highlighting the evolution of the field and connections between different approaches and optimization formulations.

Conclusion: The survey provides a unified perspective on the classical reinforcement problem, connecting foundational works and optimization approaches, offering insights into the historical development and current understanding of this important area in partial differential equations and optimization.

Abstract: In the present survey, we consider the classical reinforcement problem for elliptic boundary value problems originally studied by Sanchez-Palencia in 1969. We focus on the seminar papers by Brezis, Caffarelli, & Friedman, and by Acerbi & Buttazzo, and discuss the related optimisation problems proposed by Friedman and by Buttazzo.

</details>


### [22] [Asymptotic analysis for heterogeneous elastic energies with material voids](https://arxiv.org/abs/2602.17374)
*Stefano Almi,Antonio Flavio Donnarumma,Manuel Friedrich*

Main category: math.AP

TL;DR: The paper studies Γ-convergence of heterogeneous energies in linear elasticity with material voids, showing that the effective limit contains additional surface terms from displacement jumps related to void collapse.


<details>
  <summary>Details</summary>
Motivation: To understand the effective behavior of materials with microscopic voids in geometrically linear elasticity, particularly how void interactions at small scales affect macroscopic material properties.

Method: Using Γ-convergence analysis for functionals with bulk terms depending on symmetrized displacement gradients and surface terms proportional to void surface area, studying the limit as microscale ε → 0.

Result: The Γ-limit admits an integral representation with an additional surface term from displacement jumps outside void regions, related to void collapse. Under continuity of surface density, the jump density equals twice the void energy density.

Conclusion: Microscopic voids in elastic materials lead to macroscopic surface effects from displacement discontinuities, with the effective energy capturing void collapse phenomena through additional jump terms.

Abstract: We study the effective behavior of heterogeneous energies arising in the modeling of material voids in geometrically linear elastic materials. Specifically, we consider functionals featuring bulk terms depending on the symmetrized gradient of the displacement and terms comparable to the surface area of the material voids inside the material. Under suitable growth conditions for the bulk and surface densities we prove that, as the microscale $\varepsilon$ tends to zero, the $Γ$-limit admits an integral representation that contains an additional surface term expressed by jump discontinuities of the displacement outside of the void region. This term is related to the phenomenon of collapsing of voids in the effective limit. Under a continuity assumption of the surface density at the $\varepsilon$-scale, we show that the limiting density related to jumps is twice the energy density for voids.

</details>


### [23] [Anisotropic Maximal $L^p$-regularity Estimates for a Hypoelliptic Operator](https://arxiv.org/abs/2602.17378)
*Kazuhiro Hirao*

Main category: math.AP

TL;DR: The paper proves anisotropic maximal regularity estimates for a specific Vlasov-Fokker-Planck equation involving an Ornstein-Uhlenbeck operator, using fundamental solution pointwise estimates.


<details>
  <summary>Details</summary>
Motivation: To establish maximal regularity properties for Vlasov-Fokker-Planck equations, which are important in kinetic theory and plasma physics, particularly for the specific operator combining Laplacian in y and gradient in x directions.

Method: Proves existence of solutions satisfying anisotropic maximal regularity estimates by establishing similar estimates and weak (1,1) estimates for the operator L = ∂t - A, relying on pointwise estimates of the fundamental solution of L.

Result: Existence of solutions with anisotropic maximal regularity estimates for the Vlasov-Fokker-Planck equation, plus independent results on estimates and weak (1,1) bounds for the operator L.

Conclusion: The paper successfully establishes maximal regularity for the specific Vlasov-Fokker-Planck operator, providing important analytical tools for studying kinetic equations with Ornstein-Uhlenbeck type operators.

Abstract: We consider the maximal regularity of a specific Vlasov-Fokker-Planck equation $\mathcal{A}u=f$ in the Euclidean space. The operator $\mathcal{A}=Δ_{y}u-y\cdot \nabla_x{u}$ is an example of the Ornstein-Uhlenbeck operators. We prove the existence of a solution that satisfies the anisotropic maximal regularity estimates. To prove this we also show a similar estimates and a weak (1, 1) estimate for $L=\partial_t-\mathcal{A}$, which is of independent interest. These results rely on the pointwise estimates of the fundamental solution of $L$.

</details>


### [24] [Wave front set of solutions to the fractional Schrödinger equation](https://arxiv.org/abs/2602.17406)
*Takumi Kanai,Ryo Muramatsu,Yuusuke Sugiyama*

Main category: math.AP

TL;DR: Characterizes wave front sets of solutions to fractional Schrödinger equations using wave packet transform, relating fractional order θ to potential growth rate in singularity propagation.


<details>
  <summary>Details</summary>
Motivation: To understand how singularities propagate in fractional Schrödinger equations and clarify the relationship between the fractional order θ and potential growth rate, bridging propagation mechanisms between Schrödinger and wave equations.

Method: Uses wave packet transform (short-time Fourier transform) to analyze wave front sets of solutions to fractional Schrödinger equations with 0<θ<2 and potential V(x).

Result: Presents a theorem that characterizes wave front sets and bridges propagation mechanisms of singularities between Schrödinger and wave equations, clarifying the relationship between fractional order θ and potential growth rate.

Conclusion: The wave packet transform provides effective characterization of singularity propagation in fractional Schrödinger equations, revealing how fractional order θ influences propagation behavior relative to potential growth.

Abstract: In this paper, we characterize the wave front sets of solutions to fractional Schrödinger equations \(i\partial_{t}u =(-Δ)^{θ/2}u + V(x)u\)
  with $0<θ<2$ via the wave packet transform (short-time Fourier transform). We clarify the relationship between the order \(θ\) of the fractional Laplacian and the growth rate of the potential in the problem of propagation of singularities. In particular, we present a theorem that bridges the propagation mechanisms of singularities for the Schrödinger and wave equations.

</details>


### [25] [Isoperimetric inequalities for the lowest magnetic Steklov eigenvalue](https://arxiv.org/abs/2602.17416)
*Ayman Kachmar,Vladimir Lotoreichik*

Main category: math.AP

TL;DR: The disk maximizes the lowest magnetic Steklov eigenvalue among simply-connected smooth domains of given area for moderate magnetic field strengths.


<details>
  <summary>Details</summary>
Motivation: To establish isoperimetric inequalities for the lowest eigenvalue of magnetic Steklov problems on planar domains, determining optimal domain shapes under area or perimeter constraints.

Method: Uses torsion-type trial functions for bounded domains, and trial functions dependent only on distance to the boundary for exterior domains. Combines variational methods with geometric analysis.

Result: For bounded domains: disk maximizes lowest magnetic Steklov eigenvalue among simply-connected smooth domains of given area for moderate magnetic fields. For exterior domains: similar isoperimetric inequality under perimeter constraint with additional geometric/symmetry assumptions.

Conclusion: The disk is the optimal shape for maximizing the lowest magnetic Steklov eigenvalue under area constraints, extending classical isoperimetric results to magnetic spectral problems with moderate field strengths.

Abstract: This paper studies the optimization of the lowest eigenvalue of the magnetic Steklov problem on planar domains. In the bounded domain setting and for magnetic fields of moderate strengths, we prove that among all simply-connected smooth domains of given area, the disk maximises the lowest magnetic Steklov eigenvalue. For exterior domains, we establish a similar isoperimetric inequality for magnetic fields of moderate strength under fixed perimeter constraint and additional geometric and symmetry assumptions. The proofs rely on the method of torsion-type trial functions in the bounded domain case and on the method of trial functions dependent only on the distance to the boundary in the exterior domain case.

</details>


### [26] [Construction of two-bubble blow-up solutions for the mass-critical gKdV equations](https://arxiv.org/abs/2602.17457)
*Yang Lan,Xu Yuan*

Main category: math.AP

TL;DR: Existence of global solution for mass-critical gKdV equation that blows up in infinite time as sum of two decoupled bubbles with opposite signs.


<details>
  <summary>Details</summary>
Motivation: Study blow-up behavior for mass-critical generalized Korteweg-de Vries equation, specifically understanding how solutions can blow up in infinite time as sum of interacting solitons with opposite signs.

Method: Inspired by techniques from Martel-Raphaël for 2D mass-critical NLS. Uses refined approximate solutions with non-localized profiles to handle unstable scaling directions excited by nonlinear interactions. Requires sharp understanding of soliton-profile interactions.

Result: Proves existence of global solution that blows up in infinite time and approaches sum of two decoupled bubbles with opposite signs for mass-critical gKdV equation.

Conclusion: Demonstrates complex blow-up dynamics in mass-critical gKdV where infinite-time blow-up occurs as superposition of two interacting solitons with opposite signs, overcoming challenges from excited scaling instabilities.

Abstract: For the mass-critical generalized Korteweg-de Vries equation, $$ \partial_{t}u+\partial_{x}\left( \partial_{x}^{2}u+u^{5}\right)=0,\quad (t,x)\in [0,\infty)\times \mathbb{R}.$$ We prove the existence of a global solution that blows up in infinite time and approaches the sum of two decoupled bubbles with opposite signs. The proof is inspired by the techniques developed for the two-dimensional mass-critical NLS equation in a similar context by Martel-Raphaël [37]. The main difficulty originates from the fact that the unstable directions related to scaling are excited by the nonlinear interactions. To overcome this difficulty, a refined approximate solution that involves some non-localized profiles is needed. In particular, a sharp understanding for the interactions between solitons and such profiles is also required.

</details>


### [27] [A comment on an $L^\frac{2n}{n+2}-L^\frac{2n}{n-2}$ Carleman inequality in relation to "the determination of an unbounded potential from Cauchy data"](https://arxiv.org/abs/2602.17523)
*Mourad Choulli,Hiroshi Takase*

Main category: math.AP

TL;DR: The authors identify and correct an error in Proposition 2.1 from two previous papers, providing a new proof with additional hypothesis requirements.


<details>
  <summary>Details</summary>
Motivation: To address a partially incorrect proof in Proposition 2.1 from DKS (arXiv:1104.0232) and correct the same error that was repeated in Ch (arXiv:2310.17456).

Method: Provide a new proof for Proposition 2.1 that requires an additional hypothesis, and modify this proof to also correct the same error in the second paper.

Result: Successfully corrects the flawed proofs in both papers by providing a new, valid proof with necessary additional conditions.

Conclusion: The original proofs were incorrect, but can be fixed with additional hypotheses, providing corrected versions for both papers.

Abstract: The proof of \cite[Proposition 2.1]{DKS}[arXiv:1104.0232] is partially incorrect. In this short note, we provide a new proof, which requires an additional hypothesis. A modification of this new proof also corrects the proof of \cite[Proposition 2.1]{Ch}[arXiv:2310.17456], where the incorrect argument of \cite{DKS} has been repeated.

</details>


### [28] [On putative self-similarity for incompressible 3D Euler](https://arxiv.org/abs/2602.17570)
*Peter Constantin,Mihaela Ignatova,Vlad Vicol*

Main category: math.AP

TL;DR: The paper establishes lower bounds on the similarity exponent γ for hypothetical self-similar finite-time blowup solutions of the 3D Euler equations, showing γ must be > 2/5 for finite kinetic energy, and ≥ 1/2 under additional conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the possible scaling behavior of hypothetical finite-time singularities in the 3D Euler equations, which remains an open problem in fluid dynamics. By establishing lower bounds on the similarity exponent, the authors constrain the possible blowup scenarios.

Method: The authors use mathematical analysis of self-similar solutions to the 3D Euler equations. They prove bounds on the similarity exponent γ through rigorous mathematical arguments, considering different scenarios: general solutions with finite kinetic energy, smooth globally self-similar profiles with outgoing property, and axisymmetric solutions.

Result: 1) For initial data with finite kinetic energy, γ > 2/5. 2) For smooth globally self-similar blowup profiles with outgoing property, γ ≥ 1/2. 3) For axisymmetric solutions, γ ≥ 1/2 even without the outgoing property.

Conclusion: The paper provides rigorous constraints on possible self-similar blowup scenarios for the 3D Euler equations, showing that the similarity exponent must be at least 1/2 in many cases, which limits how fast hypothetical singularities could develop.

Abstract: We consider hypothetical solutions of 3D Euler which blow up in finite time in a self-similar fashion. We prove that if the initial data has finite kinetic energy, then the similarity exponent $γ$ which governs the rate of zooming in must be larger than $2/5$. If a smooth globally self-similar blowup profile exists, and this profile satisfies an outgoing property, we prove that $γ\geq 1/2$. For axisymmetric solutions, we establish the bound $γ\geq 1/2$ in more general settings, including ones in which the outgoing property is not present.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [29] [Distillation and Interpretability of Ensemble Forecasts of ENSO Phase using Entropic Learning](https://arxiv.org/abs/2602.16857)
*Michael Groom,Davide Bassetti,Illia Horenko,Terence J. O'Kane*

Main category: physics.comp-ph

TL;DR: A distillation framework compresses ensemble eSPA models for ENSO prediction into interpretable distilled models that preserve forecast skill while enabling diagnostic analysis of ENSO dynamics and predictability.


<details>
  <summary>Details</summary>
Motivation: While eSPA ensemble models achieve state-of-the-art ENSO forecast skill, they are difficult to interpret compared to individual models. There's a need to compress ensembles into more interpretable forms while preserving performance and enabling diagnostic analysis of ENSO predictability.

Method: Distillation framework aggregates only correctly-predicting ensemble members into compact "distilled" models. Uses feature importance vectors, spatial importance maps, and analyzes regime persistence and cross-lead clustering consistency to understand ENSO dynamics.

Result: Distilled models preserve forecast performance while enabling diagnostics impractical on full ensembles. The discretized system captures ENSO spatiotemporal dynamics, identifies predictability barriers, reveals physical precursors, and traces event evolution from precursors to mature states.

Conclusion: The distillation framework enables rigorous investigation of long-range ENSO predictability, complementing operational forecasts by providing interpretable models that maintain skill while revealing physical mechanisms and predictability patterns.

Abstract: This paper introduces a distillation framework for an ensemble of entropy-optimal Sparse Probabilistic Approximation (eSPA) models, trained exclusively on satellite-era observational and reanalysis data to predict ENSO phase up to 24 months in advance. While eSPA ensembles yield state-of-the-art forecast skill, they are harder to interpret than individual eSPA models. We show how to compress the ensemble into a compact set of "distilled" models by aggregating the structure of only those ensemble members that make correct predictions. This process yields a single, diagnostically tractable model for each forecast lead time that preserves forecast performance while also enabling diagnostics that are impractical to implement on the full ensemble.
  An analysis of the regime persistence of the distilled model "superclusters", as well as cross-lead clustering consistency, shows that the discretised system accurately captures the spatiotemporal dynamics of ENSO. By considering the effective dimension of the feature importance vectors, the complexity of the input space required for correct ENSO phase prediction is shown to peak when forecasts must cross the boreal spring predictability barrier. Spatial importance maps derived from the feature importance vectors are introduced to identify where predictive information resides in each field and are shown to include known physical precursors at certain lead times. Case studies of key events are also presented, showing how fields reconstructed from distilled model centroids trace the evolution from extratropical and inter-basin precursors to the mature ENSO state. Overall, the distillation framework enables a rigorous investigation of long-range ENSO predictability that complements real-time data-driven operational forecasts.

</details>


### [30] [Machine Learning Hamiltonians are Accurate Energy-Force Predictors](https://arxiv.org/abs/2602.16897)
*Seongsu Kim,Chanhui Lee,Yoonho Kim,Seongjun Yun,Honghui Kim,Nayoung Kim,Changyoung Park,Sehui Han,Sungbin Lim,Sungsoo Ahn*

Main category: physics.comp-ph

TL;DR: QHFlow2 is a new SO(2)-equivariant Hamiltonian model that achieves state-of-the-art performance in predicting energies and forces directly from predicted Hamiltonians, with significant improvements over previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning Hamiltonian (MLH) models are mainly evaluated by reconstruction metrics rather than their actual performance as energy-force predictors, creating a gap in understanding their practical utility for molecular dynamics and quantum chemistry applications.

Method: QHFlow2 features an SO(2)-equivariant backbone with a two-stage edge update mechanism. The authors introduce a benchmark framework that computes energies and forces directly from predicted Hamiltonians to properly evaluate Hamiltonian models.

Result: QHFlow2 achieves 40% lower Hamiltonian error than previous best models with fewer parameters. It reaches NequIP-level force accuracy on MD17/rMD17 with up to 20× lower energy MAE. On QH9, it reduces energy error by up to 20× compared to MACE. The model shows consistent scaling behavior with model capacity and data.

Conclusion: QHFlow2 represents a significant advancement in Hamiltonian modeling, demonstrating that improvements in Hamiltonian accuracy directly translate to more accurate energy and force computations, establishing a new state-of-the-art for MLH models.

Abstract: Recently, machine learning Hamiltonian (MLH) models have gained traction as fast approximations of electronic structures such as orbitals and electron densities, while also enabling direct evaluation of energies and forces from their predictions. However, despite their physical grounding, existing Hamiltonian models are evaluated mainly by reconstruction metrics, leaving it unclear how well they perform as energy-force predictors. We address this gap with a benchmark that computes energies and forces directly from predicted Hamiltonians. Within this framework, we propose QHFlow2, a state-of-the-art Hamiltonian model with an SO(2)-equivariant backbone and a two-stage edge update. QHFlow2 achieves $40\%$ lower Hamiltonian error than the previous best model with fewer parameters. Under direct evaluation on MD17/rMD17, it is the first Hamiltonian model to reach NequIP-level force accuracy while achieving up to $20\times$ lower energy MAE. On QH9, QHFlow2 reduces energy error by up to $20\times$ compared to MACE. Finally, we demonstrate that QHFlow2 exhibits consistent scaling behavior with respect to model capacity and data, and that improvements in Hamiltonian accuracy effectively translate into more accurate energy and force computations.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [31] [Operational measurement of relativistic equilibrium from stochastic fields alone](https://arxiv.org/abs/2602.16765)
*Ira Wolfson*

Main category: physics.plasm-ph

TL;DR: A method to reconstruct the relativistic temperature four-vector β^μ from electromagnetic fluctuations emitted by drifting media without external probes or calibration.


<details>
  <summary>Details</summary>
Motivation: Relativistic equilibrium is described by the inverse-temperature four-vector β^μ rather than frame-dependent scalar temperature, but existing methods require external probes, spectral lines, or absolute calibration.

Method: Analyze electromagnetic fluctuations from drifting media using Lorentz boost effects that convert isotropic rest-frame noise into correlated electric/magnetic fields. Use gain-independent fluctuation observables to extract drift velocity and angle-resolved noise spectra governed by covariant fluctuation-dissipation theorem.

Result: Monte Carlo analysis shows percent-level accuracy at realistic signal-to-noise ratios, with sub-microsecond integration times feasible for laboratory plasmas. First method to reconstruct covariant thermal state β^μ from passive stochastic fields alone.

Conclusion: Establishes vacuum electromagnetic fluctuations as a direct operational probe of relativistic equilibrium without requiring absolute calibration, spectral lines, or external probes.

Abstract: Relativistic equilibrium is described by the inverse-temperature four-vector $β^μ= u^μ/(k_B T_0)$ rather than by a frame-dependent scalar temperature. We show that $β^μ$ can be reconstructed directly from electromagnetic fluctuations emitted by a drifting medium, without external probes, spectral lines, or absolute intensity calibration. A Lorentz boost converts isotropic rest-frame noise into correlated electric and magnetic fields, producing a gain-independent fluctuation observable that yields the drift velocity purely from stochastic data. Combined with angle-resolved noise spectra governed by the covariant fluctuation--dissipation theorem, this enables full reconstruction of $β^μ$ using electromagnetic measurements alone. Monte Carlo analysis demonstrates percent-level accuracy at realistic signal-to-noise ratios, and feasibility estimates indicate sub-microsecond integration times for laboratory plasmas. To our knowledge, this constitutes the first method that reconstructs the covariant thermal state $β^μ$ of a relativistic medium from passive stochastic fields alone, without absolute calibration, spectral lines, or external probes. These results establish vacuum electromagnetic fluctuations as a direct operational probe of relativistic equilibrium.

</details>


### [32] [Enhanced Hot Electron Preheat Observed in Magnetized Laser Direct-Drive Implosions](https://arxiv.org/abs/2602.16884)
*M. Cufari,M. Gatu Johnson,C. K. Li,J. A. Frenje,P. W. Moloney,A. J. Crilly,P. V. Heuer,J. R. Davies*

Main category: physics.plasm-ph

TL;DR: Magnetic fields enhance hard x-ray emission in direct-drive implosions by confining hot electrons that would otherwise escape, leading to increased preheat but reduced fusion product energy.


<details>
  <summary>Details</summary>
Motivation: To understand how applied magnetic fields affect hot electron preheat and laser-plasma instabilities in direct-drive implosions, and their impact on fusion gain and implosion efficiency.

Method: Applied 10 T magnetic field to direct-drive implosions, observed hard x-ray emission enhancement, analyzed magnetic field alignment with ablation flow, and studied hot electron confinement in mirror-mode magnetic fields.

Result: Magnetic field enhanced hard x-ray emission by factor 1.5±0.1, confined hot electrons that would otherwise escape, reduced energy of charged fusion products, and intensified need to mitigate laser-plasma instabilities.

Conclusion: Magnetic fields significantly alter hot electron behavior in implosions, increasing preheat but decreasing fusion efficiency, highlighting the critical need to control laser-plasma instabilities for optimal magnetized implosion performance.

Abstract: Hard x-ray emission, associated with hot electron preheat, in direct-drive implosions was observed to be enhanced by a factor of $1.5\pm0.1$ by application of a $10$ T magnetic field. The applied magnetic field reaches a quasi steady-state aligned with the ablation flow prior to the onset of laser-plasma instabilities in the corona. Hot electrons that would otherwise escape the corona and lead to capsule charging in unmagnetized implosions are confined in a mirror-mode of the magnetic field in magnetized implosions. These hot electrons are shown to subsequently pitch-angle scatter from the mirror onto the capsule, thereby leading to the observed hard x-ray generation in magnetized implosions. Consequently, the energy of charged-fusion products, associated with the capsule charging, are observed to decrease when the implosion is magnetized. These results intensify the need to mitigate laser-plasma instabilities -- particularly for magnetized implosions -- to maximize fusion gain and implosion efficiency.

</details>


### [33] [Numerical study of electron acceleration by microwave-driven plasma wakefields in rectangular waveguides](https://arxiv.org/abs/2602.16896)
*Jesús E. López,Eduardo A. Orozco-Ospino*

Main category: physics.plasm-ph

TL;DR: Microwave-driven plasma wakefield acceleration in rectangular waveguides achieves ~100 keV energy gain over meter-scale lengths with quasi-monoenergetic beams when electrons are pre-accelerated to near the microwave pulse group velocity.


<details>
  <summary>Details</summary>
Motivation: Plasma-based acceleration offers compact alternatives to conventional accelerators due to large plasma electric fields. While microwave-driven wakefields in plasma-filled waveguides have been demonstrated, the conditions for efficient electron acceleration remain poorly characterized.

Method: 3D particle-in-cell simulations of electron acceleration by microwave-driven plasma wakefields in rectangular waveguides filled with low-density plasma. Both reduced and fully self-consistent numerical models were used to analyze electron injection dynamics and energy gain.

Result: Electron acceleration strongly depends on injection phase and initial velocity. Optimal acceleration occurs when electrons are pre-accelerated to velocities close to the microwave pulse group velocity. Achieved ~100 keV energy gains over meter-scale lengths with quasi-monoenergetic distributions under suitable injection conditions. Transverse dynamics and space-charge effects impose additional constraints on acceleration efficiency.

Conclusion: This work provides quantitative assessment of acceleration in microwave-driven plasma wakefield schemes, supporting their evaluation as viable platforms for compact plasma-based accelerators. Results characterize key dependencies and constraints for efficient electron acceleration.

Abstract: Plasma-based acceleration schemes have attracted sustained interest as a pathway toward compact particle accelerators, owing to the large electric fields supported by plasmas. Although recent studies have demonstrated the excitation of plasma wakefields using high-power microwave pulses in plasma-filled waveguides, the conditions required for efficient electron acceleration in such configurations remain insufficiently characterized. In this work, we investigate the acceleration of externally injected electrons by microwave-driven plasma wakefields in rectangular waveguides filled with low-density plasma. Three-dimensional particle-in-cell simulations are employed to analyze the dynamics of electron injection and energy gain under both reduced and fully self-consistent numerical models. The results show that electron acceleration is strongly dependent on the injection phase and initial velocity. Optimal acceleration is achieved when electrons are pre-accelerated to velocities close to the group velocity of the driving microwave pulse. For the parameters considered, energy gains of the order of $10^2 \mathrm{keV}$ are obtained over interaction lengths of the order of meters, while maintaining a quasi-monoenergetic energy distribution under suitable injection conditions. The influence of transverse dynamics and space-charge effects is also examined, revealing additional constraints on acceleration efficiency associated with the transverse electromagnetic field of the driving microwave pulse. These results provide a quantitative assessment of the acceleration stage in microwave-driven plasma wakefield schemes and support their evaluation as a viable platform for compact plasma-based accelerators.

</details>


### [34] [Capturing Secondary Kinetic Instabilities in Three-Dimensional Dayside Reconnection Using an Improved Gradient-Based Closure](https://arxiv.org/abs/2602.16960)
*Kolter Bradshaw,Ammar Hakim,James Juno,Joshua Pawlak,Jason TenBarge,Amitava Bhattacharjee*

Main category: physics.plasm-ph

TL;DR: The paper improves magnetic reconnection simulations by implementing a gradient-based heat flux closure in the ten-moment fluid model, enabling better replication of kinetic instabilities and turbulence observed in kinetic simulations.


<details>
  <summary>Details</summary>
Motivation: Previous ten-moment fluid models using local relaxation closures failed to accurately capture transverse current sheet instabilities and the resulting turbulence/mixing seen in kinetic simulations of magnetic reconnection.

Method: Used the Gkeyll software framework to simulate asymmetric reconnection based on the 2015 MMS crossing (Burch event), implementing an improved gradient-based heat flux closure in the ten-moment fluid model.

Result: The gradient-based closure significantly improved the simulation of secondary kinetic instabilities in the current sheet, which generated turbulence leading to growth of secondary magnetic islands and flux ropes.

Conclusion: The improved heat flux closure enables the ten-moment fluid model to better replicate kinetic physics of magnetic reconnection, particularly the current sheet instabilities and turbulence observed in kinetic simulations.

Abstract: Magnetic reconnection is a highly dynamic process that excites a wide variety of kinetic waves and instabilities. Transverse current sheet instabilities such as the lower-hybrid drift and secondary drift-kink instabilities in particular have been shown by kinetic simulations to modify the reconnection and introduce significant turbulence and mixing to the reconnection layer. Past studies using the ten-moment fluid model to capture important kinetic physics such as the electron inertia and full representation of the pressure tensor proved advantageous to a two-fluid representation of reconnection, but the model struggled when using a local relaxation closure for the heat flux to replicate the current sheet instabilities and subsequent mixing seen in kinetic simulations. This work uses the \texttt{Gkeyll} software framework to perform simulations of asymmetric reconnection based on the 16 October 2015 MMS crossing of a diffusion region, the Burch event. An improved gradient-based heat flux closure is implemented, showing significant improvement in secondary kinetic instabilities that grow in the current sheet. These instabilities generate turbulence which leads to growth of secondary magnetic islands and flux ropes.

</details>


### [35] [Resistive instabilities of current sheets in stratified plasmas with a gravitational field](https://arxiv.org/abs/2602.17400)
*Faisal Sayed,Anna Tenerani,Richard Fitzpatrick*

Main category: physics.plasm-ph

TL;DR: Gravity and density stratification significantly alter tearing mode instability in magnetic reconnection, suppressing reconnection in favorable stratification but strongly destabilizing it in unfavorable stratification, with rapid reconnecting modes emerging.


<details>
  <summary>Details</summary>
Motivation: To understand how gravity and density stratification affect magnetic reconnection via tearing instability in astrophysical and fusion plasma environments where heavy-over-light plasma configurations exist in gravitational fields with current sheets.

Method: Linear stability analysis of a slab current sheet with density gradient under constant gravitational acceleration, studying reconnecting modes in both favorable and unfavorable stratification scenarios.

Result: Favorable stratification suppresses reconnection while unfavorable stratification strongly destabilizes tearing mode; classical constant-ψ regime disappears for strong magnetic Reynolds numbers (S>>1) with unfavorable stratification, transitioning to gravity-driven G-mode with S^-1/3 growth rate scaling.

Conclusion: Gravity and stratification fundamentally modify tearing instability properties, with unfavorable stratification enabling only rapidly reconnecting modes through gravity-modified tearing that transitions to G-mode, explaining enhanced reconnection in astrophysical environments with density gradients.

Abstract: Magnetic reconnection can develop spontaneously via the tearing instability, often invoked to explain disruptive instabilities in fusion devices, solar flares, the generation of periodic density disturbances at the tip of helmet streamers, and flux transfer events at the Earth's dayside magnetopause. However, in many such environments the presence of gravity, magnetic field curvature or other forms of acceleration often result in situations of a heavy-over-light plasma in an effective gravitational field with an embedded current sheet. This paper studies the linear stability of a slab current sheet with respect to reconnecting modes in the presence of a density gradient under the effect of a constant gravitational acceleration. We show that the presence of stratification and gravity modify the properties of the tearing mode instability both in the case of favorable and unfavorable stratification. Favorable stratification suppresses reconnection while unfavorable stratification strongly destabilizes the tearing mode. Furthermore, we show that the classical constant-ψ regime effectively does not exist, even for weak unfavorable stratification, for S>>1. Instead, the gravity-modified tearing progressively transitions into the G-mode, which is a gravity-driven reconnecting mode with a growth rate scaling as S^-1/3. As a consequence, unfavorable stratification only permits rapidly reconnecting modes.

</details>


### [36] [Plasma Mixing in Collisionless Kelvin-Helmholtz Dynamics](https://arxiv.org/abs/2602.17404)
*Silvia Ferro,Fabio Bacchini,Giuseppe Arrò,Francesco Pucci,Pierre Henri*

Main category: physics.plasm-ph

TL;DR: The paper investigates plasma mixing driven by the nonlinear Kelvin-Helmholtz instability using high-resolution PIC simulations, finding localized mixing mainly in interface regions with ions mixing more effectively than electrons.


<details>
  <summary>Details</summary>
Motivation: To understand the efficiency and physical mechanisms of plasma mixing driven by the nonlinear evolution of the Kelvin-Helmholtz instability at the magnetosphere-magnetosheath boundary, which influences transport and particle acceleration.

Method: High-resolution two-dimensional Particle-In-Cell (PIC) simulations using a finite-Larmor-radius shear-flow initial configuration, with plasma mixing quantified using particle tracking, passive tracers, and magnetic reconnection diagnostics.

Result: Plasma mixing is present but localized, occurring mainly in narrow interface regions and plasma structures. Ions mix more effectively than electrons, which remain largely frozen to field lines. Enhanced mixing correlates with localized reconnection within and between KH vortices.

Conclusion: Cross-boundary transport driven by the kinetic KHI is highly localized and mediated by vortex advection and reconnection. Electron mixing is strongly constrained, providing an upper bound on kinetic-scale transport across collisionless shear layers.

Abstract: Simulations and observations of the low-latitude magnetosphere-magnetosheath boundary layer indicate that the Kelvin-Helmholtz instability (KHI) drives vortex structures that enhance plasma mixing and magnetic reconnection, influencing transport and particle acceleration. We investigate the efficiency and physical mechanisms of plasma mixing driven by the nonlinear evolution of the KHI. We perform high-resolution two-dimensional Particle-In-Cell (PIC) simulations using a finite-Larmor-radius shear-flow initial configuration. Plasma mixing is quantified using particle tracking, passive tracers, and diagnostics of magnetic reconnection. Mixing across the shear layer is present but localized, occurring mainly in narrow interface regions and plasma structures. Ions mix more effectively than electrons, which remain largely frozen to field lines. Enhanced mixing correlates with localized reconnection within and between KH vortices. Cross-boundary transport driven by the kinetic KHI is highly localized and mediated by vortex advection and reconnection. Electron mixing is strongly constrained, providing an upper bound on kinetic-scale transport across collisionless shear layers.

</details>


### [37] [Self-Consistent Dynamics of Electron Radiation Reaction via Structure-Preserving Geometric Algorithms for Coupled Schrödinger-Maxwell Systems](https://arxiv.org/abs/2602.17429)
*Jacob Matthew Molina,Hong Qin*

Main category: physics.plasm-ph

TL;DR: Structure-preserving algorithms for Schrödinger-Maxwell system enable first simulations of radiation reaction effects on electron coherent states in magnetic fields, showing rapid decoherence and Landau level renormalization.


<details>
  <summary>Details</summary>
Motivation: Classical radiation reaction equations fail at atomic scales where radiation destroys electron coherence, but the nonlinear Schrödinger-Maxwell system has been too complex for analytical study.

Method: Developed geometric structure-preserving algorithms (SPHINX code) that preserve gauge invariance, symplecticity, and unitarity on discrete space-time lattice for the coupled Schrödinger-Maxwell system.

Result: Electron coherent states in magnetic fields radiate strongly, rapidly lose coherence, and disperse into decoherent wave packets; Landau levels renormalize into stationary dressed eigenstates with constant energies.

Conclusion: Opens new computational window into radiation reaction physics for extreme-field phenomena in fusion plasmas, astrophysics, and next-generation laser experiments.

Abstract: Classically, a charged particle in a magnetic field emits radiation, losing momentum and experiencing the Abraham-Lorentz (AL) / Landau-Lifshitz (LL) radiation reaction (RR) force. However, at atomic scales and outside the range of their applicability, the AL/LL equations fail and RR destroys the coherent state of an electron-undermining the very concept of a RR force. This process can be described by the coupled Schrödinger-Maxwell (SM) system under appropriate limits, but the system's nonlinear complexity has long limited purely analytical studies. We present geometric structure-preserving algorithms for the SM system that preserve gauge invariance, symplecticity, and unitarity on the discrete space-time lattice, which are implemented in our Structure-Preserving scHrodINger maXwell (SPHINX) code. By constructing coherent states from the Landau levels, SPHINX simulates the fully-coupled nonlinear dynamics of an electron coherent state, the energy partition evolution, and decoherence/relaxation of the electron wave packet in time due to RR. These simulations indicate that, in an external magnetic field, an electron prepared in an atomic-scale coherent state can radiate strongly, rapidly losing coherence and dispersing into a decoherent wave packet. Additionally, we also present the fully-coupled nonlinear evolution of the non-degenerate ground- and first-excited Landau levels themselves to understand how the coupled SM system modifies the well-known ideal (i.e., Schrödinger-only) dynamics of the Landau Levels. With appropriate boundary conditions, simulations show that the Landau levels are renormalized into stationary dressed eigenstates with constant electromagnetic and kinetic energies. This opens a new computational window into RR physics and advances modeling of extreme-field phenomena in fusion plasmas, astrophysics, and next-generation laser experiments

</details>


### [38] [Tolerances to driver-witness misalignment in a quasilinear plasma wakefield accelerator](https://arxiv.org/abs/2602.17468)
*T. C. Wilson,J. Farmer,K. Lotov,A. Pukhov*

Main category: physics.plasm-ph

TL;DR: Study of transverse witness dynamics in quasilinear plasma wakefield acceleration, developing analytical models and metrics for emittance preservation with application to AWAKE experiment.


<details>
  <summary>Details</summary>
Motivation: Plasma-based accelerators offer high gradients and scalability for future colliders. Proton-driven accelerators can achieve high energy in single stages, but emittance preservation is critical for beam quality. Understanding transverse witness dynamics in quasilinear regime is essential for setting alignment constraints.

Method: Developed analytical models to describe witness motion in quasilinear regime where plasma wake is partially evacuated. Created a metric to estimate emittance preservation based on single parameter estimating witness density after phase mixing. Validated models using particle-in-cell simulations with AWAKE Run 2c baseline parameters.

Result: Excellent agreement between predictive models and particle-in-cell simulations. Models successfully describe witness dynamics and emittance preservation in quasilinear regime, particularly for misaligned driver-witness configurations.

Conclusion: The developed models and metrics enable setting alignment constraints for AWAKE experiment and other wakefield acceleration schemes operating in quasilinear regime, advancing practical implementation of plasma-based accelerators.

Abstract: Plasma-based accelerators offer high accelerating gradients and scalability through staging or long plasma sources, which makes them good candidates for future accelerator and collider concepts. Proton-driven accelerators in particular have the potential to bring particles to high energy in a single stage. In the quasilinear regime - where the plasma wake is only partially evacuated - a witness bunch of electrons drives a cavitated wake, which acts to preserve the emittance of the portion of the witness inside this self-blowout. In the case of a misalignment between the driver and witness, this behaviour can persist, but its effectiveness is reduced. In this paper, we study transverse witness dynamics in this regime, and develop analytical models to describe the witness motion, and develop a metric to estimate emittance preservation based on a single parameter which estimates the density of the witness after phase mixing. Particle in cell simulations using the AWAKE Run 2c baseline parameters show excellent agreement with the predictive models developed. This work allows alignment constraints to be set both for the AWAKE experiment and other wakefield acceleration schemes operating in the quasilinear regime.

</details>


### [39] [A Kinetic Route to Helicity-Constrained Decay](https://arxiv.org/abs/2602.17514)
*Dion Li*

Main category: physics.plasm-ph

TL;DR: PIC simulations show intermittent E·B≠0 regions reduce magnetic helicity, leading to a new helicity density definition that enables time-invariant plateau behavior in correlation integrals, consistent with 2D decay constraints.


<details>
  <summary>Details</summary>
Motivation: To understand the statistical association between intermittent E·B≠0 regions and reductions in magnetic helicity magnitude during early electron-scale turbulence, and to develop a better helicity measure that captures local conservation properties.

Method: 2D3V PIC simulations of freely decaying sub-ion turbulence, analysis of magnetic helicity behavior, proposal of a source-compensated history-dependent helicity density that satisfies exact local balance, and examination of Saffman-type two-point correlation integrals.

Result: The new helicity density enables intermediate-scale plateaus in correlation integrals that remain approximately invariant even as the usual Saffman helicity integral evolves. Simulations show rapid development of mixed-signed helicity patches and decreasing global fractional helicity, with decay consistent with cancellation-dominated scaling constraints.

Conclusion: The proposed helicity density provides a better measure for analyzing turbulence decay, revealing plateau behavior consistent with 2D decay constraints and showing that cancellation effects dominate the kinetic decay phase in initially net-helical configurations.

Abstract: Through 2D3V PIC simulations of freely decaying sub-ion turbulence, intermittent localized regions with $\mathbf{E} \cdot \mathbf{B} \neq 0$ are found to be statistically associated with reductions in the magnitude of magnetic helicity while evolving in the early electron-scale interaction phase. Motivated by this behavior, we propose a source-compensated, history-dependent helicity density that satisfies an exact local balance identity by construction, enabling Saffman-type two-point correlation integrals which, under standard flux-decorrelation assumptions, can exhibit intermediate-scale plateaus that are roughly time-independent. In our simulations we demonstrate such plateaus to remain approximately invariant even as the usual Saffman helicity integral plateau value $I_H$ evolves during the early kinetic stage. Under approximate single-scale self-similarity, the plateau behavior of the magnetic integral is consistent with the 2D decay constraint $BL \sim \text{const}$. For initially net-helical configurations, we observe rapid development of mixed-signed magnetic helicity patches and a decrease of the global fractional helicity, such that the decay over the kinetic interval is again most consistent with the cancellation-dominated scaling constraint.

</details>


### [40] [Characterization of compressible fluctuations in solar wind streams dominated by balanced and imbalanced turbulence: Parker Solar Probe, Solar Orbiter and Wind observations](https://arxiv.org/abs/2602.17606)
*C. A. Gonzalez,C. Gonzalez,A. Tenerani*

Main category: physics.plasm-ph

TL;DR: Statistical analysis of solar wind compressible fluctuations shows they're affected by both solar expansion and local plasma conditions, with slow magnetosonic modes dominating near the Sun and potentially contributing to solar wind heating.


<details>
  <summary>Details</summary>
Motivation: Compressible fluctuations in solar wind are important for understanding solar wind acceleration and heating, but their origin and evolution across different turbulence regimes remain poorly understood.

Method: Statistical analysis using in-situ measurements from Wind, Solar Orbiter, and Parker Solar Probe to investigate scale dependence of density and magnetic field fluctuations and their correlations with plasma beta and radial distance.

Result: Solar wind compressibility is affected by both expansion effects and compressible dynamics. Non-Alfvenic wind shows anti-correlated fluctuations, while Alfvenic wind contains both correlated and anti-correlated fluctuations. The dominant slow mode component explains beta dependence and enhanced density fluctuations measured by Parker Solar Probe.

Conclusion: Slow mode waves contribute significantly to the compressible energy budget near the Sun and may play an important role in solar wind heating and acceleration close to the Sun.

Abstract: Characterizing compressible fluctuations in the solar wind is essential for understanding their role in solar wind acceleration and heating, yet their origin and evolution across different turbulence regimes remain poorly understood. In this study, we carry out a statistical analysis of the properties of compressible fluctuations in solar wind dominated by balanced and imbalanced turbulence. Using in-situ measurements from Wind, Solar Orbiter, and Parker Solar Probe, we investigate the scale dependence of density and magnetic field fluctuations and their correlations with plasma beta and radial distance. Our results indicate that solar wind compressibility is likely affected by both expansion effects and compressible dynamics governed by local plasma conditions. The non-Alfvenic wind is dominated by anti-correlated fluctuations, whereas the Alfvenic wind contains a mixture of correlated and anti-correlated fluctuations, though the latter remain prevalent. While the anti-correlated component is consistent with MHD slow magnetosonic modes, the correlated (fast mode-like) component is not reproduced by predictions from either linear MHD theory or nonlinear models of forced compressible fluctuations. Nevertheless, the dominant slow mode component explains the observed dependence on beta and the enhanced density fluctuations measured by Parker Solar Probe. This further suggests that slow mode waves contribute significantly to the compressible energy budget near the Sun and may play an important role in solar wind heating and acceleration close to the Sun.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling](https://arxiv.org/abs/2602.17089)
*Xinghao Dong,Huchen Yang,Jin-long Wu*

Main category: cs.LG

TL;DR: Flow matching in latent space enables fast single-step sampling for stochastic closure models, 100x faster than diffusion models, with regularization to maintain physical fidelity.


<details>
  <summary>Details</summary>
Motivation: Diffusion models offer high-quality diverse samples for stochastic closure modeling but suffer from slow sampling speeds. Need faster generative models that maintain physical fidelity for practical applications.

Method: Use flow matching in lower-dimensional latent space for single-step sampling. Compare implicit regularization (joint training) against explicit regularizers: metric-preserving (MP) and geometry-aware (GA) constraints to control latent space distortion and ensure physical fidelity.

Result: Flow matching enables single-step sampling up to two orders of magnitude faster than iterative diffusion models. Both explicitly and implicitly regularized latent spaces preserve topological information from the original system's manifold, enabling learning with less training data.

Conclusion: Latent space flow matching with appropriate regularization provides a superior approach for stochastic closure modeling, combining fast sampling with physical fidelity and reduced data requirements compared to diffusion models.

Abstract: Diffusion models recently developed for generative AI tasks can produce high-quality samples while still maintaining diversity among samples to promote mode coverage, providing a promising path for learning stochastic closure models. Compared to other types of generative AI models, such as GANs and VAEs, the sampling speed is known as a key disadvantage of diffusion models. By systematically comparing transport-based generative models on a numerical example of 2D Kolmogorov flows, we show that flow matching in a lower-dimensional latent space is suited for fast sampling of stochastic closure models, enabling single-step sampling that is up to two orders of magnitude faster than iterative diffusion-based approaches. To control the latent space distortion and thus ensure the physical fidelity of the sampled closure term, we compare the implicit regularization offered by a joint training scheme against two explicit regularizers: metric-preserving (MP) and geometry-aware (GA) constraints. Besides offering a faster sampling speed, both explicitly and implicitly regularized latent spaces inherit the key topological information from the lower-dimensional manifold of the original complex dynamical system, which enables the learning of stochastic closure models without demanding a huge amount of training data.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [42] [Perturbation analysis of tensor $(\mathcal{B},\mathcal{C})$-inverse via Einstein product](https://arxiv.org/abs/2602.16759)
*Daochang Zhang,Jingqian Li,Dijana Mosic,Predrag S. Stanimirovic*

Main category: math.FA

TL;DR: Analysis of how small tensor perturbations affect various generalized inverses, focusing on relationships between perturbed and original tensor inverses.


<details>
  <summary>Details</summary>
Motivation: Understanding the sensitivity of generalized inverses to small perturbations is crucial for numerical stability and practical applications where exact tensor values may be unavailable or subject to noise.

Method: Theoretical analysis of perturbation effects on tensor generalized inverses, specifically examining inner, outer, and $(\mathcal{B},\mathcal{C})$ inverses under small perturbations $\mathcal{E}$.

Result: Establishes relationships between $\mathcal{D}^\Game$ and $\mathcal{A}^\Game$ for various generalized inverses, providing insights into how perturbations propagate through inverse operations.

Conclusion: Small tensor perturbations significantly affect generalized inverses, and understanding these relationships is essential for robust numerical computations and applications involving tensor inverses.

Abstract: We investigate the influence of a relatively small perturbation on various generalized inverses functions or quantities derived from a tensor $\mathcal{A}$.When a small tensor perturbation \(\mathcal{E}\) is introduced, it becomes challenging to analyze generalized inverses of the perturbed tensor \( \mathcal{D} =\mathcal{A}+\mathcal{E}\) and to determine how this perturbation affects a generalized inverse of $\mathcal{A}$.Our main goal is to understand the relationship between $\mathcal{D}^\Game$ and \( \mathcal{A}^\Game \), where $(\cdot)^\Game$ denotes a specific generalized inverse or a class of generalized inverses.In particular, classes of tensor inner, outer, and $(\mathcal{B},\mathcal{C})$ inverses are considered.

</details>


### [43] [Product Hardy Spaces on Spaces of Homogeneous Type: Discrete Product Calderón-Type Reproducing Formula, Atomic Characterization, and Product Calderón--Zygmund Operators](https://arxiv.org/abs/2602.17031)
*Ziyi He,Dachun Yang,Taotao Zheng*

Main category: math.FA

TL;DR: The paper introduces product Hardy spaces on product spaces of homogeneous type using wavelet coefficients, establishes atomic characterizations, and develops a new discrete product Calderón-type reproducing formula to handle wavelets without bounded support.


<details>
  <summary>Details</summary>
Motivation: To develop a theory of product Hardy spaces on product spaces of homogeneous type that avoids limitations of wavelet reproducing formulas when wavelets lack bounded support, enabling atomic characterizations and operator boundedness results.

Method: 1. Define product Hardy space H^p(X₁×X₂) via wavelet coefficients using CMO^p spaces as test functions. 2. Establish new discrete product Calderón-type reproducing formula with bounded support. 3. Develop atomic characterization of product Hardy spaces. 4. Apply to boundedness of linear operators from Hardy to Lebesgue spaces.

Result: 1. Introduced product Hardy spaces on product spaces of homogeneous type. 2. Established atomic characterization of these spaces. 3. Developed new discrete product Calderón-type reproducing formula with bounded support. 4. Obtained boundedness criterion for linear operators from product Hardy to Lebesgue spaces. 5. Proved boundedness of product Calderón-Zygmund operators on product Hardy spaces.

Conclusion: The paper successfully develops a comprehensive theory of product Hardy spaces on product spaces of homogeneous type, overcoming limitations of wavelet reproducing formulas through a new discrete Calderón-type formula, enabling atomic characterizations and operator boundedness results.

Abstract: Let $i\in\{1,2\}$ and $X_i$ be a space of homogeneous type in the sense of Coifman and Weiss with the upper dimension $ω_i$. Also let $η_i$ be the smoothness index of the Auscher--Hytönen wavelet function $ψ^{k_i}_{α_i}$ on $X_i$. In this article, for any $p\in(\max\{\frac{ω_1}{ω_1+η_1},\frac{ω_2}{ω_2+η_2}\}, 1]$, by regarding the product Carleson measure space $\mathrm{CMO}^p_{L^2}(X_1\times X_2)$ as the test function space and its dual space $(\mathrm{CMO}^p_{L^2}(X_1\times X_2))'$ as the corresponding distribution space, we introduce the product Hardy space $H^p(X_1\times X_2)$ in terms of wavelet coefficients. Moreover, we establish an atomic characterization of this product Hardy space and, as an application, obtain a criterion for the boundedness of linear operators from product Hardy spaces to corresponding Lebesgue spaces. To escape the wavelet reproducing formula, which is not useful for this atomic characterization because the wavelets have no bounded support, we establish a new discrete product Calderón-type reproducing formula, which holds in the product Hardy space and has bounded support. This reproducing formula also leads to the boundedness of product Calderón--Zygmund operators on the product Hardy space.

</details>


### [44] [Smoothing on $L^1$ for ground state transformed semigroups in non-local settings](https://arxiv.org/abs/2602.17178)
*Miłosz Baraniewicz,Kamil Kaleta*

Main category: math.FA

TL;DR: The paper studies L¹-smoothing properties of semigroups from ground state transformations of Schrödinger semigroups with confining potentials and non-local Lévy operators, where ultracontractivity and hypercontractivity fail.


<details>
  <summary>Details</summary>
Motivation: Inspired by Talagrand's convolution conjecture and developments on classical Ornstein-Uhlenbeck semigroup, the research aims to understand L¹-smoothing effects in settings where standard contraction properties fail, particularly for non-local operators with confining potentials.

Method: The authors study semigroups arising from ground state transformations of Schrödinger semigroups with confining potentials associated with non-local Lévy operators. Their framework encompasses fractional and relativistic Laplacians as kinetic operators, with analysis of dependence on potential and Lévy measure.

Result: The paper provides estimates showing clear dependence on potential and Lévy measure, yielding description of semigroups' action on L¹ in terms of Orlicz spaces. Results demonstrate that L¹-regularizing effects become stronger as time increases to infinity, illustrated by numerous examples.

Conclusion: The work establishes L¹-smoothing properties for a broad class of semigroups with non-local kinetic operators and confining potentials, providing quantitative estimates that reveal strengthening regularization over time, even when standard contraction properties fail.

Abstract: We study the $L^1$-smoothing properties for a broad class of semigroups arising from the ground state transformation of Schrödinger semigroups with confining potentials associated with non-local Lévy operators, for which (asymptotic) ultracontractivity and hypercontractivity fail. Our work is inspired by Talagrand's convolution conjecture in the discrete cube setting, as well as by subsequent developments on the classical Ornstein--Uhlenbeck semigroup. The estimates we provide exhibit a clear dependence on the potential and the Lévy measure defining the kinetic term operator, and they yield a description of the semigroups' action on $L^1$ in terms of Orlicz spaces. Our framework is quite general, encompassing fractional and relativistic Laplacians as kinetic operators. The results are illustrated by numerous examples demonstrating that the $L^1$-regularizing effects become stronger as $t \uparrow \infty$.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [45] [Semi-Local Exchange-Correlation Approximations in Density Functional Theory](https://arxiv.org/abs/2602.17333)
*Fabien Tran,Susi Lehtola,Stefano Pittalis,Miguel A. L. Marques*

Main category: physics.chem-ph

TL;DR: A comprehensive review of semi-local density functional approximations in DFT, covering theoretical foundations, historical developments, and practical applications for both newcomers and practitioners.


<details>
  <summary>Details</summary>
Motivation: The exact exchange-correlation functional in DFT is unknown, requiring approximations for practical applications. Hundreds of approximations have been developed over six decades with varying accuracy and efficiency, necessitating a consolidated review to organize this extensive literature and provide a unified framework for understanding semi-local functionals.

Method: The review surveys theoretical foundations of semi-local functionals including local density approximations (LDA), generalized gradient approximations (GGA), and meta-generalized gradient approximations (meta-GGA). It begins with Kohn-Sham DFT concepts, presents construction principles, and discusses physical motivations, mathematical properties, and practical considerations.

Result: Provides a comprehensive, consistently organized discussion consolidating both historical developments and recent advances in semi-local functionals. The work serves as both an introduction for newcomers and a comprehensive reference for practitioners in the field.

Conclusion: This review aims to facilitate further developments in density functional approximations by providing a unified framework for understanding semi-local functional construction and application, helping tackle diverse challenges in computational chemistry and condensed matter physics.

Abstract: Density functional theory is the workhorse of modern electronic structure calculations, with wide-ranging applications in chemistry, physics, materials science, and machine learning. At its heart lies the exchange-correlation functional, a quantity which exactly encapsulates the many-body effects stemming from the quantum mechanical interactions between the electrons. Yet, the exact functional is unknown, and computationally tractable approximations are therefore necessary for practical applications. Over the past six decades, hundreds of density functional approximations have been proposed with varying degrees of accuracy and computational efficiency.
  This review surveys the theoretical foundations of semi-local functionals, including local density approximations, generalized gradient approximations, and meta-generalized gradient approximations. We provide a comprehensive, consistently organized discussion that consolidates both historical developments and recent advances in this field. Beginning with the essential concepts of Kohn-Sham density functional theory, we present the construction principles of semi-local exchange-correlation functionals. Special attention is given to the physical motivations underlying functional development, the mathematical properties that guide their construction, and the practical considerations that determine their applicability across different chemical and physical systems.
  This work is intended to serve as both a introduction for newcomers to the field and a comprehensive reference for practitioners. By consolidating the extensive literature on semi-local functionals and providing a unified framework for understanding their construction and application, we aim to facilitate further developments in density functional approximations and their use in tackling the diverse challenges of modern computational chemistry and condensed matter physics.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [46] [Linear Landau equation as a limit of a tagged particle in mean field interaction with a free gas](https://arxiv.org/abs/2602.16440)
*Thierry Bodineau,Pierre Le Bris*

Main category: math.PR

TL;DR: In dimensions d≥4, a tagged particle interacting with a dense gas converges to diffusion described by the linear Landau equation as gas density N→∞.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous connection between microscopic particle dynamics and macroscopic diffusion processes described by the linear Landau equation in high dimensions.

Method: Analyze tagged particle in mean field interaction with equilibrium gas; prove convergence using martingale problem approach with stability results and recollision probability controls.

Result: Proved convergence of tagged particle trajectory to diffusion process associated with linear Landau equation in dimensions d≥4 as gas density N→∞.

Conclusion: Microscopic particle dynamics converge to macroscopic diffusion described by linear Landau equation in high dimensions, bridging microscopic and macroscopic descriptions.

Abstract: We consider a tagged particle in mean field interaction with a free gas of density N at equilibrium. In dimensions $d\geq4$, we prove the convergence of its trajectory, as N goes to infinity, to the one of a diffusion process associated with the linear Landau equation. The proof of the convergence of the martingale problem relies on two key ingredients: long time stability results of the microscopic dynamics, and controls on the probability of particle recollisions.

</details>


### [47] [Variance renormalisation in regularity structures -- the case of $2d$ gPAM](https://arxiv.org/abs/2602.17369)
*Máté Gerencsér,Yueh-Sheng Hsu*

Main category: math.PR

TL;DR: The paper studies variance renormalization for a singular SPDE (2D generalized parabolic Anderson model) with very rough noise, requiring both multiplicative and additive renormalization, using regularity structures and BPHZ models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to handle singular stochastic PDEs where the standard Da Prato-Debussche trick doesn't apply, specifically the 2D generalized parabolic Anderson model driven by noise much rougher than white noise, which requires both multiplicative and additive renormalization.

Method: The method uses regularity structures and considers models that lift zero noises to nontrivial models (analogous to "pure area" in rough paths). Convergence to such models is shown for the BPHZ model over vanishing noise using graphical computations.

Result: The paper demonstrates convergence to the model for the BPHZ model over vanishing noise through graphical computations, handling the discrepancy between regularity structures of approximate and limiting equations.

Conclusion: The approach successfully handles variance renormalization for singular SPDEs with very rough noise where traditional methods fail, using regularity structures and nontrivial noise lifting models analogous to rough paths techniques.

Abstract: We consider the variance renormalisation of a singular SPDE for which a Da Prato-Debussche trick is not applicable. The example taken is the $2$-dimensional generalised parabolic Anderson model (gPAM), driven by a much rougher than white noise, necessitating both a multiplicative and an additive renormalisation. To handle the discrepancy between the regularity structures of the approximate and the limiting equations, we consider models that lift $0$ noises to nontrivial models, in analogy with ``pure area'' from rough paths. The convergence to such a model is shown for the BPHZ model over the vanishing noise via graphical computations.

</details>


### [48] [Banach fixed point and flow approach for rough analysis](https://arxiv.org/abs/2602.17437)
*Yvain Bruned,Yingtong Hou,Paul Laubie,Zhicheng Zhu*

Main category: math.PR

TL;DR: The paper shows that the algebraic assumption needed for fixed point arguments in rough differential equations implies the assumption for Bailleul flow approach, and proves that multi-indices Hopf algebra fails the cocycle condition, explaining practical impossibility of fixed point arguments for multi-indices rough paths.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between algebraic assumptions in different approaches to rough differential equations (fixed point vs Bailleul flow) and to explain why fixed point arguments fail in practice for multi-indices rough paths and multi-indices in Regularity Structures.

Method: The authors analyze the algebraic structure of rough paths, specifically focusing on Hopf algebras with cocycle conditions and tree-like bases. They prove that the Hopf algebra of multi-indices does not satisfy the required cocycle condition through rigorous mathematical analysis.

Result: 1) The main algebraic assumption for fixed point arguments implies the assumption for Bailleul flow approach. 2) The Hopf algebra of multi-indices fails to satisfy the cocycle condition, providing a rigorous explanation for the practical impossibility of performing fixed point arguments for multi-indices rough paths.

Conclusion: The failure of the cocycle condition in the multi-indices Hopf algebra explains why fixed point arguments cannot be applied to multi-indices rough paths in practice, connecting theoretical algebraic requirements with observed computational limitations in Regularity Structures.

Abstract: In this paper, we show that the main algebraic assumption required to perform a fixed point argument for rough differential equations implies the algebraic assumption for the Bailleul flow approach. This assumption requires that the rough path associated with the equation is given by a Hopf algebra whose coproduct admits a cocycle and has a tree-like basis. We show that the Hopf algebra of multi-indices does not satisfy the cocycle condition. This is a rigorous result on the impossibility, observed in practice, of performing a fixed point argument for multi-indices rough paths and multi-indices in Regularity Structures.

</details>


### [49] [An Allen-Cahn equation with jump-diffusion noise for biological damage and repair processes](https://arxiv.org/abs/2602.17495)
*Andrea Di Primio,Marvin Fritz,Luca Scarpa,Margherita Zanella*

Main category: math.PR

TL;DR: Stochastic Allen-Cahn equation models biomolecular damage/repair with multiplicative Wiener noise (continuous fluctuations) and jump noise (abrupt damage), using singular logarithmic potential. Proves well-posedness, analyzes long-term behavior (invariant measures, ergodicity, mixing), and presents simulation scheme capturing biological phenomena.


<details>
  <summary>Details</summary>
Motivation: To develop a mathematical model for biomolecular damage and repair dynamics that captures both continuous stochastic fluctuations and abrupt damage events, using phase-separation dynamics framework with realistic logarithmic potential.

Method: Stochastic Allen-Cahn equation with singular logarithmic Flory-Huggins potential, driven by multiplicative cylindrical Wiener process (continuous noise) and jump-type noise (abrupt damage). Analytical study of well-posedness and long-term behavior, plus numerical Euler-Maruyama scheme for simulation.

Result: Proves strong probabilistic well-posedness of the model, establishes existence and uniqueness of invariant measures, demonstrates ergodicity and mixing properties, and develops numerical scheme that captures biological phenomena like damage clustering and stress-induced topology perturbations.

Conclusion: The stochastic Allen-Cahn framework successfully models biomolecular damage/repair dynamics, capturing both continuous fluctuations and abrupt damage events, with rigorous mathematical foundations and practical simulation capabilities for studying biological phenomena.

Abstract: This paper analyzes a stochastic Allen--Cahn equation for the dynamics of biomolecular damage and repair. The system is driven by two distinct noise processes: a multiplicative cylindrical Wiener process, modeling continuous background stochastic fluctuations, and a jump-type noise, modeling the abrupt, localized damage induced by external shocks. The drift of the equation is singular and covers the typical logarithmic Flory-Huggins potential required in phase-separation dynamics. We prove well-posedness of the model in a strong probabilistic sense, and analyze its long-time behavior in terms of existence and uniqueness of invariant measures, ergodicity, and mixing properties. Eventually, we present an Euler--Maruyama scheme to simulate the model and illustrate how it captures fundamental biological phenomena, such as damage clustering, stress-induced topology perturbations, and damage dynamics.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [50] [Order of Magnitude Analysis and Data-Based Physics-Informed Symbolic Regression for Turbulent Pipe Flow](https://arxiv.org/abs/2602.17082)
*Yunus Emre Ünal,Özgür Ertunç,Ismail Ari,Ivan Otić*

Main category: physics.flu-dyn

TL;DR: Derives scaling relations for pipe friction using order-of-magnitude analysis, combines with symbolic regression to create accurate, interpretable friction factor correlations validated up to Re~10^7.


<details>
  <summary>Details</summary>
Motivation: Existing semi-empirical correlations like Colebrook-White don't fully replicate Nikuradse's rough-pipe experiments, creating need for more physically-grounded friction factor predictions.

Method: Order-of-magnitude analysis of Reynolds-averaged Navier-Stokes and kinetic-energy equations to derive scaling relations, then modified genetic programming (GPTIPS2) for symbolic regression combining Nikuradse's rough-pipe and smooth-pipe data.

Result: Develops interpretable expressions that accurately reproduce friction factors across various roughness levels and Reynolds numbers, validated up to Re~10^7.

Conclusion: The approach successfully creates compact, physically-constrained friction factor correlations that fit experimental data better than traditional semi-empirical equations.

Abstract: Friction losses in rough pipes are often predicted using semi-empirical correlations, such as the Colebrook-White equation (Colebrook,1939), which do not fully replicate Nikuradse's rough-pipe experiments (1950). This study derives scaling relations for the viscous and turbulent contributions to the streamwise pressure drop through an order-of-magnitude analysis of the Reynolds-averaged Navier-Stokes equations and the kinetic-energy transport equations. These relations impose constraints on the local sensitivity of the pressure drop to factors such as mean velocity, roughness, viscosity, and density through exponent envelopes and serve as a physical prior for symbolic regression. By combining Nikuradse's rough-pipe and smooth-pipe data of Zagarola and Smits (1998), we aim to derive compact correlations for the friction factor that fit experimental data while adhering to the derived constraints. A modified genetic programming engine (GPTIPS2) optimizes model structure and evaluates it based on fitness, complexity, and constraint violation. This method yields interpretable expressions that accurately reproduce friction factors across various roughness levels and Reynolds numbers, validated up to $Re \sim 10^7$.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [51] [On the Concept of Violence: A Comparative Study of Human and AI Judgments](https://arxiv.org/abs/2602.17256)
*Mariachiara Stellato,Francesco Lancia,Chiara Galeazzi,Nico Curti*

Main category: physics.soc-ph

TL;DR: This study compares human and LLM classifications of violence across 22 morally ambiguous scenarios, using violence as a proxy to examine how AI systems operationalize complex moral concepts and shape social interpretations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how LLMs interpret and classify complex social behaviors related to violence, given that definitions of violence are contested and LLMs are increasingly consulted for such judgments in everyday contexts. The study aims to determine whether LLMs reproduce, reshape, or simplify human conceptions of violence.

Method: The researchers systematically compared human judgments with LLM classifications across 22 carefully designed scenarios spanning physical aggression, verbal aggression, relational dynamics, marginalization, symbolic actions, and verbal expressions. They used multiple instruction-tuned models of varying sizes and architectures, conducting global, sentence-level, and thematic-domain analyses to assess patterns of convergence and divergence.

Result: The study treats violence as a strategic proxy to observe broader belief formation dynamics. It enables investigation of how LLMs operationalize ambiguous moral constructs, negotiate conceptual boundaries, and transform plural human interpretations into singular outputs.

Conclusion: The findings contribute to debates about the epistemic role of conversational AI in shaping interpretations of harm, responsibility, and social norms. The study highlights the importance of transparency and critical engagement as these systems increasingly mediate public reasoning about complex moral concepts.

Abstract: Background: What counts as violence is neither self-evident nor universally agreed upon. While physical aggression is prototypical, contemporary societies increasingly debate whether exclusion, humiliation, online harassment or symbolic acts should be classified within the same moral category. At the same time, Large Language Models (LLMs) are being consulted in everyday contexts to interpret and label complex social behaviors. Whether these systems reproduce, reshape or simplify human conceptions of violence remains an open question. Methods: Here we present a systematic comparison between human judgements and LLM classifications across 22 scenarios carefully designed to be morally dividing, spanning from physical and verbally aggressive behavior, relational dynamics, marginalization, symbolic actions and verbal expressions. Human responses were compared with outputs from multiple instruction-tuned models of varying sizes and architectures. We conducted global, sentence-level and thematic-domain analyses, and examined variability across models to assess patterns of convergence and divergence. Findings: This study treats violence as a strategically chosen proxy through which broader belief formation dynamics can be observed. Violence is not the focus of the study, but it serves as a tool to investigate broader analysis. It enables a structured investigation of how LLMs operationalize ambiguous moral constructs, negotiate conceptual boundaries, and transform plural human interpretations into singular outputs. More broadly, the findings contribute to ongoing debates about the epistemic role of conversational AI in shaping everyday interpretations of harm, responsibility and social norms, highlighting the importance of transparency and critical engagement as these systems increasingly mediate public reasoning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [52] [AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing](https://arxiv.org/abs/2602.17607)
*Jianda Du,Youran Sun,Haizhao Yang*

Main category: cs.AI

TL;DR: AutoNumerics is a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for PDEs from natural language descriptions, generating transparent solvers based on classical numerical analysis rather than black-box neural approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional PDE solver design requires substantial mathematical expertise and manual tuning, while recent neural network approaches are computationally expensive and lack interpretability. There's a need for an accessible, automated approach that maintains transparency.

Method: Multi-agent framework with coarse-to-fine execution strategy and residual-based self-verification mechanism. The system generates transparent solvers grounded in classical numerical analysis from natural language PDE descriptions.

Result: Experiments on 24 canonical and real-world PDE problems show competitive or superior accuracy compared to existing neural and LLM-based baselines. The framework correctly selects numerical schemes based on PDE structural properties.

Conclusion: AutoNumerics demonstrates viability as an accessible paradigm for automated PDE solving, offering transparent, interpretable solvers that bridge the gap between traditional expertise-heavy approaches and modern black-box neural methods.

Abstract: PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [53] [A Lower Bound for the First Non-zero Basic Eigenvalue on a Singular Riemannian Foliation](https://arxiv.org/abs/2602.17501)
*Bach Tran*

Main category: math.DG

TL;DR: Lower bounds for first non-zero basic eigenvalue on singular Riemannian manifolds with basic mean curvature, generalizing Zhong-Yang and Shi-Yang estimates, plus rigidity results.


<details>
  <summary>Details</summary>
Motivation: Extend classical eigenvalue estimates (Zhong-Yang, Shi-Yang) to singular Riemannian foliations with basic mean curvature, establishing connections between spectral properties, curvature bounds, and geometric structure.

Method: Develop lower bounds for first basic eigenvalue using Ricci curvature lower bounds and diameter of leaf space, then prove rigidity when eigenvalue achieves theoretical maximum.

Result: Generalized eigenvalue estimates for singular Riemannian foliations with basic mean curvature, and rigidity theorem showing that when λ₁ᴮ = π²/d², the manifold is isometric to a mapping torus of an isometry on a nonnegative Ricci curvature manifold.

Conclusion: The paper establishes fundamental spectral-geometric relationships for singular Riemannian foliations, providing both quantitative estimates and qualitative structural characterizations when spectral bounds are achieved.

Abstract: In this paper, we provide the lower bounds of the first non-zero basic eigenvalue on a closed singular Riemannian manifold $(M,\mathcal{F})$ with basic mean curvature that depends on the given non-negative lower bound of the Ricci curvature of $M$ and the diameter of the leaf space $M/\mathcal{F}$. These can be regarded as generalized versions of the Zhong-Yang estimate and a generalized Shi-Yang's estimate for singular Riemannian foliations with basic mean curvature. We also provide a rigidity result corresponding to the generalized Zhong-Yang estimate, which is a generalized Hang-Wang rigidity for singular Riemannian foliations with basic mean curvature. More precisely, when the first basic eigenvalue $λ_1^B$ is equal to $\frac{π^2}{d_{M/\mathcal{F}^2}} $, where $d_{M/\mathcal{F}}$ is the diameter of the leaf space, $M$ is isometric to a mapping torus of an isometry $\varphi:N\to N$ where $N$ is an $(n-1)$-dimensional Riemannian manifold of nonnegative Ricci curvature and $\mathcal{F}$ has the form $\{[\{\text{point}\}\times N]\}$.

</details>


### [54] [Distance Functions, Curvature and Topology](https://arxiv.org/abs/2602.17629)
*Carlo Mantegazza,Francesca Oronzio*

Main category: math.DG

TL;DR: The paper explores relationships between distance functions on Riemannian manifolds and their geometric properties, providing alternative proofs of classical curvature-topology theorems.


<details>
  <summary>Details</summary>
Motivation: To investigate how distance functions on Riemannian manifolds reflect underlying geometric structure and to provide new perspectives on established theorems connecting curvature and topology.

Method: Analysis of properties of distance functions on Riemannian manifolds and their relationship to manifold geometry, leading to alternative proof techniques for classical theorems.

Result: Established connections between distance function behavior and manifold geometry, yielding alternative proofs of classical curvature-topology theorems.

Conclusion: Distance functions on Riemannian manifolds provide valuable insights into geometric structure and offer alternative approaches to proving fundamental theorems in Riemannian geometry.

Abstract: We discuss some properties of the distance functions on Riemannian manifolds and we relate their behavior to the geometry of the manifolds. This leads to alternative proofs of some "classical" theorems connecting curvature and topology.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [55] [Optimal control of stochastic Volterra integral equations with completely monotone kernels and stochastic differential equations on Hilbert spaces with unbounded control and diffusion operators](https://arxiv.org/abs/2602.17578)
*Gabriele Bolli,Filippo de Feo*

Main category: math.OC

TL;DR: The paper develops a dynamic programming framework for optimal control of stochastic Volterra integral equations with completely monotone kernels, using Markovian lifting to transform them into Hilbert space SDEs with unbounded operators, establishing Γ-smoothing properties and solving HJB equations.


<details>
  <summary>Details</summary>
Motivation: Optimal control of stochastic Volterra integral equations (SVIEs) with completely monotone kernels faces deep mathematical difficulties despite their importance in applications like mathematical finance under rough volatility. Existing dynamic programming approaches struggle with these infinite-dimensional problems.

Method: 1. Use a recent Markovian lift to reformulate SVIE optimal control problems as SDEs on Hilbert spaces with unbounded control and diffusion operators. 2. Analyze regularity of Ornstein-Uhlenbeck transition semigroups. 3. Prove a new Γ-smoothing property in control directions. 4. Establish existence/uniqueness of mild HJB solutions, verification theorem, and optimal feedback controls.

Result: First results for this abstract class of infinite-dimensional problems: 1. Established Γ-smoothing property of transition semigroups. 2. Proved existence and uniqueness of mild solutions to HJB equations. 3. Developed verification theorem and constructed optimal feedback controls. 4. Applied results to optimal control of SVIEs with completely monotone kernels.

Conclusion: The paper successfully extends dynamic programming to optimal control of SVIEs with completely monotone kernels via Markovian lifting and Hilbert space methods, establishing foundational results for this challenging class of problems with applications in mathematical finance and other fields.

Abstract: The dynamic programming approach is one of the most powerful ones in optimal control. However, when dealing with optimal control problems of stochastic Volterra integral equations (SVIEs) with completely monotone kernels, deep mathematical difficulties arise and it is still not understood. These very classical problems have applications in most fields and have now become even more popular due to their applications in mathematical finance under rough volatility. In this article, we consider a class of optimal control problems of SVIEs with completely monotone kernels. Via a recent Markovian lift \cite{FGW2024}, the problem can be reformulated as an optimal control problem of stochastic differential equations (SDEs) on suitable Hilbert spaces, which due to the roughness of the kernel, presents a generator of an analytic semigroup and unbounded control and diffusion operators.
  This analysis leads us to study a general class of optimal control problems of abstract SDEs on Hilbert spaces with unbounded control and diffusion operators. This class includes optimal control problems of SVIEs with completely monotone kernels, but it is also motivated by other models. We analyze the regularity of the associated Ornstein-Uhlenbeck transition semigroup. We prove that the semigroup exhibits a new smoothing property in control directions through a general observation operator $Γ$, which we call $Γ$-smoothing. This allows us to establish existence and uniqueness of mild solutions of the Hamilton-Jacobi-Bellman equation, establish a verification theorem, and construct optimal feedback controls. We apply these results to optimal control problems of SVIEs with completely monotone kernels. To the best of our knowledge these are the first results of this kind for this abstract class of infinite dimensional problems and for the optimal control of SVIEs with completely monotone kernels.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [56] [Structured Analytic Mappings for Point Set Registration](https://arxiv.org/abs/2602.16753)
*Wei Feng,Tengda Wei,Haiyong Zheng*

Main category: eess.IV

TL;DR: Analytic-ICP: A non-rigid point set registration method using multivariate Taylor expansions for structured function representation, achieving higher accuracy and faster convergence than classical methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop an efficient and accurate non-rigid point set registration method that avoids the computational complexity of kernel-based approaches and provides a unified framework for various deformation types.

Method: Uses multivariate Taylor expansion to create structured function space with truncated basis terms. Develops quasi-Newton optimization to progressively lift identity maps into higher-order analytic forms. Embeds this into ICP loop with nearest-neighbor correspondences.

Result: Analytic-ICP achieves higher accuracy and faster convergence than classical methods (CPD and TPS-RPM), particularly for small and smooth deformations. It has quasi-linear time complexity and demonstrates effectiveness on 2D and 3D datasets.

Conclusion: The analytic approximation model provides a unified closed-form framework for rigid, affine, and nonlinear deformations without kernel functions or high-dimensional parameterizations, offering an efficient and accurate registration solution.

Abstract: We present an analytic approximation model for non-rigid point set registration, grounded in the multivariate Taylor expansion of vector-valued functions. By exploiting the algebraic structure of Taylor expansions, we construct a structured function space spanned by truncated basis terms, allowing smooth deformations to be represented with low complexity and explicit form. To estimate mappings within this space, we develop a quasi-Newton optimization algorithm that progressively lifts the identity map into higher-order analytic forms. This structured framework unifies rigid, affine, and nonlinear deformations under a single closed-form formulation, without relying on kernel functions or high-dimensional parameterizations. The proposed model is embedded into a standard ICP loop -- using (by default) nearest-neighbor correspondences -- resulting in Analytic-ICP, an efficient registration algorithm with quasi-linear time complexity. Experiments on 2D and 3D datasets demonstrate that Analytic-ICP achieves higher accuracy and faster convergence than classical methods such as CPD and TPS-RPM, particularly for small and smooth deformations.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [57] [Universal Fine-Grained Symmetry Inference and Enforcement for Rigorous Crystal Structure Prediction](https://arxiv.org/abs/2602.17176)
*Shi Yin,Jinming Mu,Xudong Zhu,Lixin He*

Main category: cond-mat.mtrl-sci

TL;DR: A new crystal structure prediction method using LLMs to generate Wyckoff patterns and constrained optimization to enforce physical consistency, achieving state-of-the-art performance without database reliance.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for crystal structure prediction either treat symmetry as a soft heuristic or rely on database templates, limiting physical fidelity and ability to discover genuinely new structures. There's a need for methods that can explore uncharted materials space without relying on existing databases.

Method: Uses large language models to encode chemical semantics and directly generate fine-grained Wyckoff patterns from composition. Incorporates domain knowledge through efficient constrained-optimization search that enforces algebraic consistency between site multiplicities and atomic stoichiometry. Integrates this symmetry-consistent template into a diffusion backbone to constrain generative trajectory to physically valid geometric manifold.

Result: Achieves state-of-the-art performance across stability, uniqueness, and novelty (SUN) benchmarks, with superior matching performance. Establishes a new paradigm for rigorous exploration of targeted crystallographic space.

Conclusion: The framework enables efficient expansion into previously uncharted materials space, eliminating reliance on existing databases or a priori structural knowledge, representing a significant advancement in crystal structure prediction methodology.

Abstract: Crystal structure prediction (CSP), which aims to predict the three-dimensional atomic arrangement of a crystal from its composition, is central to materials discovery and mechanistic understanding. Existing deep learning models often treat crystallographic symmetry only as a soft heuristic or rely on space group and Wyckoff templates retrieved from known structures, which limits both physical fidelity and the ability to discover genuinely new material structures. In contrast to retrieval-based methods, our approach leverages large language models to encode chemical semantics and directly generate fine-grained Wyckoff patterns from composition, effectively circumventing the limitations inherent to database lookups. Crucially, we incorporate domain knowledge into the generative process through an efficient constrained-optimization search that rigorously enforces algebraic consistency between site multiplicities and atomic stoichiometry. By integrating this symmetry-consistent template into a diffusion backbone, our approach constrains the stochastic generative trajectory to a physically valid geometric manifold. This framework achieves state-of-the-art performance across stability, uniqueness, and novelty (SUN) benchmarks, alongside superior matching performance, thereby establishing a new paradigm for the rigorous exploration of targeted crystallographic space. This framework enables efficient expansion into previously uncharted materials space, eliminating reliance on existing databases or a priori structural knowledge.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [58] [Modeling of Relativistic Plasmas with a Conservative Discontinuous Galerkin Method](https://arxiv.org/abs/2602.17487)
*James Juno,Grant Johnson,Alexander Philippov,Ammar Hakim,Alexander Chernoglazov,Shuzhe Zeng*

Main category: astro-ph.HE

TL;DR: A noise-free, high-order discontinuous Galerkin method for solving relativistic Vlasov-Maxwell equations on phase-space grids, with flexible velocity mapping for extreme energy scales.


<details>
  <summary>Details</summary>
Motivation: Traditional Monte-Carlo methods suffer from Poisson noise, limiting detailed analysis of plasma dynamics in extreme high-energy-density environments like astrophysical and laboratory settings.

Method: Direct discretization of kinetic equations on high-dimensional phase-space grids using discontinuous Galerkin finite elements, with novel flexible velocity-space mapping for efficient treatment of wide energy ranges.

Result: A conservative, high-order numerical scheme free from Poisson noise, capable of handling extreme relativistic plasma phenomena including QED pair-production, neutron star magnetospheres, and magnetic reconnection.

Conclusion: The noise-free approach enables unique insights into plasma dynamics, allowing detailed analysis of electromagnetic emission and fine-scale phase-space structures in relativistic plasmas.

Abstract: We present a new method for solving the relativistic Vlasov--Maxwell system of equations, applicable to a wide range of extreme high-energy-density astrophysical and laboratory environments. The method directly discretizes the kinetic equation on a high-dimensional phase-space grid using a discontinuous Galerkin finite element approach, yielding a high-order, conservative numerical scheme that is free from the Poisson noise inherent to traditional Monte-Carlo methods. A novel and flexible velocity-space mapping technique enables the efficient treatment of the wide range of energy scales characteristic of relativistic plasmas, including QED pair-production discharges, instabilities in strongly magnetized plasmas surrounding neutron stars, and relativistic magnetic reconnection. Our noise-free approach is capable of providing unique insight into plasma dynamics, enabling detailed analysis of electromagnetic emission and fine-scale phase-space structure.

</details>


### [59] [Theory of striped dynamic spectra of the Crab pulsar high-frequency interpulse](https://arxiv.org/abs/2602.16955)
*Mikhail V. Medvedev*

Main category: astro-ph.HE

TL;DR: The paper develops a theory explaining the zebra pattern in Crab pulsar's high-frequency interpulse radio emission as interference maxima from multiple ray propagation through the magnetosphere, combining gravitational lensing and plasma de-lensing effects.


<details>
  <summary>Details</summary>
Motivation: To explain the observed spectral zebra pattern in the Crab pulsar's high-frequency interpulse radio emission and use this phenomenon to probe the pulsar magnetosphere structure and test strong-field gravity.

Method: Developed a theoretical model where the zebra pattern arises from interference maxima caused by multiple ray propagation through the pulsar magnetosphere, combining gravitational lensing and plasma de-lensing effects.

Result: The model enables space-resolved tomography of the pulsar magnetosphere, yielding a radial plasma density profile of n_e ∝ r^{-3} that agrees with theoretical predictions. The model also predicts the zebra pattern will change at frequencies between 42-650 GHz when ray separation becomes smaller than pulsar size.

Conclusion: The zebra pattern provides a powerful tool for magnetospheric tomography and testing strong-field gravity. Future observations with facilities like ALMA and SMA in the predicted frequency range could validate the model and advance our understanding of pulsar magnetospheres and fundamental physics.

Abstract: A theory of the spectral "zebra" pattern of the Crab pulsar's high-frequency interpulse (HFIP) radio emission is developed. The observed emission bands are interference maxima caused by multiple ray propagation through the pulsar magnetosphere. The high-contrast interference pattern is the combined effect of gravitational lensing and plasma de-lensing of light rays. The model enables space-resolved tomography of the pulsar magnetosphere, yielding a radial plasma density profile of $n_{e}\propto r^{-3}$, which agrees with theoretical insights. We predict the zebra pattern trend to change at a higher frequency when the ray separation becomes smaller than the pulsar size. This frequency is predicted to be in the range between 42 GHz and 650 GHz, which is within the reach of existing facilities like ALMA and SMA. These observations hold significant importance and would contribute to our understanding of the magnetosphere. Furthermore, they offer the potential to investigate gravity in the strong field regime near the star's surface.

</details>
