<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 5]
- [math.AP](#math.AP) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [cs.NE](#cs.NE) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Two approaches to low-parametric SimRank computation](https://arxiv.org/abs/2602.20282)
*Egor P. Berezin,Robert T. Zaks,German Z. Alekhin,Stanislav V. Morozov,Sergey A. Matveev*

Main category: math.NA

TL;DR: Proposes two low-parametric approaches for approximating SimRank matrices with memory-efficient implementations, validated on real datasets.


<details>
  <summary>Details</summary>
Motivation: SimRank matrices require significant memory for computation and storage, but existing approaches mainly focus on algorithmic complexity rather than memory efficiency. There's a need for economical embedding formats that maintain accuracy while reducing memory consumption.

Method: Two major formats: 1) Non-symmetric form computed using specialized alternating optimization algorithm, and 2) Symmetric representation using Newton-type iterations. Both avoid dense matrices and maintain low memory consumption through specialized numerical implementations.

Result: Algorithms yield good approximation of SimRank matrices in terms of error norm (particularly Chebyshev norm) and preserve the average number of most similar elements for each node, as demonstrated on real publicly available datasets.

Conclusion: The proposed low-parametric approaches provide effective memory-efficient alternatives for SimRank matrix approximation while maintaining accuracy in similarity estimation between graph nodes.

Abstract: In this work, we discuss low-parametric approaches for approximating SimRank matrices, which estimate the similarity between pairs of nodes in a graph. Although SimRank matrices and their computation require a significant amount of memory, common approaches mostly address the problem of algorithmic complexity. We propose two major formats for the economical embedding of target data. The first approach adopts a non-symmetric form that can be computed using a specialized alternating optimization algorithm. The second is based on a symmetric representation and Newton-type iterations. We propose numerical implementations for both methodologies that avoid working with dense matrices and maintain low memory consumption. Furthermore, we study both types of embeddings numerically using real data from publicly available datasets. The results show that our algorithms yield a good approximation of the SimRank matrices, both in terms of the error norm (particularly the Chebyshev norm) and in preserving the average number of the most similar elements for each given node.

</details>


### [2] [The largest 5th pivot may be the root of a 61st degree polynomial](https://arxiv.org/abs/2602.20390)
*James Chen,Alan Edelman,John Urschel*

Main category: math.NA

TL;DR: The paper introduces computational techniques combining JuMP, Gröbner bases, and interval arithmetic to study the maximum growth factor in Gaussian elimination with complete pivoting, achieving exact results for n=5 and improving upper bounds.


<details>
  <summary>Details</summary>
Motivation: To determine the largest possible growth factor in Gaussian elimination with complete pivoting, a long-standing open problem in numerical linear algebra that involves complex polynomial inequality constraints.

Method: Develops a hybrid approach combining numerical optimization (JuMP) with exact algebraic computations (Gröbner bases and discriminant polynomials), plus interval arithmetic for rigorous verification.

Result: For n=5, reproduces the previously observed maximum value 4.1325..., proves it's exactly a root of a 61st degree polynomial, and slightly improves the upper bound from ~4.94 to 4.84. Also applies the method to n=6, 7, and 8.

Conclusion: The paper demonstrates a powerful computational framework bridging numerical and exact mathematics for studying growth factors, provides exact characterization for n=5, and conjectures the observed value is the true maximum.

Abstract: This paper introduces a number of new techniques in the study of the famous question from numerical linear algebra: what is the largest possible growth factor when performing Gaussian elimination with complete pivoting? This question is highly complex, due to a complicated set of polynomial inequalities that need to be simultaneously satisfied. This paper introduces the JuMP + Groebner basis + discriminant polynomial approach as well as the use of interval arithmetic computations. Thus, we are introducing a marriage of numerical and exact mathematical computations.
  In 1988, Day and Peterson performed numerical optimization on $n=5$ with NPSOL and obtained a largest seen value of $4.1325...$. This same best value was reproduced by Gould with LANCELOT in 1991. We ran extensive comparable experiments with the modern software tool JuMP and also saw the same value $4.1325...$. While the combinatorial explosion of possibilities prevents us from knowing there may not be a larger maximum, we succeed in obtaining the exact mathematical value: the number $4.1325...$ is exactly the root of a 61st degree polynomial provided in this work, and is a maximum given the equality constraints seen by JuMP. In light of the numerics, we pose the conjecture that this lower bound is indeed the maximum. We also apply this technique to $n = 6$, $7$, and $8$.
  Furthermore, in 1969, an upper bound of $4\frac{17}{18}\approx 4.94$ was produced for the maximum possible growth for $n = 5$. We slightly lower this upper bound to $4.84$.

</details>


### [3] [A parametrix for the surface Stokes equation](https://arxiv.org/abs/2602.20395)
*Tristan Goodwill,Jeremy Hoskins,Zydrunas Gimbutas,Bowei Wu*

Main category: math.NA

TL;DR: A new integral equation formulation for surface Stokes equations using 2D Stokeslets, enabling high-order discretization with fast direct solvers via proxy shell method.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for solving surface Stokes equations, which are important in fluid dynamics on surfaces, by creating a formulation that allows for high-order discretization and fast solution techniques.

Method: Construct integral equation formulation using two-dimensional Stokeslets, resulting in Fredholm integral equations of the second kind. Discretize using standard high-order tools and implement proxy shell method for fast direct solvers of dense linear systems.

Result: The formulation produces Fredholm integral equations that can be discretized to high order. The proxy shell method enables construction of fast direct solvers for the resulting dense linear systems, with performance demonstrated through numerical examples.

Conclusion: The proposed integral equation formulation combined with proxy shell method provides an effective approach for solving surface Stokes equations with high-order accuracy and computational efficiency.

Abstract: We introduce an integral equation formulation of the surface Stokes equations, constructed using two-dimensional Stokeslets. The resulting integral equations are Fredholm integral equations of the second kind and can be discretized to high order using standard tools. Since the resulting discrete linear systems are dense, we describe and analyze a proxy shell method to construct fast direct solvers for these systems. The properties of our integral equation, and the performance of the resulting numerical scheme, are illustrated with several representative numerical examples.

</details>


### [4] [Implicit-explicit all-speed schemes for compressible Cahn-Hilliard-Navier-Stokes equations](https://arxiv.org/abs/2602.20679)
*Andreu Martorell,Pep Mulet,Dionisio F. Yáñez*

Main category: math.NA

TL;DR: A second-order IMEX scheme for compressible Cahn-Hilliard-Navier-Stokes equations in low Mach regime using staggered finite differences


<details>
  <summary>Details</summary>
Motivation: Standard explicit schemes face severe time-step restrictions in low Mach number regime due to fourth-order diffusion terms and stiffness from fast acoustic waves

Method: Second-order implicit-explicit (IMEX) time-stepping scheme using finite differences on staggered grids; splits equations into stiff (pressure, viscous forces, fourth-order Cahn-Hilliard terms) treated implicitly and non-stiff terms treated explicitly

Result: Method designed to handle challenges of low Mach number limit where system approaches incompressible behavior

Conclusion: IMEX strategy overcomes time-step restrictions by appropriate splitting of stiff and non-stiff components in compressible Cahn-Hilliard-Navier-Stokes equations

Abstract: We propose a second-order implicit-explicit (IMEX) time-stepping scheme for the isentropic, compressible Cahn-Hilliard-Navier-Stokes equations in the low Mach number regime.
  The method is based on finite differences on staggered grids and is specifically designed to handle the challenges posed by the low Mach number limit, where the system approaches to an incompressible behavior.
  In this regime, standard explicit schemes suffer from severe time-step restrictions due to fourth-order diffusion terms and the stiffness induced by fast acoustic waves.
  To overcome this, we employ an IMEX strategy which splits the governing equations into stiff and non-stiff components.
  The stiff terms, arising from pressure, viscous forces and fourth-order Cahn-Hilliard contributions, are treated implicitly, while the remaining are dealt explicitly.

</details>


### [5] [Reduced-order computational homogenization for hyperelastic media using gradient based sensitivity analysis of microstructures](https://arxiv.org/abs/2602.20697)
*Vladimír Lukeš,Eduard Rohan*

Main category: math.NA

TL;DR: A computational homogenization algorithm for locally periodic hyperelastic structures under large deformations that uses clustering and sensitivity analysis to reduce microscopic problem solving in nonlinear simulations.


<details>
  <summary>Details</summary>
Motivation: To accelerate computational homogenization of hyperelastic structures undergoing large deformations by reducing the number of microscopic problems that need to be solved in nonlinear simulations.

Method: Clusters macroscopic deformations into subsets called "centroids" and approximates homogenized coefficients using sensitivity analysis of micro-configurations with respect to macroscopic deformation, implemented in SfePy finite element framework.

Result: The novel model-order reduction approach significantly reduces computational cost compared to proper orthogonal decomposition method and full FE-square simulations, with reduction degree controllable by error tolerance parameter.

Conclusion: The algorithm effectively accelerates computational homogenization for hyperelastic structures, with demonstrated performance in 2D test examples and potential for extensions beyond current implementation scope.

Abstract: We propose an algorithm for the computational homogenization of locally periodic hyperelastic structures undergoing large deformations due to external quasi-static loading. The algorithm performs clustering of macroscopic deformations into subsets called "centroids", and, as a new ingredient, approximates the homogenized coefficients using sensitivity analysis of micro-configurations with respect to the macroscopic deformation. The novel "model-order reduction" approach significantly reduces the number of microscopic problems that must be solved in nonlinear simulations, thereby accelerating the overall computational process. The degree of reduction can be controlled by a user-defined error tolerance parameter. The algorithm is implemented in the finite element framework SfePy, and its performance effectiveness is demonstrated using two-dimensional test examples, when compared with solutions obtained by the proper orthogonal decomposition method, and by the full "FE-square" simulations. Extensions beyond the present implementations and the scope of tractable problems are discussed.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [6] [Uniform rectifiability of brittle fractures in linear elasticity](https://arxiv.org/abs/2602.20381)
*Camille Labourie*

Main category: math.AP

TL;DR: The paper proves uniform rectifiability of brittle fractures in arbitrary dimensions, overcoming limitations of existing approaches for the Griffith setting.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for the Mumford-Shah functional face serious obstacles in the Griffith setting due to the lack of coarea formula for the symmetric gradient, necessitating an alternative route to prove uniform rectifiability of brittle fractures.

Method: The authors present an alternative approach by proving that cracks have "plenty of big projections" to establish uniform rectifiability for free-discontinuity problems.

Result: The paper successfully proves the uniform rectifiability of brittle fractures in arbitrary dimension, overcoming the limitations of previous methods.

Conclusion: The "plenty of big projections" approach provides a viable alternative route to establish uniform rectifiability for free-discontinuity problems in the Griffith setting where traditional methods fail.

Abstract: We prove the uniform rectifiability of brittle fractures in arbitrary dimension. The existing approach for the Mumford-Shah functional, which relies on separation-type properties of the singular set, faces serious obstacles in the Griffith setting due to the lack of coarea formula for the symmetric gradient. We present an alternative route to uniform rectifiability for free-discontinuity problems by proving that cracks have ``plenty of big projections''.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [7] [A Novel NPT Thermodynamic Integration Scheme to Derive Rigorous Gibbs Free Energies for Crystalline Solids](https://arxiv.org/abs/2602.20738)
*Karel L. K. De Witte,Tom Braeckevelt,Massimo Bocus,Sander Vandenhaute,Veronique Van Speybroeck*

Main category: physics.comp-ph

TL;DR: A new two-step thermodynamic integration method operates entirely in NPT ensemble, eliminating approximate NVT-to-NPT corrections and accounting for full cell fluctuations.


<details>
  <summary>Details</summary>
Motivation: Conventional TI schemes require three corrections starting from NVT harmonic reference, and the NVT-to-NPT correction neglects full cell flexibility, limiting accuracy for materials with complex cell-shape behavior.

Method: A rigorous two-step TI scheme operating entirely in NPT ensemble with a novel NPT reference that explicitly accounts for full cell fluctuations, eliminating the approximate NVT-to-NPT step.

Result: For ice polymorphs with simple cell-shape distributions, the new approach reproduces conventional TI results with excellent agreement. For CsPbI3 black phase with complex cell-shape behavior, the novel method provides more accurate Gibbs free energy differences than conventional TI.

Conclusion: The new NPT TI scheme provides rigorous and direct Gibbs free energy calculations for solids with comparable computational cost and simplified workflow, offering improved accuracy for materials with complex cell-shape fluctuations.

Abstract: Thermodynamic Integration (TI) is the state-of-the-art computational technique for accurate Gibbs free energy predictions of solids. Conventional TI schemes start from an NVT harmonic reference and require three successive corrections to recover the Gibbs free energy of the real crystal in the NPT ensemble. However, the NVT-to-NPT correction neglects full cell flexibility. Here, we present a rigorous (and only) two-step TI scheme that operates entirely in the NPT ensemble, eliminating the need for the approximate NVT-to-NPT step. The key methodological advancement is the novel NPT reference that explicitly accounts for full cell fluctuations. The new approach is compared with the conventional one via two complementary case studies. For ice polymorphs, having simple cell-shape distributions, the new approach reproduces conventional TI results with excellent agreement. For CsPbI3, whose black phase exhibits complex cell-shape behavior, we demonstrate that our novel method provides more accurate Gibbs free energy differences than the conventional one. Moreover, the proposed framework maintains comparable computational cost while offering a simplified workflow. Overall, the new NPT TI scheme provides rigorous and direct Gibbs free energy calculations for solids.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [8] [Deuterium-Tritium Levitated Dipole Fusion Power Plants](https://arxiv.org/abs/2602.20564)
*T. Simpson,R. A. Badcock,T. Berry,C. S. Chisholm,P. J. Fimognari,P. Fisher,D. T. Garnier,K. Lenagh-Glue,B. Leuw,R. Mataira,L. Meadows,T. McIntosh,J. Poata,K. Richardson,B. Smith,A. Simpson,J. D. Tyler,T. Wordsworth*

Main category: physics.plasm-ph

TL;DR: DT levitated dipole fusion reactors with shielded REBCO magnets can achieve economic fusion power using Deuterium-Tritium fuel cycle with replaceable sacrificial coil sections.


<details>
  <summary>Details</summary>
Motivation: Levitated dipole reactors offer economic fusion power with good accessibility/maintainability, but previous designs avoided DT fuel due to neutron damage concerns on superconducting magnets. This study addresses that limitation.

Method: Two DT levitated dipole power plant designs with heavily neutron-shielded REBCO core magnets (23 T peak field), using layered tungsten/boron carbide shielding to radiate 92% of heat to first wall while providing sufficient neutron attenuation.

Result: Larger design produces 667 MW fusion power and 208 MW net electric power. Core magnet uses sacrificial section (~20% of coil) with 1-year lifetime that can be replaced quickly, allowing refurbishment and reuse for economic operation.

Conclusion: DT levitated dipole reactors with shielded REBCO magnets and replaceable sacrificial coil sections are feasible for economic fusion power generation, overcoming historical neutron damage limitations.

Abstract: Levitated dipole reactors offer an attractive path towards economic fusion power generation. The intrinsic decoupling of the confining magnetic field-generating REBCO magnets and the vacuum vessel offer unparalleled accessibility and maintainability, allowing for high plant duty factors and theoretically low electricity prices. In order to achieve rapid deployment of fusion power to the grid, the use of the Deuterium-Tritium (DT) fuel cycle is required due to its lower required plasma triple products. Historically, designs of levitated dipole fusion power plants have targeted advanced fuels as a DT device was seen to be infeasible due to the high fluxes of 14.1 MeV neutrons on the superconducting core magnet. This study presents high level designs for two feasible first-of-a-kind (FOAK) DT levitated dipole fusion power plants, the larger of which produces 667 MW of fusion power and is predicted to produce 208 MW of net electric power. Both designs consist of a heavily neutron-shielded, high-field REBCO core magnet capable of producing peak magnetic field strengths of 23 T while keeping peak mechanical strains below 0.4%. The neutron shielding is comprised of a layered structure of tungsten and boron carbide, which allows for 92% of the heat deposited in the neutron shield to be radiated out to the first wall while still providing sufficient neutron attenuation to give adequate REBCO conductor lifetimes. The core magnet REBCO coil is comprised of a small "sacrificial" section and a larger semi-permanent section. The sacrificial section, comprising ~20% of the coil, will have a neutron damage limited lifetime of ~1 year, after which the core magnet will be quickly removed from the vacuum vessel and replaced. This allows the damaged core magnet to be refurbished and reused, reducing cost and allowing for economic fusion power generation from a DT levitated dipole reactor.

</details>


### [9] [Entropy stable numerical schemes for divergence diminishing Chew, Goldberger & Low equations for plasma flows](https://arxiv.org/abs/2602.20757)
*Chetan Singh,Harish Kumar,Deepak Bhoriya,Dinshaw S. Balsara*

Main category: physics.plasm-ph

TL;DR: The paper proposes an entropy-stable numerical method for the CGL plasma model using GLM technique to control magnetic field divergence, with improved divergence diminishing results.


<details>
  <summary>Details</summary>
Motivation: The CGL equations model plasma flows without local thermodynamic equilibrium, but controlling magnetic field divergence is important as the magnetic field evolves. The GLM technique can help address this divergence issue.

Method: Apply generalized Lagrange multiplier (GLM) technique to CGL model to create GLM-CGL system. Reformulate the system by treating some conservative terms as non-conservative to make it suitable for entropy-stable schemes. Then propose entropy stable numerical methods for the GLM-CGL model.

Result: Numerical results show the GLM approach leads to significant improvement in magnetic field divergence diminishing compared to the CGL system without GLM divergence diminishing approach.

Conclusion: The GLM technique successfully improves magnetic field divergence control in CGL plasma modeling, and the proposed entropy-stable numerical methods provide effective implementation for the GLM-CGL system.

Abstract: Chew, Goldberger & Low (CGL) equations are a set of hyperbolic PDEs with non-conservative products used to model the plasma flows, when the assumption of local thermodynamic equilibrium is not valid, and the pressure tensor is assumed to be rotated by the magnetic field. This results in the pressure tensor, which is described by the two scalar components. As the magnetic field also evolves, controlling the divergence of the magnetic field is important. In this work, we consider the generalized Lagrange multiplier (GLM) technique for the CGL model. The resulting model is referred to as the GLM-CGL system. To make the system suitable for entropy-stable schemes, we reformulate the GLM-CGL system by treating some conservative terms as non-conservative. The resulting system has a non-conservative part that does not affect entropy evolution. We then propose entropy stable numerical methods for the GLM-CGL model. The numerical results for the GLM-CGL system are then compared with the CGL system without the GLM divergence diminishing approach to demonstrate that the GLM approach indeed leads to significant improvement in the magnetic field divergence diminishing.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [10] [Enhancing Heat Sink Efficiency in MOSFETs using Physics Informed Neural Networks: A Systematic Study on Coolant Velocity Estimation](https://arxiv.org/abs/2602.20177)
*Aniruddha Bora,Isabel K. Alvarez,Julie Chalfant,Chryssostomos Chryssostomidis*

Main category: cs.NE

TL;DR: PINNs with sequential layer training solve inverse cooling problem for MOSFETs, predicting required coolant velocity from temperature data.


<details>
  <summary>Details</summary>
Motivation: MOSFETs in power electronics experience high thermal loads and require effective cooling to prevent overheating. Determining the required coolant velocity is an ill-posed inverse problem difficult to solve with traditional methods.

Method: Physics Informed Neural Networks (PINNs) with sequential training of MOSFET layers (aluminum, PGS, stainless steel pipes with water). The algorithm decouples optimization by treating other layer parameters as constants during each layer's training, reducing optimization dimensionality.

Result: Theoretical analysis shows PINNs converge to analytical solution. Predictions show good agreement with experimental results for required coolant velocity.

Conclusion: Sequential training PINNs provide effective solution to the inverse cooling problem for multilayer MOSFETs, overcoming ill-posed nature and traditional method limitations.

Abstract: In this work, we present a methodology using Physics Informed Neural Networks (PINNs) to determine the required velocity of a coolant, given inlet and outlet temperatures for a given heat flux in a multilayered metal-oxide-semiconductor field-effect transistor (MOSFET). MOSFETs are integral components of Power Electronic Building Blocks (PEBBs) and experiences the majority of the thermal load. Effective cooling of MOSFETs is therefore essential to prevent overheating and potential burnout. Determining the required velocity for the purpose of effective cooling is of importance but is an ill-posed inverse problem and difficult to solve using traditional methods. MOSFET consists of multiple layers with different thermal conductivities, including aluminum, pyrolytic graphite sheets (PGS), and stainless steel pipes containing flowing water. We propose an algorithm that employs sequential training of the MOSFET layers in PINNs. Mathematically, the sequential training method decouples the optimization of each layer by treating the parameters of other layers as constants during its training phase. This reduces the dimensionality of the optimization landscape, making it easier to find the global minimum for each layer's parameters and avoid poor local minima. Convergence of the PINNs solution to the analytical solution is theoretically analyzed. Finally we show the prediction of our proposed methodology to be in good agreement with experimental results.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [11] [Stochastic tensor contraction for quantum chemistry](https://arxiv.org/abs/2602.17158)
*Jiace Sun,Garnet Kin-Lic Chan*

Main category: physics.chem-ph

TL;DR: Stochastic tensor contraction reduces computational cost of high-order tensor operations in quantum chemistry, achieving mean-field scaling for chemical accuracy while approaching mean-field absolute cost.


<details>
  <summary>Details</summary>
Motivation: High-order tensor contractions in ab initio quantum chemistry methods like coupled cluster theory have prohibitive computational costs that limit system sizes that can be studied.

Method: Introduces stochastic tensor contraction to perform high-order tensor operations with reduced cost, applied to coupled cluster theory with perturbative triples (CCSD(T)).

Result: Achieves computational scaling reduction to mean-field theory level for chemical accuracy, approaches mean-field absolute cost, shows order-of-magnitude improvement in computation time and error compared to local correlation approximations, with reduced sensitivity to system dimensionality and electron delocalization.

Conclusion: Stochastic tensor contraction is a powerful computational primitive that can accelerate a wide range of quantum chemistry methods by dramatically reducing computational costs while maintaining accuracy.

Abstract: Many computational methods in ab initio quantum chemistry are formulated in terms of high-order tensor contractions, whose cost determines the size of system that can be studied. We introduce stochastic tensor contraction to perform such operations with greatly reduced cost, and present its application to the gold-standard quantum chemistry method, coupled cluster theory with up to perturbative triples. For total energy errors more stringent than chemical accuracy, we reduce the computational scaling to that of mean-field theory, while starting to approach the mean-field absolute cost, thereby challenging the existing cost-to-accuracy landscape. Benchmarks against state-of-the-art local correlation approximations further show that we achieve an order-of-magnitude improvement in both total computation time and error, with significantly reduced sensitivity to system dimensionality and electron delocalization. We conclude that stochastic tensor contraction is a powerful computational primitive to accelerate a wide range of quantum chemistry.

</details>
