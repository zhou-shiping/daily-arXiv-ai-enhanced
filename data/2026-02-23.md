<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 11]
- [math.AP](#math.AP) [Total: 15]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.CE](#cs.CE) [Total: 2]
- [physics.optics](#physics.optics) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Solving and learning advective multiscale Darcian dynamics with the Neural Basis Method](https://arxiv.org/abs/2602.17776)
*Yuhe Wang,Min Wang*

Main category: math.NA

TL;DR: Neural Basis Method: A projection-based approach that couples physics-conforming neural basis spaces with operator-induced residual metrics for stable, interpretable physics-informed machine learning.


<details>
  <summary>Details</summary>
Motivation: Current physics-informed ML methods treat governing equations as penalty losses with heuristic balancing, which blurs operator structure and confounds solution approximation error with equation enforcement error, making progress hard to interpret and control.

Method: Introduces Neural Basis Method: a projection-based formulation that couples predefined physics-conforming neural basis spaces with operator-induced residual metrics to obtain well-conditioned deterministic minimization. The residual serves as a computable certificate tied to approximation and enforcement.

Result: Method produces accurate and robust solutions in single solves and enables fast and effective parametric inference with operator learning. Demonstrates effectiveness using advective multiscale Darcian dynamics.

Conclusion: The Neural Basis Method provides a stable, interpretable framework for physics-informed ML where residuals serve as reliable certificates rather than just optimization objectives, enabling better control and understanding of solving and learning progress.

Abstract: Physics-governed models are increasingly paired with machine learning for accelerated predictions, yet most "physics--informed" formulations treat the governing equations as a penalty loss whose scale and meaning are set by heuristic balancing. This blurs operator structure, thereby confounding solution approximation error with governing-equation enforcement error and making the solving and learning progress hard to interpret and control. Here we introduce the Neural Basis Method, a projection-based formulation that couples a predefined, physics-conforming neural basis space with an operator-induced residual metric to obtain a well-conditioned deterministic minimization. Stability and reliability then hinge on this metric: the residual is not merely an optimization objective but a computable certificate tied to approximation and enforcement, remaining stable under basis enrichment and yielding reduced coordinates that are learnable across parametric instances. We use advective multiscale Darcian dynamics as a concrete demonstration of this broader point. Our method produce accurate and robust solutions in single solves and enable fast and effective parametric inference with operator learning.

</details>


### [2] [Variational optimization approach for reconstruction of dielectric permittivity and conductivity functions using partial boundary measurements](https://arxiv.org/abs/2602.17819)
*Eric Lindström,Larisa Beilina*

Main category: math.NA

TL;DR: Variational optimization approach for simultaneous reconstruction of dielectric permittivity and conductivity in Maxwell's system using limited boundary electric field measurements.


<details>
  <summary>Details</summary>
Motivation: To solve coefficient inverse problems for time-dependent Maxwell's system where both dielectric permittivity and conductivity functions need to be reconstructed simultaneously from limited boundary observations of electric field.

Method: Variational optimization approach based on constructing weak form Lagrangian enabling finite element reconstruction algorithms. Derives optimality conditions, stability estimates for adjoint problem, and Frechét differentiability of Lagrangian and regularized Tikhonov functional.

Result: Theoretical framework developed with optimality conditions, stability estimates, and differentiability proofs. Numerical studies in both 2D and 3D confirm theoretical investigations.

Conclusion: The variational optimization approach provides effective framework for solving simultaneous reconstruction problems in Maxwell's system, validated through theoretical analysis and numerical experiments in multiple dimensions.

Abstract: We present a variational optimization approach for the solution of a coefficient inverse problem of simultaneous reconstruction of the dielectric permittivity and conductivity functions in time-dependent Maxwell's system using limited boundary observations of the electric field.
  The variational optimization approach is based on constructing a weak form of a Lagrangian which allows to use finite element based reconstruction algorithms.
  The optimality conditions for the Lagrangian and stability estimate for the adjoint problem are derived, as well as Frechét differentiability of it and of the regularized Tikhonov functional are also presented. Two- and three-dimensional numerical studies confirm our theoretical investigations.

</details>


### [3] [A finite-difference summation-by-parts, conditionally stable partitioned algorithm for conjugate heat transfer problems](https://arxiv.org/abs/2602.17843)
*Sarah Nataj,David C. Del Rey Fernández,David Brown,Rajeev Jaiman*

Main category: math.NA

TL;DR: A novel partitioned scheme for conjugate heat transfer problems with provable conditional stability using high-order finite-difference methods and interface coupling techniques.


<details>
  <summary>Details</summary>
Motivation: To develop a stable partitioned solver for conjugate heat transfer problems that can handle general geometries while maintaining high-order accuracy and stability.

Method: Uses high-order summation-by-parts finite-difference operators with simultaneous-approximation-terms in curvilinear coordinates, combined with first- and second-order time discretizations and temporal extrapolation at the interface. Energy stability is achieved through careful selection of SAT parameters.

Result: Identified stable coupling parameters through systematic exploration, provided a stepwise approach for selecting SAT parameters that ensure stability, and demonstrated effectiveness through 2D numerical experiments on rectangular domains with curvilinear grids.

Conclusion: The proposed approach enables development of high-order, conditionally stable partitioned solvers suitable for general geometries in conjugate heat transfer problems.

Abstract: In this work, we design and analyze a novel, provably conditionally stable, weakly coupled partitioned scheme to solve the conjugate heat transfer (CHT) problem. We consider a model CHT problem consisting of linear advection-diffusion and heat equations, coupled at an interface through continuity of temperature and heat flux. We employ high-order summation-by-parts finite-difference operators in conjunction with simultaneous-approximation-terms (SATs) in curvilinear coordinates for spatial derivatives, combined with first- and second-order time discretizations and temporal extrapolation at the interface. Energy stability is maintained by carefully selecting SAT parameters at the interface. A range of coupling parameters are explored to identify those that yield a stable scheme, and a stepwise approach for choosing SAT parameters that ensure stability is given. The effectiveness of the method is demonstrated through numerical experiments in a two-dimensional model problem on a rectangular domain with curvilinear grids. The proposed approach enables the development of high-order, conditionally stable partitioned solvers suitable for general geometries.

</details>


### [4] [Hybrid ABBA-GMRES for Unmatched Backprojectors in Large Scale X-Ray Computerized Tomography](https://arxiv.org/abs/2602.17892)
*Ryan Bentley,Mirjeta Pasha,Malena Sabaté Landman,Luisa Yang,Jeffery Zhang*

Main category: math.NA

TL;DR: Hybrid AB- and BA-GMRES methods with Tikhonov regularization for CT reconstruction with unmatched projector pairs, featuring automatic parameter selection to mitigate semi-convergence and improve image quality.


<details>
  <summary>Details</summary>
Motivation: In large-scale CT, unmatched forward/backprojector pairs violate adjointness assumptions of classical least-squares solvers, causing non-symmetric behavior and semi-convergence. While AB-GMRES and BA-GMRES handle unmatched pairs better than traditional methods, they still exhibit semi-convergent behavior that needs regularization.

Method: Develop hybrid AB- and BA-GMRES methods incorporating Tikhonov regularization directly into Krylov subspace iterations. Propose automatic regularization parameter selection using L-curve and generalized cross validation (GCV). Compare with hybrid LSQR/LSMR variants for both matched and unmatched projector scenarios.

Result: Numerical experiments on 2D CT problems with GPU-accelerated projectors show hybrid methods mitigate semi-convergence, produce higher-quality reconstructions, and exhibit more stable stopping behavior than non-hybrid counterparts.

Conclusion: Hybrid AB-/BA-GMRES with automatic regularization parameter selection effectively addresses the challenges of unmatched projector pairs in CT reconstruction, providing improved regularization and more reliable stopping behavior compared to existing methods.

Abstract: In large-scale X-ray computed tomography (CT), matrix-free iterative methods are essential due to the prohibitive cost of explicitly forming the system matrix. In practice, forward projectors and backprojectors are often implemented with different discretizations or accelerations, leading to unmatched projector pairs. This mismatch violates the adjointness assumptions underlying classical least-squares solvers, so the resulting iterations no longer correspond to a true least-squares problem and can exhibit non-symmetric or inconsistent behavior. Prior work has explored Krylov subspace solvers such as AB-GMRES and BA-GMRES to handle unmatched projector pairs, where these methods exhibit semi-convergent regularizing behavior. Under matched conditions, AB-GMRES and BA-GMRES reduce to LSQR and LSMR, respectively. However, in the presence of unmatched projectors, AB- and BA-GMRES have been observed to yield improved reconstruction quality compared to classical least-squares solvers. In this paper, we develop hybrid AB- and BA-GMRES methods that incorporate Tikhonov regularization directly into the Krylov subspace iterations. We also examine the relationship between the proposed methods and hybrid variants of LSQR and LSMR, considering both matched and unmatched backprojectors. We propose automatic strategies for selecting regularization parameters, including approaches based on the L-curve and generalized cross validation (GCV), and analyze their effect on convergence behavior and image quality. Numerical experiments on two-dimensional CT problems using GPU-accelerated projectors demonstrate that the proposed hybrid AB- and BA-GMRES methods mitigate semi-convergence, produce higher-quality reconstructions, and exhibit more stable stopping behavior than their non-hybrid counterparts.

</details>


### [5] [Optimal error estimate of an isoparametric upwind discontinuous Galerkin method for radiation transport equation on curved domains](https://arxiv.org/abs/2602.17936)
*Changhui Yao,Yunpan Ma,Lingxiao Li*

Main category: math.NA

TL;DR: The paper analyzes an isoparametric upwind discontinuous Galerkin method for radiation transport on curved domains, achieving high-order optimal convergence by balancing discretization and geometric approximation errors.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate numerical method for solving radiation transport equations on domains with curved boundaries, where geometric approximation errors need to be properly balanced with discretization errors.

Method: An isoparametric upwind discontinuous Galerkin method using an auxiliary mapping to approximate the original curved domain, with analysis under the DG norm.

Result: Theoretical analysis shows high-order optimal convergence rate that balances numerical discretization and geometric approximation errors. Numerical experiments in 2D and 3D validate the theoretical results.

Conclusion: The proposed method effectively handles curved boundaries in radiation transport problems, achieving optimal convergence by properly accounting for both numerical and geometric approximation errors.

Abstract: This work investigates the isoparametric upwind discontinuous Galerkin method for solving the radiation transport equation defined on a bounded domain $D$ with a piecewise $C^{k+1}$ smooth curved boundary. An auxiliary mapping is constructed to approximate the original curved domain. The analysis delineates a high-order optimal convergence rate under the DG norm, which comprehensively balances the errors stemming from the numerical discretization and the geometric approximation. Two- and three-dimensional numerical experiments validate the theoretical results.

</details>


### [6] [Mathematical and numerical study on the ground states of rotating spin-orbit coupled spin-1 Bose-Einstein condensates](https://arxiv.org/abs/2602.17950)
*Jing Wang,Wei Yang,Yongjun Yuan,Yong Zhang*

Main category: math.NA

TL;DR: Mathematical study of ground states in three-component rotating spin-orbit coupled spin-1 BECs using coupled Gross-Pitaevskii equations, with rigorous existence proofs, development of efficient preconditioned nonlinear conjugate gradient algorithm, and extensive numerical experiments revealing interesting vortex phenomena.


<details>
  <summary>Details</summary>
Motivation: To understand the ground states of three-component rotating spin-orbit coupled spin-1 Bose-Einstein condensates, which are important quantum systems with complex interactions between rotation, spin-orbit coupling, and external potentials.

Method: 1) Rigorous mathematical analysis proving existence of ground states and deriving analytical properties (virial identity, negativity of SOC energy). 2) Development of efficient preconditioned nonlinear conjugate gradient (PCG) algorithm with Fourier spectral method on bounded domain, adaptive step size control, and optimized preconditioners. 3) Extensive numerical experiments using cascadic multigrid and FFT for efficiency.

Result: Successfully proved existence of ground states, developed highly efficient numerical algorithm with spectral accuracy, verified analytical properties numerically, and discovered interesting physical phenomena including giant vortex and U-shape vortex line formations influenced by local interactions, rotation, spin-orbit coupling, and external trapping potentials.

Conclusion: The study provides both rigorous mathematical foundation and efficient computational tools for analyzing ground states of complex three-component rotating SOC spin-1 BECs, revealing rich vortex physics that can be systematically explored through the developed framework.

Abstract: In this article, we study mathematically and numerically the ground states of three-component rotating spin-orbit coupled (SOC) spin-1 Bose-Einstein condensates modeled by the coupled Gross-Pitaevskii equations (CGPEs). Firstly, we rigorously prove existence result of the ground state and derive some analytical properties, including the virial identity and negativity of SOC energy. Secondly, we propose an efficient and accurate preconditioned nonlinear conjugate gradient (PCG) algorithm to compute the ground states. We truncate the whole space into a bounded rectangular domain and readily apply the Fourier spectral method to approximate the wave function. The PCG method is successfully adapted with appropriate modifications to the adaptive step size control strategy for the one-parameter energy minimization problem and to the choice of preconditioners, achieving great performance in terms of accuracy and efficiency. Lastly, we carry out extensive numerical experiments to verify the existence and property results of the ground states, confirm the spatial spectral accuracy by traversing the most commonly-used initial guesses for each component thanks to its great efficiency, which is also attributed to a utilization of cascadic multigrid and discrete Fast Fourier Transform (FFT). Moreover, we investigate the effects of local interaction, rotation and spin-orbit coupling and external trapping potential on the ground state, and unveil some interesting physical phenomena, such as giant vortex and U-shape vortex line.

</details>


### [7] [Strong convergence of finite element schemes for the stochastic Landau--Lifshitz--Bloch equation](https://arxiv.org/abs/2602.18021)
*Agus L. Soenjaya*

Main category: math.NA

TL;DR: Strong convergence analysis of finite element schemes for stochastic Landau-Lifshitz-Bloch equation with explicit convergence rates and stability results.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze reliable numerical methods for the stochastic Landau-Lifshitz-Bloch equation, which describes magnetization dynamics in ferromagnets at high temperatures, with rigorous convergence guarantees and stability properties.

Method: Analysis of semi-implicit and implicit fully discrete finite element schemes for the sLLB equation using localized error estimates and new exponential moment bounds for the exact solution, with both theoretical convergence analysis and numerical experiments.

Result: Established strong convergence in L²(Ω) with explicit convergence rates for numerical schemes, proved mean-square exponential stability of solutions and uniqueness of invariant measure in 1D under small noise, and sharpened existing convergence-in-probability results.

Conclusion: The paper provides rigorous numerical analysis for sLLB equation with strong convergence guarantees, stability results, and practical numerical validation, advancing computational methods for stochastic magnetization dynamics.

Abstract: The dynamics of magnetisation in a bounded ferromagnet in $\mathbb{R}^d$ ($d=1,2$) at high temperatures can be described by the stochastic Landau--Lifshitz--Bloch (sLLB) equation, which is a vector-valued quasilinear stochastic partial differential equation. In this paper, assuming adequate regularity of the initial data, we establish strong convergence in $L^2(Ω)$ of several semi-implicit and implicit fully discrete finite element schemes for the sLLB equation, together with explicit convergence rates. The analysis relies on localised error estimates and new exponential moment bounds for the exact solution. As a by-product, these moment bounds yield mean-square exponential stability of solutions and uniqueness of the invariant measure in one spatial dimension under a small noise assumption. We also sharpen existing convergence-in-probability results for the numerical schemes. Numerical experiments are presented to illustrate and support the theoretical findings.

</details>


### [8] [Computing accurate singular values using a mixed-precision one-sided Jacobi algorithm](https://arxiv.org/abs/2602.18134)
*Zhengbo Zhou,Françoise Tisseur,Marcus Webb*

Main category: math.NA

TL;DR: Mixed-precision preconditioned one-sided Jacobi SVD algorithm achieves higher accuracy than standard methods, especially for ill-conditioned matrices, with potential speed improvements if hardware/software bottlenecks are addressed.


<details>
  <summary>Details</summary>
Motivation: Standard SVD algorithms have limitations in accuracy, particularly for ill-conditioned matrices. The authors aim to develop a more accurate SVD algorithm using mixed-precision techniques that leverages low precision for preconditioner computation while maintaining high precision for the main computations.

Method: A mixed-precision preconditioned one-sided Jacobi algorithm that computes the preconditioner in low precision, applies it in high precision, and performs the singular value decomposition using one-sided Jacobi at working precision. Two approaches for constructing effective preconditioners are presented and analyzed.

Result: The algorithm achieves smaller relative forward error bounds for computed singular values than standard SVD algorithms. Numerical experiments show it outperforms LAPACK routines DGESVJ and DGEJSV, as well as MATLAB's svd function, particularly for ill-conditioned matrices. Timing tests show accelerated Jacobi convergence with the main bottleneck being a single high-precision matrix-matrix multiplication.

Conclusion: The mixed-precision preconditioned one-sided Jacobi algorithm is significantly more accurate than existing methods, especially for ill-conditioned matrices. With improved hardware/software support for the matrix-matrix multiplication bottleneck, it could achieve comparable speed to state-of-the-art preconditioned algorithms while maintaining superior accuracy.

Abstract: We present a relative forward error analysis of a mixed-precision preconditioned one-sided Jacobi algorithm, analogous to a two-sided version introduced in [N. J. Higham, F. Tisseur, M. Webb and Z. Zhou, SIAM J. Matrix Anal. Appl. 46 (2025), pp. 2423-2448], which uses low precision to compute the preconditioner, applies it in high precision, and computes the singular value decomposition using the one-sided Jacobi algorithm at working precision. Our analysis yields smaller relative forward error bounds for the computed singular values than those of standard SVD algorithms. We present and analyse two approaches for constructing effective preconditioners. Our numerical experiments support the theoretical results and demonstrate that our algorithm achieves smaller relative forward errors than the LAPACK routines $\texttt{DGESVJ}$ and $\texttt{DGEJSV}$, as well as the MATLAB function $\texttt{svd}$, particularly for ill-conditioned matrices. Timing tests show that our approach accelerates the convergence of the Jacobi iterations and that the dominant cost arises from a single high-precision matrix-matrix multiplication. With improved software or hardware support for this bottleneck, our algorithm would be faster than the LAPACK one-sided Jacobi algorithm $\texttt{DGESVJ}$ and comparable in speed to the state-of-the-art preconditioned one-sided Jacobi algorithm $\texttt{DGEJSV}$, but much more accurate.

</details>


### [9] [Theoretical insights on the residual transformation from bi-conjugate gradient into bi-conjugate residual via a smoothing scheme](https://arxiv.org/abs/2602.18159)
*Arisa Kawase,Kensuke Aihara*

Main category: math.NA

TL;DR: The paper provides theoretical proof that applying residual smoothing to Bi-CG yields an algorithm with the same bi-orthogonal properties as Bi-CR, establishing theoretical validity for transformations between these nonsymmetric linear system solvers.


<details>
  <summary>Details</summary>
Motivation: Previous work showed heuristic and experimental observations that residual smoothing can transform Bi-CG into Bi-CR, but lacked theoretical foundations. This study aims to provide rigorous theoretical analysis to validate these transformations.

Method: Theoretical analysis proving that applying residual smoothing technique to Bi-CG method results in an algorithm with identical bi-orthogonal properties to the original Bi-CR method. Also presents a more concise transformation algorithm.

Result: Proved theoretical equivalence of bi-orthogonal properties between smoothed Bi-CG and original Bi-CR. Provided a simplified transformation algorithm and demonstrated with numerical examples.

Conclusion: The study complements previous work by establishing theoretical validity for residual transformations between Bi-CG and Bi-CR methods, confirming that residual smoothing properly transforms one method into the other with equivalent mathematical properties.

Abstract: Bi-conjugate gradient (Bi-CG) and bi-conjugate residual (Bi-CR) methods are underlying iterative solvers for linear systems with nonsymmetric matrices. Residual smoothing is a standard technique for obtaining smooth convergence behavior of residual norms; additionally, it represents the transformation between iterative methods. For example, the residuals of the CR method can be obtained by applying a smoothing scheme to those of the CG method for symmetric linear systems. Based on this relationship, the transformation from Bi-CG residuals to Bi-CR residuals using a smoothing scheme was examined in our previous study [Kawase, A., Aihara, K.: Transformation from Bi-CG into Bi-CR Using a Residual Smoothing-like Scheme. AIP Conference Proceedings (2026)]; however, we only provided heuristic and experimental observations. In the present study, we provide a detailed discussion on the theoretical aspects of these transformations. Specifically, we prove that the resulting algorithm transformed from the Bi-CG method using the residual smoothing technique has the same bi-orthogonal properties as those of the original Bi-CR method. We also present a more concise transformation algorithm and its numerical example. These analyses complement our previous study and provide theoretical validity of the residual transformation between the Bi-CG and Bi-CR methods.

</details>


### [10] [A Parametric Finite Element Approach for an Anisotropic Multi-Phase Mullins-Sekerka Problem with Kinetic Undercooling](https://arxiv.org/abs/2602.18226)
*Tokuhiro Eto,Harald Garcke,Robert Nürnberg*

Main category: math.NA

TL;DR: A fully discrete unfitted finite element method for anisotropic multi-phase Mullins-Sekerka flow with kinetic undercooling, featuring unconditional stability and capable of modeling multiple ice crystals with junctions.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method for simulating anisotropic multi-phase Mullins-Sekerka problems with kinetic undercooling, which can model complex phenomena like multiple ice crystal evolution with junctions.

Method: Derived variational formulation, then introduced fully discrete unfitted finite element method where interface approximations are independent of bulk triangulations.

Result: Method is unconditionally stable and demonstrated through numerical examples, including successful modeling of multiple ice crystals with junctions.

Conclusion: The proposed unfitted finite element approach effectively simulates anisotropic multi-phase Mullins-Sekerka flow and can model complex multi-crystal evolution phenomena.

Abstract: We consider a sharp interface formulation for an anisotropic multi-phase Mullins-Sekerka problem with kinetic undercooling. The flow is characterized by a cluster of surfaces evolving such that the total surface energy plus a weighted sum of the volumes of the enclosed phases decreases in time. Upon deriving a suitable variational formulation, we introduce a fully discrete unfitted finite element method. In this approach, the approximations of the moving interfaces are independent of the triangulations used for the equations in the bulk. Our method can be shown to be unconditionally stable. Several numerical examples demonstrate the capabilities of the introduced method. In particular, it is demonstrated that the evolution of multiple ice crystals with junctions can be modeled using the proposed approach.

</details>


### [11] [Well-posedness and time stepping adaptivity for a class of collocation discretisations of time-fractional subdiffusion equations](https://arxiv.org/abs/2602.18404)
*Sebastian Franz,Natalia Kopteva*

Main category: math.NA

TL;DR: Analysis of collocation methods for time-fractional parabolic equations with Caputo derivatives, focusing on existence/uniqueness conditions and adaptive time stepping applications.


<details>
  <summary>Details</summary>
Motivation: To develop reliable numerical methods for time-fractional parabolic equations with Caputo derivatives, particularly addressing the challenges of existence/uniqueness proofs for collocation discretizations and enabling effective adaptive time stepping.

Method: Collocation methods that assume the Caputo derivative of the solution is piecewise-polynomial, with analysis for any order m≥0 and any collocation points. Includes investigation of a-posteriori error estimation and adaptive time stepping algorithms.

Result: Provides sufficient conditions for existence and uniqueness of collocation solutions for all orders m≥0 and any collocation point choices. Demonstrates applicability of these schemes for a-posteriori error estimation and adaptive time stepping.

Conclusion: Collocation methods are viable for time-fractional parabolic equations with rigorous existence/uniqueness guarantees, and they can be effectively combined with adaptive time stepping strategies for improved computational efficiency.

Abstract: Time-fractional parabolic equations with a Caputo time derivative of order $α\in(0,1)$ are discretised in time using collocation methods, which assume that the Caputo derivative of the computed solution is piecewise-polynomial. For such discretisations of any order $m\ge 0$, with any choice of collocation points, we give sufficient conditions for existence and uniqueness of collocation solutions. Furthermore, we investigate the applicability and performance of such schemes in the context of the a-posteriori error estimation and adaptive time stepping algorithms.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [12] [Invariant Manifolds for Capillary Waves and a Class of Quasilinear PDEs](https://arxiv.org/abs/2602.17844)
*Jalal Shatah,Chongchun Zeng*

Main category: math.AP

TL;DR: The paper establishes existence, uniqueness, and smoothness of local stable/unstable manifolds for quasilinear and fully nonlinear PDEs with regularity loss, with applications to water waves and other equations.


<details>
  <summary>Details</summary>
Motivation: While stable/unstable manifolds are well-understood for ODEs and semilinear PDEs, they remain challenging for quasilinear PDEs with regularity loss where linear parts don't provide sufficient smoothing. Existing invariant manifold theorems are often unavailable for such equations.

Method: Develops a framework based on suitable energy estimates for nonlinear PDEs. The approach focuses on establishing existence, uniqueness, and smoothness properties through rigorous analytical methods.

Result: Main results prove existence, uniqueness, and smoothness of local stable and unstable manifolds for nonlinear PDEs satisfying appropriate energy estimates. The framework applies to irrotational water waves with surface tension, nonlinear Schrödinger equations, nonlinear wave equations, MMT model, and gradient-type PDEs.

Conclusion: The paper provides a general framework for studying local stable/unstable manifolds in quasilinear and fully nonlinear PDEs with regularity loss, extending invariant manifold theory to previously inaccessible classes of equations including water waves and other physically important systems.

Abstract: This paper studies the local stable and unstable manifolds of equilibria for quasilinear and fully nonlinear PDEs. These manifolds are fundamental objects in the analysis of local dynamics. While their existence is well understood for ODEs, semilinear PDEs, and certain parabolic-type quasilinear PDEs, invariant manifold theorems are often unavailable for quasilinear PDEs whose nonlinearities involve a loss of regularity and whose linear parts do not provide sufficient smoothing.
  Our main results establish the existence, uniqueness, and smoothness of local stable and unstable manifolds for nonlinear PDEs that satisfy suitable energy estimates. With the main focus on irrotational water waves with surface tension, this framework applies to a broad class of PDEs, including nonlinear Schrödinger equations, nonlinear wave equations, and the MMT model, as well as to certain gradient-type PDEs.

</details>


### [13] [Liouville theorems for mixed local and nonlocal indefinite equations](https://arxiv.org/abs/2602.17944)
*Pengyan Wang,Leyun Wu*

Main category: math.AP

TL;DR: The paper studies qualitative properties of positive solutions to mixed local-nonlocal equations with indefinite nonlinearities, establishing maximum principles, monotonicity results, nonexistence theorems, and extending findings to parabolic settings with dual nonlocality.


<details>
  <summary>Details</summary>
Motivation: To understand the interaction between classical and fractional Laplacians in mixed local-nonlocal equations with indefinite nonlinearities, and to investigate qualitative properties of positive solutions in both elliptic and parabolic contexts.

Method: Establishes maximum principles and strict monotonicity for mixed elliptic operators; uses mollified first eigenfunction with sub-solution for contradiction arguments to prove nonexistence; adapts method of moving planes to handle distinct scaling behaviors of local and nonlocal terms; extends results to parabolic setting with Marchaud-type fractional time derivative and classical first-order derivative.

Result: Proves strict monotonicity along x₁-direction for mixed elliptic operators; derives nonexistence results for mixed operator (-Δ)^s - Δ; extends results to parabolic setting; obtains monotonicity and Liouville-type results without standard decay assumptions; provides framework applicable to broader class of mixed elliptic and parabolic problems.

Conclusion: The paper successfully develops analytical tools for mixed local-nonlocal equations, revealing new qualitative features under dual nonlocality and providing a framework that can be applied to broader classes of mixed elliptic and parabolic problems.

Abstract: We investigate the qualitative properties of positive solutions to mixed local-nonlocal equations with indefinite nonlinearities, emphasizing the interaction between classical and fractional Laplacians. We first establish maximum principles and prove strict monotonicity along the $x_1$-direction for mixed elliptic operators. By combining a mollified first eigenfunction with a suitable sub-solution, we derive nonexistence results for the mixed operator $ (-Δ)^s - Δ$ via a contradiction argument. These results are further extended to the parabolic setting, incorporating both the Marchaud-type fractional time derivative and the classical first-order derivative, revealing new qualitative features under dual nonlocality.
  A key aspect of our approach is a careful adaptation of the method of moving planes to the mixed local-nonlocal context. By addressing the distinct scaling behaviors of local and nonlocal terms, the method yields monotonicity and Liouville-type results without standard decay assumptions, and provides a framework potentially applicable to a broader class of mixed elliptic and parabolic problems.

</details>


### [14] [Weakly-singular formulation of the Fractional Laplacian operator](https://arxiv.org/abs/2602.17970)
*Oscar P. Bruno,Sabhrant Sachan*

Main category: math.AP

TL;DR: New formulation of fractional Laplacian as composition of classical Laplacian and weakly singular integral operator, enabling efficient numerical solution via integral equations.


<details>
  <summary>Details</summary>
Motivation: To develop a new mathematical formulation of the fractional Laplacian operator that facilitates efficient numerical implementation for solving fractional PDEs.

Method: Reformulates fractional Laplacian (-Δ)^s as composition of classical Laplace operator and weakly singular integral operator, reducing Dirichlet problem to weakly singular integral equation with volumetric and boundary integrals.

Result: Demonstrates high accuracy and computational efficiency through numerical examples, though full high-order algorithm details are deferred to subsequent paper.

Conclusion: The proposed formulation provides an effective framework for accurate and efficient numerical solution of fractional Laplacian problems via integral equation methods.

Abstract: This paper presents a new formulation of the fractional Laplacian operator $(-Δ)^s$ in $n$-dimensional space ($n \ge 1$). The proposed formulation expresses $(-Δ)^s$ as a composition of the classical Laplace differential operator and a weakly singular integral operator -- which can be used to reduce e.g. the Dirichlet problem for the fractional Laplacian to a weakly singular integral equation involving both volumetric and boundary integral operators. This reformulation is well suited for efficient and accurate numerical implementation. Although a full description of the associated high-order algorithm is deferred to a subsequent contribution, several numerical examples are included in this paper to demonstrate the high accuracy and computational efficiency achieved by the proposed approach.

</details>


### [15] [A Carleman Semi-Discrete Convexification Method Combined With Deep Learning for Electrical Impedance Tomography](https://arxiv.org/abs/2602.18001)
*Michael V. Klibanov,Kirill V. Golubnichiy,Benjamin Jiang*

Main category: math.AP

TL;DR: A semi-discrete Carleman estimate-based convexification method is developed to provide starting points for deep learning training, with applications in Electrical Impedance Tomography.


<details>
  <summary>Details</summary>
Motivation: To develop a globally convergent numerical method that provides good starting points for deep learning training without requiring a good initial guess, particularly for highly nonlinear inverse problems like Electrical Impedance Tomography.

Method: A semi-discrete version of Carleman estimate-based convexification method is developed, introducing the concept of h-strong convexity (where h is grid step size) to obtain a priori accuracy estimates for starting points used in deep learning training.

Result: The method demonstrates computational feasibility through numerical experiments on complicated media structures in Electrical Impedance Tomography, showing it can provide accurate starting points for deep learning procedures.

Conclusion: The proposed semi-discrete convexification method successfully provides globally convergent starting points for deep learning training in highly nonlinear inverse problems, with convergence independent of initial guesses and with provable accuracy bounds.

Abstract: In this paper, a new semi-discrete version of the Carleman estimate-based convexification globally convergent numerical method is developed. It is used for the delivery of the starting point for the training procedure of deep learning. An important feature of the continuous version of the convexification method is that its convergence to the true solution is independent on the availability of a good first guess about this solution. A new concept of the h-strong convexity is introduced, where h is the grid step size in the semi-discrete version of the convexification method. The h -strong convexity allows to obtain an a priori accuracy estimate of the starting point for the training step of the deep learning procedure. This approach is demonstrated for a highly nonlinear problem of Electrical Impedance Tomography. Results of numerical experiments for complicated media structures demonstrate the computational feasibility of this procedure.

</details>


### [16] [On Counterexamples to Interior $C^2$ Estimates for Monge-Ampère Type Equations](https://arxiv.org/abs/2602.18009)
*Cheuk Yan Fung*

Main category: math.AP

TL;DR: The paper demonstrates that a priori C² estimates fail for certain Monge-Ampère type equations involving gradient terms in dimensions n≥3, by constructing counterexamples where solutions' second derivatives blow up while right-hand sides remain uniformly C² bounded.


<details>
  <summary>Details</summary>
Motivation: To investigate the regularity theory for Monge-Ampère type equations with gradient terms, specifically whether a priori C² estimates exist for equations of the form det(D²u ± Du⊗Du) = f(x). This is important for understanding the well-posedness and regularity properties of these geometric PDEs.

Method: Modified Pogorelov's classic construction to build a sequence of explicit counterexamples. The solutions are given by z_ε(x₁,...,x_n) = (1+x₁²)(1+x₂²)(ε² + η²)^{α/2}, where η = √(x₃²+...+x_n²) and α = 2 - 2/n. As ε→0, these solutions demonstrate blow-up of second derivatives at the origin while the corresponding right-hand sides f_ε maintain uniform C² estimates.

Result: Successfully constructed counterexamples showing that a priori C² estimates fail for equations det(D²u ± Du⊗Du) = f(x) in dimensions n≥3. The sequence z_ε has second derivatives that blow up at the origin as ε→0, while the corresponding f_ε remain uniformly bounded in C² norm.

Conclusion: The absence of a priori C² estimates for these Monge-Ampère type equations with gradient terms in higher dimensions (n≥3) is established, highlighting fundamental differences from the classical Monge-Ampère equation and indicating that different analytical approaches are needed for studying regularity of such equations.

Abstract: We modify Pogorelov's classic construction to demonstrate the absence of a priori $C^2$ estimates for the equations $\det(D^2 u \pm Du \otimes Du) = f(x)$ in dimension $n \ge 3$. We construct a sequence of solutions $z_\varepsilon$ with second derivatives blowing up at the origin as $\varepsilon \rightarrow 0$, while the corresponding right-hand sides $f_\varepsilon$ admit uniform $C^2$ estimates. Specifically, the counterexamples are given by $z_\varepsilon(x_1, \dots, x_n) = (1+x_1^2)(1+x_2^2)(\varepsilon^2 + η^2)^{α/2},$ where $η= \sqrt{x_3^2 + \dots + x_n^2}$ and $α= 2 - \frac{2}{n}$.

</details>


### [17] [Nonlocal eigenvalue problems and superposition operators](https://arxiv.org/abs/2602.18035)
*Serena Dipierro,Edoardo Proietti Lippi,Caterina Sportelli,Enrico Valdinoci*

Main category: math.AP

TL;DR: The paper studies spectral theory of mixed local/nonlocal operators with lower-order terms, analyzing eigenvalue problems with "wrong sign" lower-order terms, focusing on convergence to classical cases, behavior in disconnected domains, and regularity theory.


<details>
  <summary>Details</summary>
Motivation: Motivated by analyzing superposition operators of mixed order with "wrong sign" lower-order terms that violate classical elliptic theory assumptions.

Method: Spectral analysis of mixed local/nonlocal operators with lower-order terms, studying eigenvalue equations, convergence properties, and behavior in disconnected domains.

Result: 1) Convergence to classical cases when RHS localizes, recovering simplicity and sign-definiteness; 2) In disconnected domains, first eigenfunctions must change sign (unlike classical case) and first eigenvalue of union is strictly smaller than components; 3) Examples showing first eigenvalue can be simple or non-simple in disconnected domains; 4) Underlying regularity theory.

Conclusion: Mixed local/nonlocal operators with "wrong sign" lower-order terms exhibit fundamentally different spectral behavior from classical elliptic operators, particularly in disconnected domains where first eigenfunctions must change sign and eigenvalues behave non-additively.

Abstract: We study the spectral theory of mixed local and nonlocal operators with lower-order terms in the right-hand side of the equation.
  This kind of problems is motivated by the analysis of superposition operators of mixed order and with the "wrong sign" of the lower-order terms with respect to the classical elliptic theory.
  Our results include:
  -convergence to classical cases when the right-hand side of the eigenvalye equations "localizes", recovering the simplicity and sign-definiteness of eigenfunctions in the limit;
  -a detailed analysis of disconnected domains, showing that, unlike the classical case, any eigenfunction associated with the first eigenvalue must change sign, and that the first eigenvalue of a union of disconnected domains is strictly smaller than that of its individual components;
  -examples in which the first eigenvalue is either simple or non-simple in disconnected domains;
  -a regularity theory that underpins these results.

</details>


### [18] [On the weighted logarithmic potential operator](https://arxiv.org/abs/2602.18138)
*T. V. Anoop,Jiya Rose Johnson*

Main category: math.AP

TL;DR: Study of weighted logarithmic potential operator eigenvalue problems: monotonicity/continuity of largest positive eigenvalue, reverse Faber-Krahn inequality under polarization, conditions for negative eigenvalues, and special properties in 2D when Δlog w is constant.


<details>
  <summary>Details</summary>
Motivation: To analyze spectral properties of weighted logarithmic potential operators, particularly how eigenvalues behave with respect to domain changes, weight functions, and establish geometric insights about eigenfunctions.

Method: Mathematical analysis of weighted eigenvalue problems using potential theory, harmonic analysis, and geometric methods including polarization techniques and representation formulas for eigenfunctions in special cases.

Result: Proved monotonicity/continuity of largest positive eigenvalue with respect to domain and weights; established reverse Faber-Krahn inequality; gave sufficient condition for negative eigenvalues; characterized when 0 can be eigenvalue in 2D; derived representation formula for eigenfunctions when log w is harmonic.

Conclusion: The weighted logarithmic potential operator exhibits rich spectral properties with connections to geometric function theory, providing insights into eigenfunction geometry and establishing important inequalities for eigenvalue optimization.

Abstract: For a bounded open set $Ω\subset \mathbb{R}^N$ with $N\geq 2$, and for positive continuous functions $w,g$ on $\overlineΩ$, we consider the weighted eigenvalue problem
  \begin{equation*}
  \mathcal{L}_{w} u =τgu,
  \end{equation*}
  where $\mathcal{L}_{w}$ is the weighted logarithmic potential operator on $L^2(Ω)$ as defined below:
  \begin{equation*}
  \mathcal{L}_{w} u(x)=\int_Ω\log\left(\frac{w(x)w(y)}{|x-y|}\right)u(y)dy.
  \end{equation*}
  We study the monotonicity and continuity of the largest positive eigenvalue $τ_{w,g}^+(Ω)$ with respect to $Ω$, $w$, and $g$. We also establish that $τ_{w,g}^+(Ω)$ satisfies a reverse Faber Krahn inequality under polarization. We provide a sufficient condition for the existence of a negative eigenvalue in terms of the weighted transfinite diameter of $Ω$, under the assumption that $\log w$ is superharmonic. For $Ω\subset \mathbb{R}^2$, if $Δ\log w $ is a constant $C$, we show that 0 can be an eigenvalue of $\mathcal{L}_{w}$ only when $C=\frac{2π}{|Ω|}$. For such domains, if $\log w$ is a harmonic function on $Ω$, we provide a representation formula for the eigenfunctions. Using this representation, we establish variants of the maximum principles that give some insight into the geometry of these eigenfunctions.

</details>


### [19] [Stability of the Shape for Circular Vortex Filaments under Non-Symmetric Perturbations](https://arxiv.org/abs/2602.18155)
*Masashi Aiki,Mitsuo Higaki*

Main category: math.AP

TL;DR: The paper proves nonlinear orbital stability of circular vortex filaments under non-symmetric perturbations, extending previous work by removing symmetry assumptions.


<details>
  <summary>Details</summary>
Motivation: Previous work by Aiki (2025) established stability under symmetric perturbations, but real-world perturbations are typically non-symmetric. Circular vortex filaments are known to be Lyapunov unstable due to translational mode growth, but their shape stability modulo translations/rotations remained an open question for non-symmetric perturbations.

Method: Uses the framework from Tani-Nishiyama (1997). Key innovation is a geometric stability lemma derived from conservation of vector fluid impulse, which constrains low-frequency modulations not covered by relative energy methods.

Result: Proves that circular vortex filaments remain globally stable in shape modulo spatial translations and rotations about the symmetry axis, even under non-symmetric perturbations.

Conclusion: The paper successfully extends stability results to non-symmetric perturbations using geometric conservation laws, providing a more complete understanding of vortex filament stability in realistic scenarios.

Abstract: We establish the nonlinear orbital stability of circular vortex filaments governed by the Localized Induction Equation (LIE) under non-symmetric perturbations, within the framework of [Tani-Nishiyama, 1997]. This result extends the first author's recent work [Aiki, 2025] by removing symmetry assumptions on perturbations. While the circular filaments are known to be Lyapunov unstable due to linear growth of the translational mode, we prove that their shape remains globally stable modulo spatial translations and rotations about the symmetry axis. The crucial ingredient is a geometric stability lemma derived from the conservation of vector fluid impulse, which constrains the low-frequency modulations that are not covered by the relative energy.

</details>


### [20] [Nonlocal-to-local $L^p$-convergence of convolution operators with singular, anisotropic kernels](https://arxiv.org/abs/2602.18183)
*Helmut Abels,Christoph Hurm,Patrik Knopf*

Main category: math.AP

TL;DR: Nonlocal convolution operators with singular anisotropic kernels converge to local differential operators with boundary conditions, providing justification for mathematical models when microscopic laws don't directly yield desired local operators.


<details>
  <summary>Details</summary>
Motivation: To establish nonlocal-to-local convergence for singular anisotropic kernels, providing a tool for physical justification of mathematical models when desired local differential operators cannot be directly derived from microscopic laws.

Method: Study nonlocal convolution-type operators with singular, possibly anisotropic kernels, and establish their convergence to local differential operators with natural boundary conditions as kernels concentrate at the origin in a suitable way.

Result: Substantially extends previous results by allowing kernels with stronger singularities (comparable to fractional Laplacians), anisotropic and non-localized kernels, and proves strong convergence in general L^p spaces with explicit convergence rates.

Conclusion: The convergence results provide a useful tool for physical justification of mathematical models, particularly when desired local differential operators cannot be directly derived from microscopic laws, with improved generality and quantitative results over previous work.

Abstract: We study nonlocal convolution-type operators with singular, possibly anisotropic kernels. Our main objective is to establish and quantify their nonlocal-to-local convergence to a local differential operator with natural boundary conditions, as the kernels concentrate at the origin in a suitable way. Such convergence results provide a useful tool for the physical justification of mathematical models, particularly in situations where the desired local differential operator cannot be directly derived from microscopic laws. The present work substantially extends previous results by allowing kernels with stronger singularities (comparable to those of fractional Laplacians), anisotropic and non-localized kernels, and by proving strong convergence in general $L^p$ spaces together with explicit convergence rates.

</details>


### [21] [On the shape of minimizers for the periodic nonlocal perimeter in $\mathbb{R}^2$](https://arxiv.org/abs/2602.18215)
*Renzo Bruera*

Main category: math.AP

TL;DR: Planar nonlocal Delaunay sets with constant nonlocal mean curvature are proven to be unstable unless they are straight bands, supporting the conjecture that straight bands are the minimizers in the nonlocal isoperimetric problem for large areas.


<details>
  <summary>Details</summary>
Motivation: The paper investigates the stability of planar nonlocal Delaunay sets (open sets with constant nonlocal mean curvature) that are periodic and even. This study is motivated by the nonlocal isoperimetric problem (nonlocal liquid drop problem) with prescribed area between parallel hyperplanes, aiming to understand the minimizers in the large area regime.

Method: The authors use bifurcation analysis and fine explicit computations to analyze planar nonlocal Delaunay sets. They prove that every sufficiently C^{1,β}-flat nonlocal Delaunay set in ℝ² that is not a straight band is unstable with respect to volume-preserving periodic variations.

Result: The main result shows that all sufficiently flat nonlocal Delaunay sets (except straight bands) are unstable. This supports the conjecture that, similar to the local case, straight bands are the only minimizers of the periodic nonlocal isoperimetric problem in the range of large areas.

Conclusion: The instability of non-straight-band nonlocal Delaunay sets provides strong evidence that straight bands are indeed the minimizers for the nonlocal isoperimetric problem with large prescribed areas, extending known results from the local case to the nonlocal setting.

Abstract: In this paper, we study planar nonlocal Delaunay sets. That is, open sets in $\mathbb{R}^2$ with constant nonlocal mean curvature that are periodic in $x_1$, and even in $x_1$ and in $x_2$. Using bifurcation analysis and fine explicit computations, we prove that every sufficiently $C^{1,β}$-flat nonlocal Delaunay set in $\mathbb{R}^2$ that is not a straight band is unstable with respect to volume-preserving periodic variations.
  Our results support the conjecture that, as in the local case, in the range of large areas, minimizers of the periodic nonlocal isoperimetric problem -- also known as the nonlocal liquid drop problem with prescribed area between two parallel hyperplanes -- are all straight bands.

</details>


### [22] [Periodic Delaunay cylinders with constant anisotropic nonlocal mean curvature](https://arxiv.org/abs/2602.18219)
*Francesc Alcover,Renzo Bruera*

Main category: math.AP

TL;DR: The paper proves existence and symmetry of periodic surfaces of revolution with constant anisotropic nonlocal mean curvature, generalizing Delaunay surfaces to anisotropic nonlocal setting.


<details>
  <summary>Details</summary>
Motivation: To extend classical Delaunay surfaces (constant mean curvature surfaces of revolution) to the anisotropic nonlocal setting, bridging classical differential geometry with modern nonlocal analysis and anisotropic materials science.

Method: Two main approaches: 1) Study periodic isoperimetric problem using rearrangement inequalities to prove Wulff inequality extension, yielding existence/symmetry of minimizers. 2) Use bifurcation theory to construct one-parameter family of Delaunay near-cylinders bifurcating from straight cylinders.

Result: Proves existence and symmetry properties of periodic surfaces with constant anisotropic nonlocal mean curvature for every given volume per period. Constructs explicit one-parameter family of Delaunay near-cylinders in R² with constant anisotropic mean curvature.

Conclusion: Successfully generalizes classical Delaunay surfaces to anisotropic nonlocal setting, extending previous results by Cabré, Csató, Mas, Fall, Solà-Morales, and Weth to anisotropic case, with stability analysis planned for future work.

Abstract: In this article we prove existence and symmetry properties of periodic surfaces of revolution with constant anisotropic nonlocal mean curvature, generalizing a classical result of Delaunay to the anisotropic nonlocal setting.
  First, by studying the corresponding periodic isoperimetric problem, under natural assumptions on the kernel, we use rearrangement inequalities to extend a periodic version of the Wulff inequality to the nonlocal setting. This leads to the existence and symmetry properties of minimizers for every given volume in each period, thus generalizing the results of Cabré, Csató, and Mas to the anisotropic case.
  Second, under the same hypotheses on the kernel, we prove the existence of a one-parameter family of Delaunay near-cylinders in $\mathbb{R}^2$ bifurcating from a straight cylinder and having each constant anisotropic mean curvature. This extends the results of Cabré, Fall, Solà-Morales, and Weth to the anisotropic case. The stability of these near-cylinders will be studied in a forthcoming paper.

</details>


### [23] [Overdetermined problems for the rotationally invariant Poisson equation in model manifolds](https://arxiv.org/abs/2602.18289)
*Antonio Greco,Marcello Lucia,Pieralberto Sicbaldi*

Main category: math.AP

TL;DR: The paper presents rigidity results for overdetermined Poisson problems in warped product manifolds, showing that under certain conditions on the prescribed functions, solutions must be radial and domains must be geodesic balls.


<details>
  <summary>Details</summary>
Motivation: To establish rigidity theorems for overdetermined boundary value problems in rotationally symmetric manifolds, extending classical Serrin-type results to more general geometric settings including warped product manifolds.

Method: The authors study two overdetermined problems: 1) interior problem in bounded domains containing the origin, 2) exterior Bernoulli problem in domains outside a geodesic ball. They use the method of moving planes and Pohozaev-type identities adapted to warped product geometry.

Result: The paper proves that under appropriate conditions on the prescribed functions f, φ, and κ, the solution u must be radial and the domain Ω must be a geodesic ball centered at the origin O. These results apply to Euclidean, hyperbolic, and spherical spaces.

Conclusion: The work establishes rigidity theorems for overdetermined Poisson problems in warped product manifolds, showing that symmetry of solutions and domains follows from overdetermined boundary conditions, generalizing classical results to broader geometric contexts.

Abstract: We present rigidity results for overdetermined problems associated to the rotationally invariant Poisson equation $-Δ_{g_\mathcal{M}} u = f(r)$ in a model manifold $\mathcal{M} = [0,S) \times_h \mathbb S^{N-1}$ with warping function $h$. The variable $r$ ranges in the interval $[0,S)$, whose endpoint $S$ is positive and possibly infinite. The first part of the paper deals with the problem \[
  \begin{array}{ll}
  -Δ_{g_\mathcal{M}} {u}=f(r) &\mbox{in $Ω$},
  u=\varphi(r) &\mbox{on $\partial Ω$},
  \frac{\partial u}{\partial ν} = κ(r) &\mbox{on $\partial Ω$},
  \end{array} \] where $Ω\subset \mathcal{M}$ is a bounded domain containing the point $O \in \mathcal{M}$ corresponding to $r = 0$, $ν$ is the exterior unit normal vector on $\partial Ω$, and $f$, $\varphi$, $κ$ are three prescribed functions.
  In the second part of the paper, we consider a similar overdetermined problem for the exterior Bernoulli problem in a domain $Ω\setminus \overline B_{R_0}(O)$, where $B_{R_0}(O)$ denotes the geodesic ball centered at $O$ with radius $R_0$, within the class of functions that vanish on $\partial B_{R_0}(O)$. In both cases, we give conditions on $f$, $\varphi$ and $κ$ implying that the solution $u$ is radial and $Ω$ is a geodesic ball centered at $O$. Our results apply in particular to the three space forms $\mathbb{R}^N$, $\mathbb{H}^N$ and $\mathbb{S}^N$.

</details>


### [24] [Application of uncertainty principles for decaying densities to the observability of the Schrödinger equation](https://arxiv.org/abs/2602.18371)
*Kévin Le Balc'h,Jiaqi Yu*

Main category: math.AP

TL;DR: Observability inequalities for Schrödinger equation on measurable sets with decaying density thickness, using quantitative uncertainty principles.


<details>
  <summary>Details</summary>
Motivation: To establish observability inequalities for the Schrödinger equation when the observation sets are not necessarily open or with positive Lebesgue measure, but rather measurable sets that are thick with respect to decaying densities.

Method: The proof relies on quantitative uncertainty principles adapted to decaying densities, building on established results by Shubin, Vakilian, Wolff, and Kovrijkine. These principles provide control over functions based on their concentration properties relative to decaying density measures.

Result: The authors prove observability inequalities for the Schrödinger equation posed in Euclidean space, where the observation sets are measurable sets that are thick with respect to decaying densities.

Conclusion: The work extends observability results to more general observation sets beyond classical open sets, using modern uncertainty principles to handle decaying density thickness conditions.

Abstract: In this article, we study the Schrödinger equation posed in the Euclidean space. We prove observability inequalities for measurable sets that are thick with respect to decaying densities. The proof relies on quantitative uncertainty principles adapted to decaying densities, notably those established by Shubin, Vakilian, Wolff, and Kovrijkine.

</details>


### [25] [Limiting Absorption Principle and Radiation Condition for the Fractional Helmholtz Equation](https://arxiv.org/abs/2602.18387)
*Dana Zilberberg,Fioralba Cakoni,Michael S. Vogelius*

Main category: math.AP

TL;DR: The paper develops scattering theory for fractional Helmholtz equations, introducing Sommerfeld radiation conditions, computing Green's functions, and proving existence/uniqueness of outgoing solutions for fractional Laplacian perturbations.


<details>
  <summary>Details</summary>
Motivation: To extend classical scattering theory for the Helmholtz equation to fractional orders, establishing rigorous foundations for scattering theory of fractional Helmholtz operators and creating a framework suitable for numerical implementation of nonlocal wave propagation models.

Method: Using contour integration and limiting absorption principle to compute outgoing free space Green's function for $(-Δ)^s-k^{2s}$; employing resolvent estimates and limiting absorption framework to establish existence/uniqueness; reformulating inhomogeneous media problems as Lippmann-Schwinger integral equations of Fredholm type.

Result: Explicit computation of outgoing Green's function for all $0<s<1$, any dimension, $k>0$; demonstration that asymptotic behavior matches rescaled classical Helmholtz solution; proof of existence/uniqueness for compactly supported and weighted sources; unique solvability of Lippmann-Schwinger equations away from discrete frequencies.

Conclusion: The paper provides rigorous mathematical foundation for scattering theory of fractional Helmholtz operators, justifies standard Sommerfeld conditions for fractional orders, and offers framework suitable for numerical implementation of nonlocal wave propagation models.

Abstract: We investigate elliptic fractional equations in the whole space, involving zero order perturbations of the fractional Laplacian $(-Δ)^s$, $0<s<1$. Our main objective is to determine appropriate radiation conditions at infinity that ensure existence and uniqueness of solutions to the fractional type Helmholtz equation. Extending classical scattering theory for the Helmholtz equation, we introduce and analyze suitable Sommerfeld type radiation conditions for fractional orders. A central contribution is the explicit computation of the outgoing free space Greens function for the operator $(-Δ)^s-k^{2s}$, for all $0<s<1$, any dimension and $k>0$, obtained via contour integration and a limiting absorption principle. We show that its asymptotic behavior at infinity coincides with a rescaled version of the classical Helmholtz fundamental solution, thereby justifying the standard Sommerfeld radiation condition for compactly supported sources. In addition, using resolvent estimates and a limiting absorption framework, we establish existence and uniqueness of outgoing solutions for compactly supported data, and for weighted sources. We further derive a convolution representation of the solution in terms of the outgoing fundamental solution. For inhomogeneous media with compactly supported perturbations, we reformulate the problem as a Lippmann Schwinger integral equation of Fredholm type and prove unique solvability away from a discrete set of frequencies. Our analysis provides a rigorous foundation for scattering theory of fractional Helmholtz operators and offers a framework suitable for numerical implementation of these nonlocal wave propagation models.

</details>


### [26] [Reconstruction algorithms for the fractional Laplacian and applications to inverse problems](https://arxiv.org/abs/2602.18407)
*Ethan Rinaldo,Mahamadi Warma*

Main category: math.AP

TL;DR: Two reconstruction schemes recover functions in ℝⁿ from local fractional Laplacian data, applied to inverse problems: Calderón-type potential recovery and space-fractional heat equation solution recovery.


<details>
  <summary>Details</summary>
Motivation: Develop methods to reconstruct functions from highly localized fractional Laplacian data, leveraging the weak Unique Continuation Property to address severely ill-posed inverse problems in nonlocal settings.

Method: Two reconstruction schemes using weak UCP for fractional Laplacian; new analytical tools: generalized weak Kelvin transform and fractional Robin-to-Robin map; applied to Calderón-type problem and space-fractional heat equation recovery.

Result: Successful recovery of functions from local data; numerical simulations demonstrate stability issues and severe ill-posedness of inverse problems.

Conclusion: Local fractional Laplacian data enables function reconstruction in entire space; new analytical tools facilitate solution of nonlocal inverse problems despite inherent ill-posedness.

Abstract: We introduce two reconstruction schemes that enable the recovery of a function in the entire Euclidean space $\mathbb{R}^n$ from local data $(u|_W, [(-Δ)^s u]|_W)$, where $W$ is an arbitrarily small nonempty open subset of $\mathbb R^n$ and $(-Δ)^s$ denotes the fractional Laplace operator of order $s\in (0,1)$. These procedures rely crucially on the weak Unique Continuation Property (UCP) for the fractional Laplacian. We apply these schemes to two distinct inverse problems. Following the seminal work from Ghosh et al., the first one concerns the recovery of a potential (Calderón-type problem) from the fractional Schrödinger equation under nonlocal Robin-type exterior conditions. The second one involves recovering the solution of the space-fractional heat equation in $\mathbb{R}^n$ from localized time-dependent measurements within a ball. To tackle these problems, we introduce new analytical tools such as a generalized weak Kelvin transform and a fractional Robin-to-Robin map. Finally, we provide numerical simulations for one of the reconstruction methods, illustrating the stability issues and the severe ill-posedness inherent to such inverse problems.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [Nonlinear Saturation of Ballooning Modes in Stellarators](https://arxiv.org/abs/2602.17964)
*X. Chu,S. C. Cowley,N. Ferraro,Y. Zhou,F. I. Parra*

Main category: physics.plasm-ph

TL;DR: Ballooning mode saturation in stellarators using flux tube approach, adapted for force balance issues in stellarator equilibria, showing saturated states exist for unstable profiles and reproducing features from W7X simulations.


<details>
  <summary>Details</summary>
Motivation: To investigate ballooning mode saturation in realistic stellarator configurations, addressing the challenge of force balance errors in stellarator equilibrium solvers that assume nested flux surfaces.

Method: Adapted flux tube approach from Ham et al. with variational method for calculating flux tube energy to overcome force error problems in stellarator numerical equilibria.

Result: Found saturated flux tube states crossing 10-20% of plasma minor radius for linearly ballooning unstable profiles; reproduced features from W7X nonlinear MHD simulations; discovered metastable states below marginal instability in compact stellarator.

Conclusion: Ballooning mode saturation is possible in stellarators, with evidence suggesting Edge-Localized-Mode-like explosive MHD behavior may occur in stellarator configurations, particularly in metastable states below marginal instability.

Abstract: Ballooning mode saturation is investigated in realistic stellarator configurations using the flux tube approach of Ham et. al. [1] [2]. The method is adapted to account for the lack of exact force balance in stellarator equilibrium solvers that assume existence of nested flux surfaces. A variational approach for calculating flux tube energy is developed to overcome this force error problem in stellarator numerical equilibria. Saturated (equilibrium) flux tube states that cross 10-20% of the plasma minor radius are shown to exist for linearly ballooning unstable profiles. It is shown that several features of the displaced flux tube structure in a full nonlinear MHD simulation of Wendelstein 7X are reproduced by our model. Saturated states are found in a compact stellarator equilibrium close but below the marginal ballooning linear instability, i.e. the unperturbed equilibrium is metastable. This suggests that Edge-Localized-Mode-like explosive MHD behavior may be possible in stellarators.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [28] [Spectral Homogenization of the Radiative Transfer Equation via Low-Rank Tensor Train Decomposition](https://arxiv.org/abs/2602.17708)
*Y. Sungtaek Ju*

Main category: physics.chem-ph

TL;DR: Radiative transfer equations have finite spectral complexity exploitable by tensor decomposition, achieving bounded TT ranks independent of spectral resolution and opacity sources.


<details>
  <summary>Details</summary>
Motivation: Radiative transfer in absorbing-scattering media requires solving transport equations across spectral domains with millions of molecular absorption lines. Line-by-line computation is too expensive, while existing approximations sacrifice spectral fidelity.

Method: Young-measure homogenization framework produces solution tensors that admit low-rank tensor-train (TT) decompositions. Using molecular line parameters from HITRAN database for H2O and CO2, and atomic plasma opacity from TOPS database.

Result: TT rank saturates at r=8 for molecular opacities and r=15 for atomic plasma opacity, independent of spectral resolution, scattering parameters, temperature, and pressure. QTT achieves sub-linear storage scaling. Homogenized approach achieves order of magnitude lower L2 error than correlated-k distribution at equal cost.

Conclusion: Spectral complexity of radiative transfer has finite effective rank exploitable by tensor decomposition, complementing existing spatial-angular compression methods. Rank boundedness is a property of the transport equation rather than any particular opacity source.

Abstract: Radiative transfer in absorbing-scattering media requires solving a transport equation across a spectral domain with 10^5 - 10^6 molecular absorption lines. Line-by-line (LBL) computation is prohibitively expensive, while existing approximations sacrifice spectral fidelity. We show that the Young-measure homogenization framework produces solution tensors I that admit low-rank tensor-train (TT) decompositions whose bond dimensions remain bounded as the spectral resolution Ns increases. Using molecular line parameters from the HITRAN database for H2O and CO2, we demonstrate that: (i) the TT rank saturates at r = 8 (at tolerance e = 10^-6) from Ns = 16 to 4096, independent of single-scattering albedo, Henyey-Greenstein asymmetry, temperature, and pressure; (ii) quantized tensor-train (QTT) representations achieve sub-linear storage scaling; (iii) in a controlled comparison using identical opacity data and transport solver, the homogenized approach achieves over an order of magnitude lower L2 error than the correlated-k distribution at equal cost; and (iv) for atomic plasma opacity (aluminum at 60 eV, TOPS database), the TT rank saturates at r = 15 with fundamentally different spectral structure (bound-bound and bound-free transitions spanning 12 decades of dynamic range), confirming that rank boundedness is a property of the transport equation rather than any particular opacity source. These results establish that the spectral complexity of radiative transfer has a finite effective rank exploitable by tensor decomposition, complementing the spatial-angular compression achieved by existing TT and dynamical low-rank approaches.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [29] [Machine-learning force-field models for dynamical simulations of metallic magnets](https://arxiv.org/abs/2602.18213)
*Gia-Wei Chern,Yunhao Fan,Sheng Zhang,Puhan Zhang*

Main category: cond-mat.str-el

TL;DR: ML force-field methods enable scalable and accurate simulations of nonequilibrium spin dynamics in itinerant electron magnets, revealing novel phenomena like anomalous coarsening and frozen phase separation.


<details>
  <summary>Details</summary>
Motivation: To develop scalable and transferable machine learning methods for Landau-Lifshitz-Gilbert simulations of itinerant electron magnets, overcoming computational limitations of traditional approaches while maintaining accuracy.

Method: Developed a deep neural network model based on locality principle to predict electron-mediated forces for spin dynamics. Used symmetry-aware descriptors constructed through group-theoretical approach to incorporate lattice and spin-rotation symmetries. Demonstrated framework using prototypical s-d exchange model.

Result: ML-enabled large-scale simulations revealed novel nonequilibrium phenomena: anomalous coarsening of tetrahedral spin order on triangular lattice and freezing of phase separation dynamics in lightly hole-doped, strong-coupling square-lattice systems.

Conclusion: ML force-field frameworks establish themselves as scalable, accurate, and versatile tools for modeling nonequilibrium spin dynamics in itinerant magnets, enabling discovery of new phenomena beyond traditional simulation capabilities.

Abstract: We review recent advances in machine learning (ML) force-field methods for Landau-Lifshitz-Gilbert (LLG) simulations of itinerant electron magnets, focusing on scalability and transferability. Built on the principle of locality, a deep neural network model is developed to efficiently and accurately predict the electron-mediated forces governing spin dynamics. Symmetry-aware descriptors constructed through a group-theoretical approach ensure rigorous incorporation of both lattice and spin-rotation symmetries. The framework is demonstrated using the prototypical s-d exchange model widely employed in spintronics. ML-enabled large-scale simulations reveal novel nonequilibrium phenomena, including anomalous coarsening of tetrahedral spin order on the triangular lattice and the freezing of phase separation dynamics in lightly hole-doped, strong-coupling square-lattice systems. These results establish ML force-field frameworks as scalable, accurate, and versatile tools for modeling nonequilibrium spin dynamics in itinerant magnets.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [30] [Electrodynamics of swift-electron momentum transfer to a large spherical nanoparticle](https://arxiv.org/abs/2602.18222)
*Jesús Castrejón-Figueroa,Jorge Luis Briseño-Gómez,Eduardo Enrique Viveros-Armas,José Ángel Castellanos-Reyes,Alejandro Reyes-Coronado*

Main category: cond-mat.mes-hall

TL;DR: Swift electrons transfer attractive linear momentum to spherical nanoparticles when using causal dielectric functions and fully converged multipolar calculations, contradicting earlier predictions of repulsive behavior.


<details>
  <summary>Details</summary>
Motivation: Previous studies on momentum transfer from swift electrons to nanoparticles used non-causal dielectric functions or insufficient numerical convergence, leading to incorrect predictions of repulsive behavior that don't match experimental observations.

Method: Developed analytical expressions and numerically efficient electrodynamic framework using fully causal, local dielectric response for isolated spherical nanoparticles. Applied to 50nm radius nanoparticles with explicit spectral resolution across full frequency domain using causal dielectric functions for aluminum and bismuth.

Result: When enforcing causality and full multipolar convergence, net transverse linear momentum transferred to spherical nanoparticles remains attractive toward electron trajectory for all nanoparticles considered, despite material-dependent sign changes in individual electric/magnetic contributions.

Conclusion: Results contradict earlier theoretical predictions of net repulsion, indicating additional physical mechanisms beyond isolated local description are needed to explain experimental observations. Establishes robust reference framework for momentum transfer calculations relevant to electron-beam nanomanipulation.

Abstract: Swift electrons from highly focused beams produced in aberration-corrected scanning transmission electron microscopes offer a powerful route for probing and manipulating matter at the nanoscale. Although linear momentum transfer from swift electrons to nanoparticles has been investigated theoretically and experimentally, subsequent analyzes revealed that several earlier predictions relied on non-causal dielectric functions or insufficient numerical convergence, leading to spurious sign reversals in the transferred momentum. Here, we derive analytical expressions and develop a numerically efficient electrodynamic framework to compute the linear momentum transferred from a swift electron to an isolated spherical nanoparticle described by a fully causal, local dielectric response. We apply our framework to large nanoparticles with 50 nm radius and explicitly resolve the spectral density of linear momentum transfer across the full frequency domain. Using causal dielectric functions for aluminum and bismuth, we analyze the role of electron velocity, impact parameter, and material-specific resonances. We find that, when causality and full multipolar convergence are enforced, the net transverse linear momentum transferred to spherical nanoparticles remains attractive toward the electron trajectory for all nanoparticles considered, despite the presence of material-dependent sign changes in individual electric and magnetic contributions. These results contrast with earlier theoretical predictions of net repulsive behavior and indicate that additional physical mechanisms beyond the present isolated, local description are required to account for experimentally observed repulsion. Our work establishes a robust reference framework for momentum transfer calculations and provides quantitative benchmarks relevant for electron-beam-based nanoscale manipulation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [31] [Recursive Sketched Interpolation: Efficient Hadamard Products of Tensor Trains](https://arxiv.org/abs/2602.17974)
*Zhaonan Meng,Yuehaw Khoo,Jiajia Li,E. Miles Stoudenmire*

Main category: quant-ph

TL;DR: RSI algorithm reduces Hadamard product computation cost from O(χ⁴) to O(χ³) for tensor-train format using randomized sketching and interpolative decomposition.


<details>
  <summary>Details</summary>
Motivation: Hadamard product of tensor-trains is fundamental for applications like TT-based function multiplication in nonlinear differential equations and convolutions, but conventional methods scale as O(χ⁴) with TT bond dimension χ, creating severe computational bottlenecks.

Method: Combines randomized tensor-train sketching with slice selection via interpolative decomposition to create Recursive Sketched Interpolation (RSI), a "scale product" algorithm that computes Hadamard product efficiently.

Result: RSI achieves computational cost of O(χ³), offering superior scalability compared to traditional methods while maintaining comparable accuracy across various TT scenarios. The method generalizes to compute Hadamard products of multiple TTs and other element-wise nonlinear mappings without increasing complexity beyond O(χ³).

Conclusion: RSI provides an efficient O(χ³) algorithm for Hadamard products in tensor-train format, overcoming the O(χ⁴) bottleneck of conventional methods, with applications to various nonlinear operations in tensor computations.

Abstract: The Hadamard product of two tensors in the tensor-train (TT) format is a fundamental operation across various applications, such as TT-based function multiplication for nonlinear differential equations or convolutions. However, conventional methods for computing this product typically scale as at least $\mathcal{O}(χ^4)$ with respect to the TT bond dimension (TT-rank) $χ$, creating a severe computational bottleneck in practice. By combining randomized tensor-train sketching with slice selection via interpolative decomposition, we introduce Recursive Sketched Interpolation (RSI), a ``scale product'' algorithm that computes the Hadamard product of TTs at a computational cost of $\mathcal{O}(χ^3)$. Benchmarks across various TT scenarios demonstrate that RSI offers superior scalability compared to traditional methods while maintaining comparable accuracy. We generalize RSI to compute more complex operations, including Hadamard products of multiple TTs and other element-wise nonlinear mappings, without increasing the complexity beyond $\mathcal{O}(χ^3)$.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [32] [Observer-robust energy condition verification for warp drive spacetimes](https://arxiv.org/abs/2602.18023)
*An T. Le*

Main category: gr-qc

TL;DR: warpax is a GPU-accelerated Python toolkit for comprehensive energy condition analysis of warp drive spacetimes using continuous observer optimization instead of discrete sampling, revealing that single-frame evaluation systematically underestimates violations.


<details>
  <summary>Details</summary>
Motivation: Existing tools for analyzing warp drive spacetimes only evaluate energy conditions for a finite sample of observer directions, which can miss violations and underestimate their severity. There's a need for more rigorous, continuous analysis that covers all possible observer frames.

Method: warpax uses gradient-based optimization over the timelike observer manifold (rapidity and boost direction) with Hawking-Ellis algebraic classification. For Type I stress-energy points (96% of grid points), it uses algebraic eigenvalue checks for exact determination. For non-Type I points, it provides rapidity-capped diagnostics. Stress-energy tensors are computed via forward-mode automatic differentiation from ADM metrics, eliminating truncation error.

Result: Analysis of five warp drive metrics shows that standard Eulerian-frame analysis misses violations at over 28% of grid points for dominant energy condition and over 15% for weak energy condition in Rodal metric. Even when violations are identified, observer optimization reveals severity can be orders of magnitude larger (e.g., Alcubierre weak energy condition shows ~90,000× larger violations at rapidity cap 5).

Conclusion: Single-frame evaluation systematically underestimates both the spatial extent and magnitude of energy condition violations in warp drive spacetimes. warpax provides a more rigorous analysis tool that is freely available as open-source software.

Abstract: We present \textbf{warpax}, an open-source, GPU-accelerated Python toolkit for observer-robust energy condition analysis of warp drive spacetimes. Existing tools evaluate energy conditions for a finite sample of observer directions; \textbf{warpax} replaces discrete sampling with continuous, gradient-based optimization over the timelike observer manifold (rapidity and boost direction), backed by Hawking--Ellis algebraic classification. At Type~I stress-energy points, which comprise ${>}\,96$\% of all grid points across the tested metrics, an algebraic eigenvalue check determines energy-condition satisfaction \emph{exactly}, independent of any observer search or rapidity cap. At non-Type~I points the optimizer provides rapidity-capped diagnostics. Stress-energy tensors are computed from the ADM metric via forward-mode automatic differentiation, eliminating finite-difference truncation error. Geodesic integration with tidal-force and blueshift analysis is also included.
  We analyze five warp drive metrics (Alcubierre, Lentz, Van~Den~Broeck, Natário, Rodal) and one warp shell metric (used primarily as a numerical stress test). For the Rodal metric, the standard Eulerian-frame analysis misses violations at over $28\%$ of grid points (dominant energy condition) and over $15\%$ (weak energy condition). Even where the Eulerian frame identifies the correct violation set, observer optimization reveals that violation severity can be orders of magnitude larger (e.g.\ Alcubierre weak energy condition: ${\sim}\,90{,}000\times$ at rapidity cap $ζ_{\max} = 5$, scaling as $e^{2ζ_{\max}}$). These results demonstrate that single-frame evaluation can systematically underestimate both the spatial extent and the magnitude of energy condition violations in warp drive spacetimes. \textbf{warpax} is freely available at https://github.com/anindex/warpax.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver](https://arxiv.org/abs/2602.18248)
*Pietro Sittoni,Emanuele Zangrando,Angelo A. Casulli,Nicola Guglielmi,Francesco Tudisco*

Main category: cs.LG

TL;DR: Neural-HSS: A parameter-efficient neural architecture based on Hierarchical Semi-Separable matrix structure that is provably data-efficient for solving PDEs, especially in low-data regimes.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods for PDEs require substantial computational costs for dataset generation and model training, limiting their application in critical domains despite available computing infrastructure.

Method: Introduces Neural-HSS architecture inspired by Green's functions structure for elliptic PDEs, built upon Hierarchical Semi-Separable (HSS) matrix structure. Theoretically analyzes exactness properties in low-data regimes and connects with Fourier neural operators and convolutional layers.

Result: Experimentally validated on 3D Poisson equation over 2 million grid points, showing superior data efficiency in low-data regimes compared to baselines. Demonstrated capability across diverse PDE domains including electromagnetism, fluid dynamics, and biology.

Conclusion: Neural-HSS provides a parameter-efficient, provably data-efficient architecture for solving broad classes of PDEs, addressing computational bottlenecks in deep learning-based PDE solvers while maintaining performance across diverse applications.

Abstract: Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regimes. We also investigate its connections with other architectural primitives, such as the Fourier neural operator layer and convolutional layers. We experimentally validate the data efficiency of Neural-HSS on the three-dimensional Poisson equation over a grid of two million points, demonstrating its superior ability to learn from data generated by elliptic PDEs in the low-data regime while outperforming baseline methods. Finally, we demonstrate its capability to learn from data arising from a broad class of PDEs in diverse domains, including electromagnetism, fluid dynamics, and biology.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [34] [Optimization of Higher-Order Harmonic Surface Tessellations for Additively Manufactured Air-to-Air Heat Exchangers](https://arxiv.org/abs/2602.17824)
*Patrick Adegbaye,Aigbe E. Awenlimobor,Justin An,Zhang Xiao,Jiajun Xu*

Main category: physics.flu-dyn

TL;DR: Optimized higher-order harmonic heat-transfer surfaces outperform conventional and TPMS designs in turbulent flow regimes, achieving up to 70% effectiveness increase with better pressure drop characteristics.


<details>
  <summary>Details</summary>
Motivation: Conventional air-to-air heat exchangers suffer from reduced effectiveness, high pressure losses, and increased pumping power. Nature-inspired geometries like TPMS structures enhance heat transfer but cause excessive pressure drops.

Method: Developed an optimization framework integrating analytical and numerical methods to create optimized higher-order harmonic heat-transfer surface tessellations. Conducted sensitivity analysis to evaluate thermal-hydraulic performance.

Result: Secondary surface modification yields up to 70% effectiveness increase (with associated pressure drop increase). Optimized second-order harmonic structure outperforms gyroid TPMS in turbulent flow (Re≥7000) with higher effectiveness and lower pressure drop.

Conclusion: Higher-order harmonic surfaces offer superior thermal-hydraulic performance in turbulent regimes compared to TPMS structures. Surface wave frequency is more critical than amplitude for optimization, providing a promising design approach for efficient heat exchangers.

Abstract: Air-to-air heat exchangers are vital for energy recovery and thermal management but often suffer from reduced effectiveness, high pressure losses, and increased pumping power in conventional designs. Advances in additive manufacturing have enabled nature-inspired geometries, such as lattice and triply periodic minimal surface (TPMS) structures, which enhance heat transfer through complex first-order surfaces but frequently cause excessive pressure drops. This study proposes an optimized higher-order harmonic heat-transfer surface tessellation developed through an optimization framework integrating analytical and numerical methods. The goal is to improve the overall thermal-hydraulic performance of the heat exchanger over a range of operating conditions. Results of sensitivity analysis show that secondary surface modification of this type can yield significant increase in the effectiveness reaching up to 70% although with associated increase in the pressure drop. The secondary surface wave frequency was found to be a more important control parameter than the amplitude in achieving high thermal-hydraulic performance. Additionally, we show that the optimized second order harmonic-type structure achieved relatively higher effectiveness and lower pressure-drop than the gyroid structure in the turbulent flow regime for Re>=7000. Although the gyroid TPMS structure had relatively higher effectiveness in the laminar and weakly turbulent flow regime, the associated pressure drop was found to be significantly higher than that of the harmonic-type structure.

</details>


### [35] [Impact of Structure-Preserving Discretizations on Compressible Wall-Bounded Turbulence of Thermally Perfect Gases](https://arxiv.org/abs/2602.17781)
*Alessandro Aiello,Andrea Palumbo,Carlo De Michele,Gennaro Coppola*

Main category: physics.flu-dyn

TL;DR: Direct numerical simulations of compressible turbulent channel flow at supersonic/hypersonic Mach numbers using CO₂ with thermally perfect gas model, assessing structure-preserving discretizations (entropy conservation, kinetic-energy preservation, thermodynamic consistency) and their impact on robustness, thermodynamic fluctuations, and turbulence statistics.


<details>
  <summary>Details</summary>
Motivation: To assess the role of structure-preserving discretizations of convective terms in high-enthalpy regimes, particularly focusing on entropy conservation, kinetic-energy preservation, and consistency with thermodynamic closure for reliable simulation of high-enthalpy compressible turbulence.

Method: Direct numerical simulations of compressible turbulent channel flow at supersonic and hypersonic Mach numbers using thermally perfect gas model for CO₂, with comparative analysis of various formulations examining their impact on robustness, thermodynamic fluctuations, and turbulence statistics.

Result: Differences among formulations originate primarily in treatment of thermodynamic variables and progressively influence dynamical fields as compressibility effects intensify; coupling between entropy consistency and pressure discretization affects Reynolds stresses and mean flow properties in high-speed regimes.

Conclusion: Consistency between numerical formulation and thermodynamic model contributes significantly to reliable simulation of high-enthalpy compressible turbulence; study systematically assesses entropy-conservative discretizations for thermally perfect gases in wall-bounded flows and examines their impact on thermodynamic-dynamic coupling at high Mach numbers.

Abstract: Direct numerical simulations of compressible turbulent channel flow at supersonic and hypersonic Mach numbers are performed using a thermally perfect gas model for CO$_2$. The objective is to assess the role of structure-preserving discretizations of the convective terms in high-enthalpy regimes, with particular emphasis on entropy conservation, kinetic-energy preservation, and consistency with the thermodynamic closure.
  The comparative analysis of various formulations examines their impact on robustness, thermodynamic fluctuations, and turbulence statistics across a range of Mach numbers. Differences among formulations are found to originate primarily in the treatment of thermodynamic variables and progressively influence the dynamical fields as compressibility effects intensify. In particular, the coupling between entropy consistency and pressure discretization is shown to affect Reynolds stresses and mean flow properties in high-speed regimes.
  Overall, the results indicate that consistency between the numerical formulation and the thermodynamic model contributes significantly to the reliable simulation of high-enthalpy compressible turbulence. The study systematically assesses entropy-conservative discretizations for thermally perfect gases in wall-bounded flows and examines their impact on thermodynamic-dynamic coupling at high Mach numbers.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [36] [Model Error Embedding with Orthogonal Gaussian Processes](https://arxiv.org/abs/2602.17923)
*Mridula Kuppa,Khachik Sargsyan,Marco Panesi,Habib N. Najm*

Main category: stat.ME

TL;DR: The paper presents an embedded Gaussian process framework to estimate and separate model error from model parameters, enabling meaningful stand-alone predictions through orthogonal constraints and dimensionality reduction techniques.


<details>
  <summary>Details</summary>
Motivation: Computational models often have simplifying assumptions that introduce model error, leading to predictive errors. There's a need to estimate model-error representations while separating them from model parameters to ensure meaningful stand-alone model predictions.

Method: Uses weight-space Gaussian process representation to capture model-error spatiotemporal correlations, extends orthogonal GP method to embedded model-error setting with appropriate orthogonality constraints, and employs likelihood-informed subspace method to address increased dimensionality from GP representation.

Result: The framework effectively corrects model predictions to match data trends in linear and non-linear examples. Extrapolation beyond training data recovers prior predictive distribution, and orthogonality constraints lead to meaningful stand-alone predictions with nearly uncorrelated posteriors between model and model-error parameters.

Conclusion: The proposed embedded model error framework with orthogonal GP constraints successfully separates model parameters from model-error parameters, enabling meaningful stand-alone predictions while maintaining computational efficiency through dimensionality reduction techniques.

Abstract: Computational models of complex physical systems often rely on simplifying assumptions which inevitably introduce model error, with consequent predictive errors. Given data on model observables, the estimation of parameterized model-error representations, along with other model parameters, would be ideally done while separating the contributions of each of the two sets of parameters, in order to ensure meaningful stand-alone model predictions. This work builds an embedded model error framework using a weight-space representation of Gaussian processes (GPs) to flexibly capture model-error spatiotemporal correlations and enable inference with GP-embedding in non-linear models. To disambiguate model and model-error/bias parameters, we extend an existing orthogonal GP method to the embedded model-error setting and derive appropriate orthogonality constraints. To address the increased dimensionality introduced by the GP representation, we employ the likelihood-informed subspace method. The construction is demonstrated on linear and non-linear examples, where it effectively corrects model predictions to match data trends. Extrapolation beyond the training data recovers the prior predictive distribution, and the orthogonality constraints lead to meaningful stand-alone model predictions and nearly uncorrelated posteriors between model and model-error parameters.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [37] [A stochastic Schauder-Tychonoff type theorem and its applications](https://arxiv.org/abs/2602.17285)
*Erika Hausenblas,Ankit Kumar,Jonas M. Tölle*

Main category: math.PR

TL;DR: Introduces and verifies a stochastic version of the Schauder-Tychonoff fixed-point theorem, then applies it to prove existence for nonlinear stochastic diffusion equations with non-Lipschitz perturbations.


<details>
  <summary>Details</summary>
Motivation: The standard Schauder-Tychonoff theorem is used to prove existence for deterministic nonlinear PDEs, but there's a need for a stochastic version to handle stochastic PDEs with non-Lipschitz perturbations where standard methods fail.

Method: Develops a stochastic variant of the Schauder-Tychonoff fixed-point theorem, then applies this new theorem to study nonlinear stochastic diffusion equations with non-Lipschitz perturbations.

Result: Successfully establishes a stochastic Schauder-Tychonoff theorem and uses it to prove existence of solutions for the considered class of nonlinear stochastic diffusion equations with non-Lipschitz perturbations.

Conclusion: The stochastic Schauder-Tychonoff theorem provides a powerful tool for proving existence of solutions to stochastic PDEs with non-Lipschitz nonlinearities, extending the applicability of fixed-point methods to stochastic settings.

Abstract: One standard way to prove existence for deterministic, highly nonlinear PDEs is to use the Schauder-Tychonoff fixed-point theorem. In what follows, we introduce and verify a stochastic variant of the Schauder-Tychonoff theorem. We apply our existence result to nonlinear stochastic diffusion equations with non-Lipschitz perturbations

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [38] [Inelastic Constitutive Kolmogorov-Arnold Networks: A generalized framework for automated discovery of interpretable inelastic material models](https://arxiv.org/abs/2602.17750)
*Chenyi Ji,Kian P. Abdolazizi,Hagen Holthusen,Christian J. Cyron,Kevin Linka*

Main category: cond-mat.mtrl-sci

TL;DR: iCKANs (inelastic Constitutive Kolmogorov-Arnold Networks) is a novel neural network architecture that automatically discovers symbolic constitutive laws for both elastic and inelastic material behavior from experimental data.


<details>
  <summary>Details</summary>
Motivation: Traditional constitutive law identification in solid mechanics is challenging, especially for complex inelastic behaviors. Machine learning offers promising approaches but needs architectures that can discover interpretable symbolic laws rather than just black-box predictions.

Method: Developed iCKANs architecture based on Kolmogorov-Arnold networks that can automatically discover symbolic constitutive laws in closed mathematical form from material testing data. The method can process both mechanical data and additional information like temperature effects.

Result: Successfully applied to synthetic data and experimental data of viscoelastic polymers VHB 4910 and VHB 4905. iCKANs accurately captured complex viscoelastic behavior while maintaining physical interpretability through symbolic representations.

Conclusion: iCKANs provide a powerful tool for automated discovery of constitutive laws that can handle both elastic and inelastic behavior, preserve interpretability, and incorporate additional material information, enabling future applications to study processing and service condition effects.

Abstract: A key problem of solid mechanics is the identification of the constitutive law of a material, that is, the relation between strain and stress. Machine learning has lead to considerable advances in this field lately. Here we introduce inelastic Constitutive Kolmogorov-Arnold Networks (iCKANs). This novel artificial neural network architecture can discover in an automated manner symbolic constitutive laws describing both the elastic and inelastic behavior of materials. That is, it can translate data from material testing into corresponding elastic and inelastic potential functions in closed mathematical form. We demonstrate the advantages of iCKANs using both synthetic data and experimental data of the viscoelastic polymer materials VHB 4910 and VHB 4905. The results demonstrate that iCKANs accurately capture complex viscoelastic behavior while preserving physical interpretability. It is a particular strength of iCKANs that they can process not only mechanical data but also arbitrary additional information available about a material (e.g., about temperature-dependent behavior). This makes iCKANs a powerful tool to discover in the future also how specific processing or service conditions affect the properties of materials.

</details>


### [39] [Lattice and Orbital-Resolved Fermiology of Metallenes](https://arxiv.org/abs/2602.18052)
*Kameyab Raza Abidi,Mohammad Bagheri,Pekka Koskinen*

Main category: cond-mat.mtrl-sci

TL;DR: Systematic DFT study of 45 elemental metallenes reveals how lattice type and buckling shape Fermi surfaces, with a new "pocketness" score for targeted ARPES tests and applications.


<details>
  <summary>Details</summary>
Motivation: Despite growing interest in atomically thin metallenes, there's a lack of comprehensive understanding of their electronic structures and Fermi surfaces, which is crucial for applications in plasmonics, catalysis, and quantum optics.

Method: Density-functional theory study of 45 elemental metallenes in six monolayer lattices (honeycomb, square, hexagonal, and their buckled forms).

Result: Lattice type determines Fermi-line shape/placement; buckling modifies Fermi surfaces by shortening segments and manipulating pockets. Electronic configuration dictates orbital dominance at Fermi level. Developed "pocketness" score from four descriptors combining structural and electronic properties.

Conclusion: The systematic analysis provides predictive insights for ARPES experiments, Lifshitz transitions, and transport/device applications, enabling rational design of metallene properties.

Abstract: Atomically thin metallenes have emerged as a new member of the two-dimensional (2D) materials family. Recent experimental realization of metallenes in the Ångström limit has further intensified interest in this class of 2D materials. However, achieving sub-atomic insight into them demands the most detailed and systematic characterization of their electronic structure. Such understanding is essential for the rational design and exploitation of their properties in plasmonics, catalysis, and quantum optics. Existing electronic-structure studies are either scattered or focus on a few selected systems, and a comprehensive view of their band structures and Fermi surfaces remains missing. Here, we address this gap by studying 45 elemental metallenes in six monolayer lattices (honeycomb, square, hexagonal, and their buckled forms) using density-functional theory. We found that lattice type primarily fixes the shape and radial placement of the Fermi-lines, while out-of-plane buckling introduces controlled modifications: it shortens long straight Fermi-line segments, and occasionally creates, removes, or merges small Fermi-line pockets. The electronic configuration determines which orbital type dominates the Fermi level. We summarized Fermiology using a single score for each element, termed pocketness, derived from four descriptors that combine element properties (symmetry, coordination) with electronic characteristics (dispersion, Fermi-surface topology). This score enables targeted angle-resolved photoemission spectroscopy (ARPES) tests, controlled Lifshitz transitions, and provides a predictive basis for transport and device applications.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [40] [PINEAPPLE: Physics-Informed Neuro-Evolution Algorithm for Prognostic Parameter Inference in Lithium-Ion Battery Electrodes](https://arxiv.org/abs/2602.18042)
*Karkulali Pugalenthi,Jian Cheng Wong,Qizheng Yang,Pao-Hsiung Chiu,My Ha Dao,Nagarajan Raghavan,Chinchun Ooi*

Main category: cs.CE

TL;DR: PINEAPPLE is a physics-informed neuro-evolution framework for rapid, non-destructive parameter inference in lithium-ion batteries using voltage-time data.


<details>
  <summary>Details</summary>
Motivation: Accurate real-time estimation of internal battery states is critical for predicting degradation, optimizing usage, and extending lifespan, but current methods lack speed, scalability, and interpretability.

Method: Integrates physics-informed neural networks (PINNs) with evolutionary search algorithm to infer parameters from voltage-time discharge curves without customized degradation physics heuristics.

Result: Achieves <0.1% test error with order-of-magnitude speed-up over conventional solvers, recovers cycle-dependent evolution of Li-ion diffusion coefficients across multiple batteries from CALCE repository.

Conclusion: PINEAPPLE enables computationally efficient, real-time parameter estimation for non-destructive physics-based characterization of battery variability, promising improved battery management systems.

Abstract: Accurate, real-time, yet non-destructive estimation of internal states in lithium-ion batteries is critical for predicting degradation, optimizing usage strategies, and extending operational lifespan. Here, we introduce PINEAPPLE (Physics-Informed Neuro-Evolution Algorithm for Prognostic Parameter inference in Lithium-ion battery Electrodes), a novel framework that integrates physics-informed neural networks (PINNs) with an evolutionary search algorithm to enable rapid, scalable, and interpretable parameter inference with potential for application to next-generation batteries. The meta-learned PINN utilizes fundamental physics principles to achieve accurate zero-shot prediction of electrode behavior with test errors below 0.1$\%$ while maintaining an order-of-magnitude speed-up over conventional solvers. PINEAPPLE demonstrates robust parameter inference solely from voltage-time discharge curves across multiple batteries from the open-source CALCE repository, recovering the evolution of key internal state parameters such as Li-ion diffusion coefficients across usage cycles. Notably, the inferred cycle-dependent evolution of these parameters exhibit consistent trends across different batteries without any customized degradation physics-embedded heuristic, highlighting the effective regularizing effect and robustness that can be conferred through incorporation of fundamental physics in PINEAPPLE. By enabling computationally efficient, real-time parameter estimation, PINEAPPLE offers a promising route towards the non-destructive, physics-based characterization of inter-cell and intra-cell variability of battery modules and battery packs, thereby unlocking new opportunities for downstream on-the-fly needs in next-generation battery management systems such as individual cell-scale state-of-health diagnostics.

</details>


### [41] [Comparative study of different quadrature methods for cut elements](https://arxiv.org/abs/2602.18130)
*Michael Loibl,Guilherme H. Teixeira,Teoman Toprak,Irina Shishkina,Chen Miao,Josef Kiendl,Florian Kummer,Benjamin Marussig*

Main category: cs.CE

TL;DR: Review and benchmarking of open-source codes for quadrature of cut elements in non-boundary-fitted FEM meshes.


<details>
  <summary>Details</summary>
Motivation: Quadrature of cut elements is essential for FEM with non-boundary-fitted meshes, requiring efficiency, accuracy, and robustness. Many approaches exist with open-source implementations, but comprehensive comparison is needed.

Method: Review existing open-source codes and methods for cut element quadrature. Develop benchmarking examples for 2D/3D geometries with implicit/explicit boundary descriptions. Test efficiency, accuracy, versatility, robustness, and parameter influence.

Result: Detailed comparison of codes through benchmarking. Analysis of how input parameters controlling quadrature order affect actual integration error. Published tests in open-source repository.

Conclusion: Benchmarking provides conclusive comparison of cut element quadrature codes and serves as valuable tool for future development. All tests are publicly available for reproducibility.

Abstract: The quadrature of cut elements is crucial for all Finite Element Methods that do not apply boundary-fitted meshes. It should be efficient, accurate, and robust. Various approaches balancing these requirements have been published, with some available as open-source implementations. This work reviews these open-sources codes and the methods used. Furthermore, benchmarking examples are developed for 2D and 3D geometries. Implicit and explicit boundary descriptions are available for all models. The different examples test the efficiency, accuracy, versatility, and robustness of the codes. Special focus is set on the influence of the input parameter, which controls the desired quadrature order, on the actual integration error. A detailed comparison of the discussed codes is carried out. The benchmarking allows a conclusive comparison and presents a valuable tool for future code development. All tests are published in an accompanying open-source repository.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [42] [Rigorous electromagnetic quasinormal-mode method made easy for users](https://arxiv.org/abs/2602.18067)
*Tong Wu,Philippe Lalanne*

Main category: physics.optics

TL;DR: Researchers develop a simplified quasinormal mode (QNM) approach that makes QNM theory more accessible to electromagnetism researchers who typically use real-frequency or time-domain methods.


<details>
  <summary>Details</summary>
Motivation: Despite QNM methods offering physical insights and computational efficiency for electromagnetic resonator analysis, many researchers still prefer real-frequency or time-domain approaches due to perceived complexity and mathematical barriers of QNM theory.

Method: Combine numerical techniques with accurate approximations to simplify QNM computation and enable ultrafast reconstructions using QNM expansions, creating an approach accessible to users familiar with real-frequency methods.

Result: Developed an open-source package implemented within widely-used commercial photonics software, demonstrating practical accessibility of the simplified QNM approach.

Conclusion: The work bridges the gap between advanced QNM theory and practical application, making QNM methods more accessible to the broader electromagnetism research community.

Abstract: Full-wave numerical methods based on quasinormal modes (QNMs) offer valuable physical insights and computational efficiency for analyzing electromagnetic resonators. However, despite their advantages, many researchers in electromagnetism continue to favor real-frequency domain or time-domain approaches, often using finite element or finite-difference time-domain methods. This preference stems from various factors, including the perception that QNM theory is still developing or requires advanced mathematical tools from complex analysis. In this work, we combine numerical techniques with accurate ap-proximations to simplify the computation of QNMs and enable ultrafast reconstructions us-ing QNM expansions. The result is a new approach that is straightforwardly accessible to users familiar with real-frequency methods. We demonstrate the practicality of our ap-proach through an open-source package [Doi: 10.5281/zenodo.18708748] implemented within a widely-used commercial photonics software.

</details>


### [43] [Pole-Expansion of the T-Matrix Based on a Matrix-Valued AAA-Algorithm](https://arxiv.org/abs/2602.18414)
*Jan David Fischbach,Fridtjof Betz,Lukas Rebholz,Puneet Garg,Kristina Frizyuk,Felix Binkowski,Sven Burger,Martin Hammerschmidt,Carsten Rockstuhl*

Main category: physics.optics

TL;DR: Pole-expansion technique for efficient frequency-domain representation of T-matrices using adaptive rational approximation.


<details>
  <summary>Details</summary>
Motivation: Traditional frequency-sampling of T-matrices is computationally expensive, memory-intensive, and lacks physical interpretability of spectral features.

Method: Use matrix-valued adaptive Antoulas-Anderson (AAA) algorithm for rational approximation to compute pole-expansion with minimal computational cost using few direct evaluations.

Result: Demonstrated benefits across various scatterers including semi-analytically accessible scatterers and quasi-dual bound states in the continuum.

Conclusion: Pole-expansion provides efficient, interpretable T-matrix representation with open-source tools for community adoption.

Abstract: The transition matrix (T-matrix) is a complete description of an object's linear scattering response. As such, it has found wide adoption for the theoretical and computational description of multiple-scattering phenomena. In its original form, the T-matrix describes the interaction of a scatterer with a monochromatic source. In practice, however, information about the T-matrix is usually needed in an extended spectral domain. To access the frequency-dispersion, one might naively sample T-matrices over a finely resolved set of discrete frequencies and store one T-matrix per frequency. This approach has multiple drawbacks: it is computationally expensive, requires excessive memory, and it disregards the physical origin of the spectral features, weakening physical interpretability. To overcome these major limitations, we leverage a pole-expansion technique to represent the T-matrix with arbitrary frequency resolution within a selected frequency domain via a set of resonant contributions. A matrix-valued variant of the recently established adaptive Antoulas-Anderson (AAA) algorithm for rational approximation enables us to compute the pole-expansion at minimal computational cost using only a small number of direct evaluations. We demonstrate the benefits of such a representation with examples ranging from semi-analytically accessible scatterers to quasi-dual bound states in the continuum. To allow the wider community to capitalize on these findings, we provide open-source tools to perform the presented pole-expansion of the T-matrix.

</details>
