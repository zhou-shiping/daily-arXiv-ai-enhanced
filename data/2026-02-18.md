<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 8]
- [math.AP](#math.AP) [Total: 11]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 4]
- [gr-qc](#gr-qc) [Total: 1]
- [math.PR](#math.PR) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Certified Reduced-Order Surrogates and Stability Margins in Viscous Incompressible Flow and Fluid--Structure Interaction](https://arxiv.org/abs/2602.15059)
*Chandrasekhar Gokavarapu,Naveen Kumar Kakumanu,Anjali Datla,Githa Harshitha Noolu*

Main category: math.NA

TL;DR: The paper develops certified reduced-order models (ROMs) for incompressible Navier-Stokes equations with computable energy certificates, a posteriori error bounds, transition indicators, and stability margins for fluid-structure interaction.


<details>
  <summary>Details</summary>
Motivation: To create reliable reduced-order models for fluid dynamics that provide mathematically rigorous guarantees on stability, error bounds, and regime-of-validity tests, addressing the need for certified computational methods in complex fluid systems.

Method: Construct ROMs constrained to satisfy certified energy inequalities, derive computable residual functionals, prove a posteriori error bounds with explicit constants, develop transition indicators from energy/enstrophy budgets, and derive stability margins for fluid-structure interaction models.

Result: Provides ROMs with global-in-time bounded energy, computable regime-of-validity tests, explicit a posteriori error bounds, transition thresholds that preclude or certify transient growth, and explicit stability constraints for fluid-structure interaction parameters.

Conclusion: The framework delivers mathematically certified ROMs with computable guarantees for Navier-Stokes equations, enabling reliable reduced-order modeling with explicit error bounds, transition indicators, and stability margins for complex fluid systems.

Abstract: Let $(u,p)$ solve the incompressible Navier--Stokes equations in a regime in which an energy inequality is available and each constant in that inequality is computable from declared data. We construct a reduced-order model $u_n$ constrained so that its discrete evolution satisfies a certified energy inequality. This certificate yields global-in-time boundedness of the ROM energy and a regime-of-validity test that fails when a stated hypothesis fails.
  It follows that one can attach a computable residual functional $\mathcal{R}_n$ to the ROM trajectory. We prove an a posteriori bound of the form \[ \norm{u-u_n}_{\mathsf{X}(0,T)} \le C(\text{declared data})\,\mathcal{R}_n, \] with $C$ explicit and with $\mathcal{R}_n$ computed from the ROM and the discretization operators. Conversely, if the certificate constraint is relaxed, the bound can fail even for stable full-order dynamics, by an explicit instability mechanism recorded in the text.
  We then derive transition indicators from rigorous energy and enstrophy budgets in simplified geometries. Each indicator is an inequality involving declared quantities such as forcing norms, viscosity, Poincaré-type constants, and a computable resolvent surrogate. These inequalities provide thresholds that preclude transition, or else certify the presence of transient growth beyond a stated level.
  Finally, for a class of fluid--structure interaction models, we identify a parameter regime that implies existence and uniqueness of weak solutions. We derive discrete coupled energy estimates that produce computable stability margins. These margins yield explicit constraints on time step and mesh parameters. They are stated as inequalities with constants determined by fluid viscosity, structure stiffness, density ratios, and interface trace bounds.

</details>


### [2] [A Unified Benchmark of Physics-Informed Neural Networks and Kolmogorov-Arnold Networks for Ordinary and Partial Differential Equations](https://arxiv.org/abs/2602.15068)
*Salvador K. Dzimah,Sonia Rubio Herranz,Fernando Carlos Lopez Hernandez,Antonio López Montes*

Main category: math.NA

TL;DR: PIKANs (Physics-Informed Kolmogorov-Arnold Networks) outperform traditional MLP-based PINNs in solving differential equations, offering better accuracy, faster convergence, and superior gradient estimates.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs using MLPs have limitations with fixed activation functions and global approximation biases, making them less effective for problems with oscillatory behavior, multiscale dynamics, or sharp gradients. KANs offer a functionally adaptive architecture that could overcome these limitations.

Method: Systematic comparison between MLP-based PINNs and KAN-based PIKANs using identical physics-informed formulations and matched parameter budgets to isolate architectural effects. Both models evaluated across representative ODEs and PDEs with known analytical solutions for direct accuracy assessment.

Result: PIKANs consistently achieve more accurate solutions, converge in fewer iterations, and yield superior gradient estimates compared to MLP-based PINNs. The architecture demonstrates clear advantages for physics-informed learning tasks.

Conclusion: KAN-based architectures like PIKANs represent a promising next-generation approach for scientific machine learning, offering rigorous evidence to guide model selection in differential equation solving and highlighting their potential for physics-informed learning applications.

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful mesh-free framework for solving ordinary and partial differential equations by embedding the governing physical laws directly into the loss function. However, their classical formulation relies on multilayer perceptrons (MLPs), whose fixed activation functions and global approximation biases limit performance in problems with oscillatory behavior, multiscale dynamics, or sharp gradients. In parallel, Kolmogorov-Arnold Networks (KANs) have been introduced as a functionally adaptive architecture based on learnable univariate transformations along each edge, providing richer local approximations and improved expressivity. This work presents a systematic and controlled comparison between standard MLP-based PINNs and their KAN-based counterparts, Physics-Informed Kolmogorov-Arnold Networks (PIKANs), using identical physics-informed formulations and matched parameter budgets to isolate the architectural effect. Both models are evaluated across a representative collection of ODEs and PDEs, including cases with known analytical solutions that allow direct assessment of gradient reconstruction accuracy. The results show that PIKANs consistently achieve more accurate solutions, converge in fewer iterations, and yield superior gradient estimates, highlighting their advantage for physics-informed learning. These findings underline the potential of KAN-based architectures as a next-generation approach for scientific machine learning and provide rigorous evidence to guide model selection in differential equation solving.

</details>


### [3] [A structure-preserving & objective discretisation of SO(3)-matrix rotation fields for finite Cosserat micropolar continua](https://arxiv.org/abs/2602.15147)
*Lucca Schek,Peter Lewintan,Wolfgang Müller,Ingo Muench,Andreas Zilian,Stéphane P. A. Bordas,Patrizio Neff,Adam Sky*

Main category: math.NA

TL;DR: Γ-SPIN is a new interpolation method that preserves geometric structure in Cosserat micropolar models while maintaining objectivity and alleviating locking effects.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop an interpolation method that simultaneously preserves physics constraints (material parameter limits) in finite-strain Cosserat micropolar models and satisfies objectivity under rigid body motions, while avoiding locking effects that plague conventional methods.

Method: The method interpolates Cosserat rotation tensor using geodesic elements for objectivity, reduces regularity by interpolating into Nédélec space, then projects back onto the rotation Lie-group. This creates lower-regularity projection-based interpolation that allows discrete rotation tensor to match polar part of deformation tensor while remaining objective.

Result: The method ensures stable behavior in asymptotic regime as Cosserat couple modulus tends to infinity (couple-stress limit). Consistency, stability, and optimality are established through benchmark problems, with efficacy demonstrated on complex curved domains compared to conventional interpolation.

Conclusion: Γ-SPIN successfully addresses the challenge of preserving geometric structure in Cosserat models while maintaining objectivity and avoiding locking, providing a robust interpolation framework for complex finite-strain micropolar simulations.

Abstract: We introduce a new method, dubbed \textbf{\textit{Geometric Structure-Preserving Interpolation (Γ-SPIN)}}, to simultaneously preserve physics-constraints inherent in the material parameter limits of the finite-strain Cosserat micropolar model, and satisfy objectivity under superimposed rigid body motions. The method advocates to interpolate the Cosserat rotation tensor using geodesic elements, which maintain objectivity and correctly represent curvature measures. At the same time, it proposes relaxing the interaction between the rotation tensor and the deformation tensor to alleviate locking effects. This relaxation is achieved in two steps. First, the regularity of the Cosserat rotation tensor is reduced by interpolating it into the Nédélec space. Second, the resulting field is projected back onto the Lie-group of rotations. Together, these steps define a lower-regularity projection-based interpolation. This construction allows the discrete Cosserat rotation tensor to match the polar part of the discrete deformation tensor while remaining objective. This ensures stable behaviour in the asymptotic regime as the Cosserat couple modulus tends to infinity, which constrains the model towards its couple-stress limit. We establish the consistency, stability, and optimality of the proposed method through several benchmark problems. The study culminates in a demonstration of its efficacy on a more intricate curved domain, contrasted with outcomes obtained from conventional interpolation techniques.

</details>


### [4] [Equivalence of mixed and nonconforming methods on general polytopal partitions. Part I: Multiscale and projection methods](https://arxiv.org/abs/2602.15193)
*Simon Lemaire*

Main category: math.NA

TL;DR: This paper establishes equivalence between mixed and nonconforming methods for variable diffusion problems, focusing on multiscale and projection methods on polytopal partitions.


<details>
  <summary>Details</summary>
Motivation: To study the equivalence between conforming mixed methods and primal nonconforming methods for variable diffusion problems on general polytopal partitions, extending previous work and providing practical criteria for method equivalence.

Method: Analyzes multiscale methods (establishing equivalence between four oversampling-free approaches) and projection methods (providing a simple criterion for primal/mixed well-posedness and equivalence). Also examines self-stabilized hybrid methods.

Result: Established first-level equivalence between four different oversampling-free multiscale approaches, broadening previous results. Provided a practical criterion for primal/mixed well-posedness and equivalence in projection methods. Offered new insights into self-stabilized hybrid methods.

Conclusion: This first part successfully establishes equivalence results for multiscale and projection methods, with Part II planned to address general polytopal element methods, contributing to a unified understanding of mixed and nonconforming approaches.

Abstract: We study equivalence, in the context of a variable diffusion problem, between (conforming) mixed methods and (primal) nonconforming methods defined on potentially general polytopal partitions. In this first paper of a series of two, we focus on multiscale and projection methods. For multiscale methods, we establish the first-level equivalence between four different (oversampling-free) approaches, thereby broadening the results of [Chaumont-Frelet, Ern, Lemaire, Valentin; M2AN, 2022]. For projection methods, in turn, we provide a simple criterion (to be checked in practice) for primal/mixed well-posedness and equivalence to hold true. In the process, we also shed a new light on some self-stabilized hybrid methods. Part II of this work will address (general) polytopal element methods.

</details>


### [5] [A Patankar predictor-corrector approach for positivity-preserving time integration](https://arxiv.org/abs/2602.15271)
*Kamila Nurkhametova,Reid J. Gomillion,Amit N. Subrahmanya,Adrian Sandu*

Main category: math.NA

TL;DR: A modular correction strategy for implicit Runge-Kutta schemes (especially SDIRK methods) that enforces positivity and conservation laws in production-destruction systems through stage-wise clipping and ratio-based scaling.


<details>
  <summary>Details</summary>
Motivation: Many natural processes modeled as production-destruction systems require positivity and conservation laws, but classical time integrators often fail to guarantee these properties, producing nonphysical negative solutions.

Method: A modular correction combining stage-wise clipping with ratio-based scaling applied to implicit Runge-Kutta schemes (particularly SDIRK methods). The correction enforces invariants and guarantees nonnegative, conservative solutions.

Result: Corrected SDIRK methods preserve positivity and invariants without significant accuracy loss in stiff ODE systems (Robertson, MAPK, stratospheric chemistry) and nonlinear PDEs (Korteweg-De Vries). Corrections applied only to final stage are sufficient in practice. For explicit RK schemes, positivity is maintained but convergence reduces to first order.

Conclusion: The proposed framework provides a simple and effective way to construct positivity-preserving integrators for stiff production-destruction systems, with practical implementation requiring only final-stage corrections for SDIRK methods.

Abstract: Many natural processes, such as chemical reactions and wave dynamics, are modeled as production-destruction (PD) systems that obey positivity and linear conservation laws. Classical time integrators do not guarantee positivity and can produce negative or nonphysical numerical solutions. This paper presents a modular correction strategy that can be applied to implicit Runge-Kutta schemes, in particular SDIRK methods. The strategy combines stage-wise clipping with a ratio-based scaling that enforces invariants and is guaranteed to yield nonnegative, conservative solutions. We provide a theoretical analysis of the corrected schemes and characterize their worst-case order of accuracy relative to the underlying base method. Numerical experiments on stiff ODE systems (Robertson, MAPK, stratospheric chemistry) and a nonlinear PDE (the Korteweg-De Vries equation) demonstrate that the corrected SDIRK methods preserve positivity and invariants without significant loss of accuracy. Importantly, corrections applied only to the final stage are sufficient in practice, while applying them at all stages may distort dynamics in some cases. For explicit Runge-Kutta schemes, the correction maintained positivity but reduced convergence to first order. These results show that the proposed framework provides a simple and effective way to construct positivity-preserving integrators for stiff PD systems.

</details>


### [6] [Total variation regularization with reduced basis in electrical impedance tomography](https://arxiv.org/abs/2602.15399)
*A. Hannukainen,N. Hyvönen,V. Toresen*

Main category: math.NA

TL;DR: Reduced basis techniques combined with smoothed total variation regularization accelerate electrical impedance tomography reconstruction to seconds on a laptop without sacrificing quality or edge enhancement.


<details>
  <summary>Details</summary>
Motivation: To speed up reconstruction algorithms for electrical impedance tomography and similar inverse elliptic boundary value problems while maintaining the edge-preserving benefits of total variation regularization.

Method: Combines reduced basis techniques with smoothed total variation regularization, using lagged diffusivity algorithm with sequential linearizations and preconditioned LSQR iteration.

Result: Achieves online reconstruction times of only a few seconds on a standard laptop computer with no significant loss of reconstruction quality or edge-enhancing properties.

Conclusion: Reduced basis techniques effectively accelerate total variation-based reconstruction for electrical impedance tomography, enabling fast 3D reconstructions on standard hardware while preserving regularization benefits.

Abstract: This work considers using reduced basis techniques in connection to (smoothened) total variation regularization in electrical impedance tomography, but analogous ideas can also be used for other inverse elliptic boundary value problems. It is demonstrated that resorting to reduced bases can speed up a reconstruction algorithm based on combining the lagged diffusivity algorithm with sequential linearizations and preconditioned LSQR iteration without any significant loss of reconstruction quality or of the edge-enhancing nature of total variation regularization. The ideas are numerically tested in three dimensions on unstructured finite element meshes with both simulated and experimental data, resulting in online reconstruction times of only a few seconds on a standard laptop computer.

</details>


### [7] [A discrete gradient scheme for preserving QSR-dissipativity](https://arxiv.org/abs/2602.15445)
*Attila Karsai,Philipp Schulze*

Main category: math.NA

TL;DR: Structure-preserving time discretization schemes using discrete gradients for dissipative systems with quadratic supply rates


<details>
  <summary>Details</summary>
Motivation: Dissipative properties deteriorate in numerical computations, especially for nonlinear systems, requiring methods that preserve these properties

Method: Discrete gradient methods for time discretization of systems dissipative with respect to quadratic supply rates

Result: Development of a class of structure-preserving discretization schemes that maintain dissipative properties

Conclusion: Discrete gradient methods provide effective structure-preserving discretization for dissipative systems with quadratic supply rates

Abstract: The notion of dissipative dynamical systems provides a formal description of processes that cannot generate energy internally. For these systems, changes in energy can only occur due to an external energy supply or dissipation effects. Unfortunately, dissipative properties tend to deteriorate in numerical computations, especially in nonlinear systems. Discrete gradient methods can help mitigate this problem. In this paper, we present a class of structure-preserving time discretization schemes based on discrete gradients for a special class of systems that are dissipative with respect to a quadratic supply rate.

</details>


### [8] [A Model Order Reduction Method for Seismic Applications Using the Laplace Transform](https://arxiv.org/abs/2602.15517)
*Fernando Henriquez,Matthias Schlottbom*

Main category: math.NA

TL;DR: A reduced basis model order reduction method for wave problems with Ricker wavelet sources, achieving exponential accuracy with parameter-robust convergence bounds.


<details>
  <summary>Details</summary>
Motivation: Wave problems with Ricker wavelet sources are important for seismic modeling applications. Existing methods need improvement in accuracy and parameter robustness, especially for vanishing initial conditions.

Method: Developed a reduced basis MOR strategy inspired by Laplace-domain methodologies. Constructed reduced bases that approximate time-domain solutions with exponential accuracy. Derived explicit convergence bounds robust to Ricker wavelet parameters.

Result: Proved convergence bounds explicit and robust to Ricker wavelet shape/width parameters. Identified intrinsic accuracy limit based on wavelet's initial value. Error bound is independent of underlying Galerkin discretization and provides computable criteria for exponential convergence regime.

Conclusion: The proposed MOR strategy achieves exponential accuracy for wave problems with Ricker wavelet sources, with parameter-robust convergence bounds that identify fundamental accuracy limits and practical implementation criteria.

Abstract: We devise and analyze a reduced basis model order reduction (MOR) strategy for an abstract wave problem with vanishing initial conditions and a source term given by the product of a temporal Ricker wavelet and a spatial profile. Such wave problems comprise the acoustic and elastic wave equations, with applications in seismic modeling. Motivated by recent Laplace-domain MOR methodologies, we construct reduced bases that approximate the time-domain solution with exponential accuracy. We prove convergence bounds that are explicit and robust with respect to the parameters controlling the Ricker wavelet's shape and width and identify an intrinsic accuracy limit dictated by the wavelet's value at the initial time. In particular, the resulting error bound is independent of the underlying Galerkin discretization space and yields computable criteria for the regime in which exponential convergence is observed.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [9] [A Two-Sink Solution to the Self-Similar Euler Equations](https://arxiv.org/abs/2602.15152)
*Hyungjun Choi,Matei P. Coiculescu*

Main category: math.AP

TL;DR: First example of 2D Euler self-similar solution with multiple stagnation points, developing velocity cusps along lines through origin.


<details>
  <summary>Details</summary>
Motivation: To construct a counterexample to the expectation that homogeneous steady states of 2D Euler equations with bounded vorticity can only have a single stagnation point at the origin.

Method: Construct a self-similar solution to the 2D incompressible Euler equations that develops cusps in the velocity field along two lines passing through the origin.

Result: Successfully constructed the first known example of a self-similar solution with multiple stagnation points, which is also a homogeneous steady state of the Euler equations.

Conclusion: Velocity cusps along lines through the origin enable multiple stagnation points in homogeneous steady states, contrasting with the single-stagnation-point result for bounded vorticity cases.

Abstract: We construct the first known example of a self-similar solution to the two-dimensional incompressible Euler equations whose pseudo-velocity has more than one stagnation point. The solution is also a homogeneous steady state of the Euler equations. In contrast, any homogeneous steady state with bounded vorticity necessarily admits only a single stagnation point at the origin. Our construction develops cusps in the velocity along two lines passing through the origin, thereby allowing stagnation points other than the origin.

</details>


### [10] [Solvability of a class of evolution operators on compact Lie groups](https://arxiv.org/abs/2602.15203)
*Alexandre Kirilov,Wagner Augusto Almeida de Moraes,Pedro Meyer Tokoro*

Main category: math.AP

TL;DR: Provides solvability conditions for Vekua-type evolution operators on T¹ × compact Lie groups, with explicit criteria for S³ case and extensions to product groups.


<details>
  <summary>Details</summary>
Motivation: To establish solvability conditions for first-order evolution operators of Vekua-type on product spaces involving compact Lie groups, addressing problems in PDE theory on manifolds with group structure.

Method: Uses spectral analysis of normalized left-invariant vector fields on compact Lie groups, expresses conditions in terms of time-dependent coefficients and spectral behavior, with detailed analysis of three-sphere case.

Result: Derives sufficient conditions for solvability, provides explicit criteria for S³ case, and extends results to operators on finite products of compact Lie groups.

Conclusion: Establishes general framework for analyzing solvability of Vekua-type evolution operators on product spaces with compact Lie group factors, with concrete results for S³ and product group cases.

Abstract: This paper provides sufficient conditions for the solvability of a class of first-order evolution operators of Vekua-type on the product of a one-dimensional torus and a compact Lie group. The conditions are expressed in terms of the time-dependent coefficients and the spectral behavior of a normalized left-invariant vector field on the group. The three-sphere case is discussed in detail, leading to more explicit criteria, and the main results are further extended to operators defined on finite products of compact Lie groups.

</details>


### [11] [A Regularized Framework and Admissible Solutions for Liquid-Vapor Phase Transitions in Steady Compressible Flows](https://arxiv.org/abs/2602.15394)
*Yazhou Chen,Qiaolin He,Dongjuan Niu,Yi Peng,Xiaoding Shi*

Main category: math.AP

TL;DR: The paper studies well-posedness of compressible Navier-Stokes with van der Waals EOS, showing phase transitions occur when specific volume average lies in Maxwell region, using artificial viscosity approximation.


<details>
  <summary>Details</summary>
Motivation: To address mathematical challenges in compressible Navier-Stokes with van der Waals equation of state, particularly non-monotonic pressure causing phase transitions and solution non-uniqueness.

Method: Introduce artificial viscosity to construct approximate system, use calculus of variations, anti-derivative technique, phase-plane analysis, and level-set method to analyze convergence.

Result: Phase transitions occur when specific volume average lies in Maxwell region - approximate solutions converge to equilibrium states with sharp interfaces; otherwise no phase transition occurs.

Conclusion: Non-monotonic pressure combined with specific volume average in Maxwell region acts as nucleation mechanism for phase transitions, providing rigorous mathematical definition of admissible solutions.

Abstract: We investigate the well-posedness of the periodic boundary value problem for the steady compressible isentropic Navier-Stokes system under the van der Waals equation of state. The main difficulty arises from the non-monotonicity of the pressure, which induces liquid-vapor phase transitions and consequently leads to both physical instabilities and mathematical non-uniqueness of solutions. It is shown that the occurrence of a phase transition is determined by whether the integral average of the specific volume lies inside the gas-liquid coexistence region defined by the Maxwell construction. By introducing an artificial viscosity, we construct an approximate system. When the integral average of the specific volume falls within the Maxwell region, the approximate solution converges, as the artificial viscosity tends to zero, to the equilibrium states given by Maxwell's construction, with the diffuse interface sharpening into a discontinuity. Conversely, if the integral average of the specific volume lies outside this region, the limiting solution remains outside as well, meaning that no phase transition occurs. These results demonstrate that the non-monotonicity of the pressure, combined with the condition that the integral average of the specific volume belongs to the Maxwell region, can act as a nucleation mechanism for phase transitions in the isentropic gas-liquid problem. Furthermore, the proposed approximation not only offers a regularized framework for describing phase transitions but also provides, from a rigorous mathematical viewpoint, a definition of admissible solutions related to phase transitions. The detailed proof relies on the artificial viscosity method, the calculus of variations, the anti-derivative technique, phase-plane analysis, and the level-set method.

</details>


### [12] [Conformal Metrics on the Disk with Prescribed Negative Gaussian Curvature and Boundary Geodesic Curvature](https://arxiv.org/abs/2602.15471)
*Rafael López-Soriano,Francisco J. Reyes-Sánchez,David Ruiz*

Main category: math.AP

TL;DR: Existence result for prescribing negative Gaussian curvature on disk and geodesic curvature on boundary via conformal metric change, overcoming bubbling/blow-up challenges.


<details>
  <summary>Details</summary>
Motivation: Study the geometric problem of prescribing Gaussian curvature on a disk and geodesic curvature on its boundary through conformal metric changes, focusing on the negative curvature regime where bubbling behavior of approximate solutions is poorly understood due to potential blow-up solutions with diverging length and area.

Method: Variational approach using families of approximated problems. Performs refined blow-up analysis for solutions with bounded Morse index to establish compactness.

Result: Provides an existence result under natural assumptions on the curvatures, considering inherent obstructions to the problem.

Conclusion: Successfully addresses the challenging negative curvature regime by developing techniques to handle bubbling and blow-up phenomena, establishing existence of solutions under appropriate curvature conditions.

Abstract: We study the problem of prescribing the Gaussian curvature on the disk and the geodesic curvature on its boundary via a conformal change of the metric. In this paper the case of negative Gaussian curvature is treated, a regime for which the bubbling behavior of approximate solutions is not so well understood. This is due to the possible appearance of blow-up solutions with diverging length and area. We give an existence result under assumptions on the curvatures which are somewhat natural, in view of some obstructions inherent to the problem. Our strategy is variational and relies on the study of certain families of approximated problems. By performing a refined blow-up analysis for solutions with bounded Morse index, we conclude compactness.

</details>


### [13] [A Degenerate Elliptic System Solvable by Transport: A Cautionary Example](https://arxiv.org/abs/2602.15479)
*Daniel Alayón-Solarz*

Main category: math.AP

TL;DR: A family of first-order elliptic PDEs with degenerating ellipticity constant (κ=O(δ⁻²)) that defeats standard elliptic solvers but is explicitly solvable via transport theory at constant cost.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that the ellipticity constant alone doesn't determine practical difficulty of solving first-order PDEs, and to caution against relying solely on elliptic solvers without checking for transport structure.

Method: Construct a one-parameter family of first-order real elliptic systems where ellipticity degenerates as δ→0. Identify transport-theoretic invariants that allow explicit solution via elementary transport methods.

Result: For any fixed elliptic solver at finite precision, δ can be chosen small enough to defeat it, yet every system in the family is explicitly solvable by transport methods at cost independent of δ.

Conclusion: Before using elliptic solvers, compute transport obstruction G; its vanishing/smallness indicates structure that standard elliptic methods miss. Ellipticity constant alone doesn't determine practical difficulty.

Abstract: We exhibit a one-parameter family of first-order real elliptic systems on the plane whose ellipticity constant degenerates to zero as $δ\to 0$, with condition number $κ= O(δ^{-2})$. For any fixed elliptic solver operating at finite precision, the parameter $δ$ can be chosen small enough to defeat the solver; no uniform numerical scheme based on the ellipticity constant alone can handle the entire family. Despite this, every member of the family is explicitly solvable -- and its initial value problem well posed -- by elementary means once a transport-theoretic invariant is identified. The cost of the transport solution is independent of $δ$. The example serves as a cautionary tale: the ellipticity constant alone does not determine the practical difficulty of a first-order PDE. Before invoking an elliptic solver, one should compute the transport obstruction $G$; its vanishing -- or smallness -- signals structure that standard elliptic methods miss entirely.

</details>


### [14] [Uniqueness and Zeroth-Order Analysis of Weak Solutions to the Non-cutoff Boltzmann equation](https://arxiv.org/abs/2602.15601)
*Dingqun Deng,Shota Sakamoto*

Main category: math.AP

TL;DR: Proves uniqueness of large solutions to non-cutoff Boltzmann equation with moderate soft potentials, requiring only finite energy and boundedness in certain norms.


<details>
  <summary>Details</summary>
Motivation: Addresses the uniqueness problem for large solutions to the non-cutoff Boltzmann equation, which is challenging due to the fractional derivative structure and lack of cutoff in collision operator.

Method: Uses dilated dyadic decompositions in phase space (v,ξ,η) to capture hypoellipticity and reduce fractional derivative structure (-Δ_v)^s to zeroth order. Employs negative-order hypoelliptic estimate to gain integrability in (t,x).

Result: Establishes uniqueness of weak solution F=μ+μ^{1/2}f as long as it has finite energy and the norm ||f||_{L∞_t L^r_{x,v}} + ||f||_{L∞_t L^2_{x,v}} remains bounded for sufficiently large r>0.

Conclusion: Successfully proves uniqueness for large solutions to non-cutoff Boltzmann equation with moderate soft potentials using novel decomposition techniques and hypoelliptic estimates.

Abstract: We establish the uniqueness of large solutions to the non-cutoff Boltzmann equation with moderate soft potentials. Specifically, the weak solution $F=μ+μ^{\frac{1}{2}}f$ is unique as long as it has finite energy, in the sense that the norm $\|f\|_{L^\infty_t L^{r}_{x,v}}+\|f\|_{L^\infty_t L^2_{x,v}}$ remains bounded (arbitrary large) for some sufficiently large $r>0$. Our approach applies dilated dyadic decompositions in phase space $(v,ξ,η)$ to capture hypoellipticity and to reduce the fractional derivative structure $(-Δ_v)^{s}$ of the Boltzmann collision operator to zeroth order. The difficulties posed by the large solution are overcome through the negative-order hypoelliptic estimate that gains integrability in $(t,x)$.

</details>


### [15] [Completeness theorems on the boundary for a parabolic equation](https://arxiv.org/abs/2602.15621)
*Alberto Cialdea,Carmine Sebastiano Mare*

Main category: math.AP

TL;DR: The paper proves completeness of polynomial solutions to parabolic equations in L^p spaces on parabolic boundaries.


<details>
  <summary>Details</summary>
Motivation: To establish completeness properties of polynomial solution systems for parabolic equations, extending similar results known for elliptic equations to the parabolic case.

Method: The authors study systems of polynomial solutions to the parabolic equation $a_{hk}\partial_{x_{h}x_{k}}u - \partial_t u = 0$ and its adjoint $a_{hk}\partial_{x_{h}x_{k}} u+ \partial_t u = 0$ in bounded $C^1$-cylinders. They use properties of elliptic operators with constant coefficients and work in the context of parabolic boundary value problems.

Result: The main result proves that the system of polynomial solutions $\{v_α\}$ is complete in $L^{p}(Σ')$, where $Σ'$ is the parabolic boundary of the cylinder $Ω_{T}$. The same completeness result holds for the adjoint equation.

Conclusion: Polynomial solutions to parabolic equations form complete systems in L^p spaces on parabolic boundaries, providing important functional analytic properties for these solution spaces that parallel known results for elliptic equations.

Abstract: Let $\{v_α\}$ be a system of polynomial solutions of the parabolic equation $a_{hk}\partial_{x_{h}x_{k}}u - \partial_t u =0$ in a bounded $C^1$-cylinder $Ω_{T}$ contained in $\mathbb{R}^{n+1}$. Here $a_{hk}\partial_{x_{h}x_{k}}$ is an elliptic operator with real constant coefficients. We prove that $\{v_α\}$ is complete in $L^{p}(Σ')$, where $Σ'$ is the parabolic boundary of $Ω_{T}$. Similar results are proved for the adjoint equation $a_{hk}\partial_{x_{h}x_{k}} u+ \partial_t u =0$.

</details>


### [16] [On the Robin problem for the Laplace equation in multiply connected domains](https://arxiv.org/abs/2602.15647)
*Alberto Cialdea,Vita Leonessa*

Main category: math.AP

TL;DR: This paper extends layer potential theory to Robin boundary conditions for Laplace equation in multiply connected domains, using double layer potentials instead of the classical single layer approach.


<details>
  <summary>Details</summary>
Motivation: To complement existing theory for Dirichlet and Neumann problems by developing a framework for Robin boundary conditions using layer potential methods in multiply connected domains.

Method: Uses layer potential methods with double layer potential representation for solutions to Laplace equation under Robin boundary conditions, contrasting with classical single layer potential approach.

Result: Develops theoretical framework for solving Laplace equation with Robin boundary conditions in multiply connected domains using double layer potentials.

Conclusion: Provides complementary theory to existing Dirichlet/Neumann problem results, establishing double layer potential approach as viable alternative to classical single layer method for Robin boundary conditions.

Abstract: This paper complements the existing theory developed in [5] for the Dirichlet and Neumann problems for the Laplace equation, in multiply connected domains. Within the framework of layer potential methods, we study the Laplace equation under Robin boundary conditions, representing the solutions by means of a double layer potential. We observe that the classical approach searches the solutions in terms of a single layer potential.

</details>


### [17] [Quantitative enstrophy bounds for measure vorticities](https://arxiv.org/abs/2602.15670)
*Luigi De Rosa,Margherita Marcotullio*

Main category: math.AP

TL;DR: Optimal quantitative estimates for enstrophy in 2D Navier-Stokes with measure initial vorticity using improved Nash inequalities, yielding conjecturally sharp dissipation rates in Delort's class.


<details>
  <summary>Details</summary>
Motivation: To establish quantitative bounds for enstrophy in 2D incompressible Navier-Stokes equations when initial vorticity is a measure, which is challenging due to the low regularity of initial data.

Method: Uses improved Nash inequalities to derive estimates connecting enstrophy to the decay of absolute vorticity on balls.

Result: Obtains optimal quantitative bounds for enstrophy that depend on vorticity decay on balls, leading to conjecturally sharp dissipation rates in Delort's class.

Conclusion: The improved Nash inequality approach provides optimal estimates for enstrophy with measure initial data, advancing understanding of dissipation rates in the Delort class for 2D Navier-Stokes.

Abstract: We consider the two-dimensional incompressible Navier-Stokes equations with measure initial vorticity. By means of improved Nash inequalities, we establish quantitative estimates for the enstrophy depending on the absolute vorticity decay on balls. The bounds are optimal in several aspects and yield to a conjecturally sharp rate of the dissipation in the Delort's class.

</details>


### [18] [Solving Dirichlet problem on unbounded uniform domains by using sphericalization techniques](https://arxiv.org/abs/2602.15701)
*Riikka Korte,Sari Rogovin,Nageswari Shanmugalingam,Timo Takala*

Main category: math.AP

TL;DR: Existence of p-harmonic solutions in unbounded domains with unbounded boundaries using sphericalization technique.


<details>
  <summary>Details</summary>
Motivation: Previous methods for solving Dirichlet problems in bounded domains (using calculus of variations and Maz'ya inequalities) fail for unbounded domains with unbounded boundaries, requiring new techniques.

Method: Uses sphericalization technique from prior work to transform the problem, enabling existence proofs for p-harmonic functions in unbounded uniform domains.

Result: Establishes existence of solutions to Dirichlet boundary value problems for p-harmonic functions in unbounded uniform domains with unbounded boundary for 1<p<∞.

Conclusion: The sphericalization technique successfully extends existence results to unbounded settings, with uniqueness considerations depending on p-parabolicity/hyperbolicity properties of the domain.

Abstract: Within the setting of metric spaces equipped with a doubling measure and supporting a $p$-Poincaré inequality, establishing existence of solutions to Dirichlet problem in a bounded domain in such a metric space is accomplished via direct methods of calculus of variation and the use of a Maz'ya type inequality, which is a consequence of the Poincaré inequality. However, when the domain and its boundary are unbounded, such a method is unavailable. In this paper, using the technique of sphericalization developed in the prior paper~[32], we establish the existence of solutions to the Dirichlet boundary value problem for $p$-harmonic functions in unbounded uniform domains with unbounded boundary when $1<p<\infty$. We also explore the issue of whether such solutions are unique by considering $p$-parabolicity and $p$-hyperbolicity properties of the domain.

</details>


### [19] [Fine regularity of fractional harmonic maps and applications](https://arxiv.org/abs/2602.15715)
*Kyeongbae Kim,Simon Nowak,Yannick Sire*

Main category: math.AP

TL;DR: The paper establishes regularity results for harmonic mappings into spheres using fractional Sobolev energies, overcoming the lack of monotonicity formulas through nonlocal equation techniques.


<details>
  <summary>Details</summary>
Motivation: To extend regularity theory for harmonic maps to more general fractional Sobolev energies, particularly when classical monotonicity formulas are unavailable, and to apply these results to harmonic maps with free boundaries and fractional heat flows.

Method: Uses techniques from nonlocal equation regularity theory to analyze harmonic mappings into Euclidean spheres associated with Gagliardo-type fractional Sobolev energies, without relying on monotonicity formulas.

Result: Proves small energy regularity results, improves known results even in isotropic cases, obtains new potential-theoretic estimates for harmonic maps with free boundaries, and establishes higher differentiability for fractional harmonic map heat flow.

Conclusion: The paper successfully extends regularity theory to fractional harmonic mappings despite the absence of monotonicity formulas, providing new tools for analyzing harmonic maps with free boundaries and fractional heat flows.

Abstract: In this paper, we derive several regularity results for harmonic mappings into Euclidean spheres associated with rather general energies related to fractional Sobolev spaces. These maps generalize families of maps introduced by Da Lio, Rivière and Schikorra and are related to harmonic maps with free boundaries. In our context, there is in general no monotonicity formula, which prevents the use of some classical methods. Despite this limitation, under natural assumptions on a Gagliardo-type energy, we succeed in proving a variety of small energy regularity results and improve on known results, even in the isotropic case for which some monotonicity formula is available. To this end, we exploit recent developments in the regularity theory of nonlocal equations and as a by-product, we explain how these results apply to classes of harmonic maps with free boundary and lead to new potential-theoretic estimates. As another application, we obtain higher differentiability results for the fractional harmonic map heat flow.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [20] [VR-PIC: An entropic variance-reduction method for particle-in-cell solutions of the Vlasov-Poisson equation](https://arxiv.org/abs/2602.15041)
*Victor Windhab,Andreas Adelmann,Mohsen Sadr*

Main category: physics.comp-ph

TL;DR: Extends variance reduction framework from Boltzmann equation to PIC method for Vlasov-Poisson, proposes weight correction via maximum cross-entropy to maintain conservation laws with minimal bias.


<details>
  <summary>Details</summary>
Motivation: To apply the successful variance reduction techniques developed for Boltzmann equation simulations to particle-in-cell (PIC) methods for solving Vlasov-Poisson equations, particularly for improving computational efficiency in low signal regimes.

Method: Extends entropic and conservative variance reduction framework to PIC method; uses zeroth-order approximation freezing importance weights during velocity-space kick; proposes correction using maximum cross-entropy formulation to ensure conservation laws while minimizing bias.

Result: The method maintains substantial speed-up of variance reduction compared to standard PIC simulations in low signal regimes (tested on Sod's shock tube and Landau damping cases) with minimal changes to simulation code.

Conclusion: Successfully extends variance reduction framework to PIC methods, achieving computational efficiency improvements while maintaining accuracy through maximum cross-entropy weight correction that preserves conservation laws.

Abstract: We extend the recently developed entropic and conservative variance reduction framework [M. Sadr, N. G. Hadjiconstantinou, A variance-reduced direct Monte Carlo simulation method for solving the Boltzmann equation over a wide range of rarefaction, Journal of Computational Physics 472 (2023) 111677.] to the particle-in-cell (PIC) method of solving Vlasov-Poisson equation. We show that a zeroth-order approximation that freezes the importance weights during the velocity-space kick is stable at the expense of introducing bias. Then, we propose a correction for the weight distribution using maximum cross-entropy formulation to ensure conservation laws while minimizing the introduced bias. In several test cases including Sod's shock tube and Landau damping we show that the proposed method maintains the substantial speed-up of variance reduction method compared to the PIC simulations in the low signal regime with minimal changes to the simulation code.

</details>


### [21] [Code-Verification Techniques for Particle-in-Cell Simulations with Direct Simulation Monte Carlo Collisions](https://arxiv.org/abs/2602.15130)
*Brian A. Freno,William J. McDoniel,Christopher H. Moore,Neil R. Matula*

Main category: physics.comp-ph

TL;DR: The paper presents code verification approaches for particle-in-cell methods with stochastic collision models using the method of manufactured solutions, enabling direct error computation for particle positions and velocities.


<details>
  <summary>Details</summary>
Motivation: Code verification for particle-in-cell methods with stochastic collisions is challenging due to interactions between spatial/temporal discretization errors, statistical sampling noise, and stochastic collision algorithms. Existing approaches struggle with these complexities.

Method: The authors incorporate the method of manufactured solutions into particle equations of motion, manufacturing the particle distribution function and inversely querying the cumulative distribution function to obtain known particle positions/velocities at each time step. For collision algorithms, they average independent outcomes and derive manufactured source terms for velocity changes.

Result: The approaches enable direct computation of errors in particle positions and velocities instead of attempting to compute differences in distribution functions. They demonstrate effectiveness in 3D for different particle-field couplings, with/without binary elastic collisions, and with/without coding errors.

Conclusion: The proposed verification approaches are valid for both particle-in-cell simulations with Monte Carlo collisions and direct simulation Monte Carlo simulations of neutral gas flows, providing robust code verification for stochastic collisional plasma simulations.

Abstract: Particle-in-cell methods with stochastic collision models are commonly used to simulate collisional plasma dynamics, with applications ranging from hypersonic flight to semiconductor manufacturing. Code verification of such methods is challenging due to the interaction between the spatial- and temporal-discretization errors, the statistical sampling noise, and the stochastic nature of the collision algorithm. In this paper, we introduce our code-verification approaches to apply the method of manufactured solutions to plasma dynamics, and we derive expected convergence rates for the different sources of discretization and statistical error. For the particles, we incorporate the method of manufactured solutions into the equations of motion. We manufacture the particle distribution function and inversely query the cumulative distribution function to obtain known particle positions and velocities at each time step. In doing so, we avoid modifying the particle weights, eliminating risks from potentially negative weights or modifications to weight-dependent collision algorithms. For the collision algorithm, we average independent outcomes at each time step and we derive a corresponding manufactured source term for the velocity change for each particle. By having known solutions for the particle positions and velocities, we are able to compute the error in these quantities directly instead of attempting to compute differences in distribution functions. These approaches are equally valid for particle-in-cell simulations with Monte Carlo collisions and direct simulation Monte Carlo simulations of neutral gas flows. We demonstrate the effectiveness of our approaches in three dimensions for different couplings between the particles and field, with and without binary elastic collisions, and with and without coding errors.

</details>


### [22] [Analysis of Fission Matrix Databases using Temperature Profiles obtained from High-Fidelity Multiphysics Simulations](https://arxiv.org/abs/2602.15244)
*Maximiliano Dalinger,Elia Merzari,Saya Lee,Alex Nellis*

Main category: physics.comp-ph

TL;DR: Fission Matrix method accuracy improves when using realistic temperature profiles from multiphysics simulations rather than uniform profiles for database construction.


<details>
  <summary>Details</summary>
Motivation: The Fission Matrix method requires precalculated databases from Monte Carlo simulations, but the temperature profiles used to create these databases can significantly affect accuracy. The paper investigates how different temperature profile selections impact simulation results.

Method: Two database sets were created for the Molten Salt Fast Reactor: one using temperature profiles from high-fidelity multiphysics simulations with Cardinal, and another using uniform temperature profiles. The Fission Matrix method was then applied using these databases.

Result: Results showed improved multiplication factor and fission source distribution when databases were constructed using temperature profiles similar to those expected during fission matrix solving, compared to using uniform temperature profiles.

Conclusion: Using realistic temperature profiles from multiphysics simulations for Fission Matrix database construction leads to more accurate neutronics simulations, highlighting the importance of proper temperature profile selection in reactor analysis.

Abstract: The Fission Matrix method is used to perform fast and still accurate neutronics simulations. It relies on precalculated databases obtained through a Monte Carlo simulation. To represent every state of the reactor, multiple databases are required. The actual state of the reactor is obtained from those databases. In this paper, we analyze the effect of the temperature profiles selected to construct the databases. To do so, the Molten Salt Fast reactor is selected. Two sets of databases are studied: the first uses temperature profiles obtained from high-fidelity Multiphysics simulations with Cardinal, and the second uses uniform temperature profiles. Results showed improved multiplication factor and fission source distribution when the temperature profiles used to generate the databases were similar to those expected when solving the fission matrix.

</details>


### [23] [Neural-POD: A Plug-and-Play Neural Operator Framework for Infinite-Dimensional Functional Nonlinear Proper Orthogonal Decomposition](https://arxiv.org/abs/2602.15632)
*Changhong Mou,Binghang Lu,Guang Lin*

Main category: physics.comp-ph

TL;DR: Neural-POD is a neural operator framework that learns nonlinear, orthogonal basis functions in infinite-dimensional space, overcoming discretization limitations of classical POD and enabling resolution-invariant representations.


<details>
  <summary>Details</summary>
Motivation: AI for Science faces "discretization" problems where learned representations are restricted to specific grids/resolutions used during training. Classical POD is limited to linear subspace approximations via SVD and lacks flexibility for nonlinear structures.

Method: Neural-POD constructs nonlinear orthogonal basis functions through neural networks by formulating basis construction as sequence of residual minimization problems, analogous to Gram-Schmidt orthogonalization. Each basis function learns to represent remaining structure in data.

Result: Neural-POD enables optimization in arbitrary norms (L², L¹), learns resolution-invariant mappings between infinite-dimensional spaces, generalizes to unseen parameter regimes, captures nonlinear structures, and produces interpretable, reusable basis functions.

Conclusion: Neural-POD serves as a high-performance, plug-and-play bridge between classical Galerkin projection and operator learning, enabling consistent integration with both projection-based reduced order models and DeepONet frameworks for complex spatiotemporal systems.

Abstract: The rapid development of AI for Science is often hindered by the "discretization", where learned representations remain restricted to the specific grids or resolutions used during training. We propose the Neural Proper Orthogonal Decomposition (Neural-POD), a plug-and-play neural operator framework that constructs nonlinear, orthogonal basis functions in infinite-dimensional space using neural networks. Unlike the classical Proper Orthogonal Decomposition (POD), which is limited to linear subspace approximations obtained through singular value decomposition (SVD), Neural-POD formulates basis construction as a sequence of residual minimization problems solved through neural network training. Each basis function is obtained by learning to represent the remaining structure in the data, following a process analogous to Gram--Schmidt orthogonalization. This neural formulation introduces several key advantages over classical POD: it enables optimization in arbitrary norms (e.g., $L^2$, $L^1$), learns mappings between infinite-dimensional function spaces that is resolution-invariant, generalizes effectively to unseen parameter regimes, and inherently captures nonlinear structures in complex spatiotemporal systems. The resulting basis functions are interpretable, reusable, and enabling integration into both reduced order modeling (ROM) and operator learning frameworks such as deep operator learning (DeepONet). We demonstrate the robustness of Neural-POD with different complex spatiotemporal systems, including the Burgers' and Navier-Stokes equations. We further show that Neural-POD serves as a high performance, plug-and-play bridge between classical Galerkin projection and operator learning that enables consistent integration with both projection-based reduced order models and DeepONet frameworks.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [24] [TokaMind: A Multi-Modal Transformer Foundation Model for Tokamak Plasma Dynamics](https://arxiv.org/abs/2602.15084)
*Tobia Boschi,Andrea Loreti,Nicola C. Amorisco,Rodrigo H. Ordonez-Hurtado,Cécile Rousseau,George K. Holt,Eszter Székely,Alexander Whittle,Samuel Jackson,Adriano Agnello,Stanislas Pamela,Alessandra Pascale,Robert Akers,Juan Bernabe Moreno,Vassil Alexandrov,Mykhaylo Zayats*

Main category: physics.plasm-ph

TL;DR: TokaMind is an open-source multi-modal transformer framework for fusion plasma modeling that outperforms baselines on most tasks in the MAST benchmark, demonstrating the value of multi-modal pretraining for tokamak dynamics.


<details>
  <summary>Details</summary>
Motivation: To create an extensible foundation model framework for fusion plasma modeling that can handle heterogeneous tokamak diagnostics (time-series, 2D profiles, videos) with different sampling rates and robust missing-signal handling.

Method: Uses a Multi-Modal Transformer (MMT) with training-free Discrete Cosine Transform embedding (DCT3D), supports alternative embeddings like VAEs, and enables efficient task adaptation via selectively loading/freezing four model components.

Result: Fine-tuned TokaMind outperforms the benchmark baseline on all but one task in the MAST TokaMark benchmark, with lightweight fine-tuning often performing better than training from scratch under matched epoch budgets.

Conclusion: Multi-modal pretraining benefits tokamak plasma dynamics modeling, and TokaMind provides a practical, extensible foundation for future fusion modeling tasks, with code and weights to be made publicly available.

Abstract: We present TokaMind, an open-source foundation model framework for fusion plasma modeling, based on a Multi-Modal Transformer (MMT) and trained on heterogeneous tokamak diagnostics from the publicly available MAST dataset. TokaMind supports multiple data modalities (time-series, 2D profiles, and videos) with different sampling rates, robust missing-signal handling, and efficient task adaptation via selectively loading and freezing four model components. To represent multi-modal signals, we use a training-free Discrete Cosine Transform embedding (DCT3D) and provide a clean interface for alternative embeddings (e.g., Variational Autoencoders - VAEs). We evaluate TokaMind on the recently introduced MAST benchmark TokaMark, comparing training and embedding strategies. Our results show that fine-tuned TokaMind outperforms the benchmark baseline on all but one task, and that, for several tasks, lightweight fine-tuning yields better performance than training the same architecture from scratch under a matched epoch budget. These findings highlight the benefits of multi-modal pretraining for tokamak plasma dynamics and provide a practical, extensible foundation for future fusion modeling tasks. Training code and model weights will be made publicly available.

</details>


### [25] [Flux pumping and bifurcated relaxations of helical core in 3D magnetohydrodynamic modelling of ASDEX Upgrade plasmas](https://arxiv.org/abs/2602.15431)
*H. Zhang,M. Hoelzl,I. Krebs,A. Burckhart,A. Bock,S. Guenter,V. Igochine,K. Lackner,D. Bonfiglio,E. Fable,F. Stefanelli,R. Ramasamy,H. Zohm,JOREK TEAM,ASDEX UPGRADE TEAM*

Main category: physics.plasm-ph

TL;DR: Flux pumping in AUG tokamak is modeled using JOREK MHD code, reproducing clamped current profiles via dynamo effect from 1/1 MHD instability. Parameter scans reveal bifurcated states (flux pumping, sawteeth, magnetic islands) with transitions between them, identifying operational windows for flux pumping.


<details>
  <summary>Details</summary>
Motivation: To understand the self-regulation mechanism of flux pumping in AUG tokamak experiments, which features sawtooth-free helical quiescent states and anomalous current redistribution, using realistic MHD modeling to advance understanding and develop predictive capabilities.

Method: Used JOREK code with two-temperature, nonlinear, full magnetohydrodynamic (MHD) model at realistic parameters. Conducted systematic parameter scans of dissipation coefficients and plasma beta. Analyzed dynamo term generation from m/n = 1/1 quasi-interchange-like MHD instability driven by pressure gradient.

Result: Quantitatively reproduced clamped current density and safety factor profiles in plasma core, demonstrating dynamo effect effectiveness. Revealed bifurcated plasma behaviors at different Hartmann numbers: flux pumping (helical core with flat current), sawteeth, single crash, and quasi-stationary magnetic island. Observed transitions from marginal flux pumping to sawteeth. Estimated operational window for flux pumping based on plasma density and temperature.

Conclusion: Flux pumping is sustained by dynamo effect from 1/1 MHD instability. System exhibits multiple bifurcated states depending on dissipation and beta parameters. The modeling advances understanding of flux pumping mechanisms and enables development of surrogate models for efficient evaluation of flux pumping operational regimes.

Abstract: Flux pumping was achieved in recent hybrid scenario experiments in the ASDEX Upgrade (AUG) tokamak, which is characterized by a sawtooth-free helical quiescent state and the anomalous radial redistribution of toroidal current density and poloidal magnetic flux. In this article, the self-regulation mechanism of the AUG core plasma during flux pumping is investigated at realistic parameters using the JOREK code based on the two-temperature, nonlinear, full magnetohydrodynamic (MHD) model. A key milestone in AUG flux pumping modelling is achieved by quantitatively reproducing the clamped current density and safety factor profiles in the plasma core, demonstrating the effectiveness of the dynamo effect in sustaining the flux pumping state. The dynamo term, that is of particular interest, is primarily generated by the pressure-gradient driven m/n = 1/1 quasi-interchange-like MHD instability. The work systematically extrapolates the parameter regimes of flux pumping from the above AUG base case by scanning dissipation coefficients and plasma beta. The simulation results reveal bifurcated plasma behaviours at different Hartmann numbers, including distinct states such as flux pumping (helical core with a flat current density), sawteeth (periodic kink-cycling), single crash (without subsequent cycle), and quasi-stationary magnetic island (peaked current density). Transitions from marginal flux pumping state to sawteeth are observed in long-term simulations. The relationships between system dissipation, plasma beta, and different plasma states are carefully analyzed. For practical purposes, the potential operational window for flux pumping, as determined by plasma density and temperature, is estimated. The modelling efforts advance the understanding of flux pumping and facilitate the development of a fast surrogate model for efficient evaluation of flux pumping.

</details>


### [26] [From Coils to Surface Recession: Fully Coupled Simulation of Ablation in ICP Wind Tunnels](https://arxiv.org/abs/2602.15500)
*Sanjeev Kumar,Alessandro Munafo,Blaine Vollmer,Daniel J. Bodony,Gregory S. Elliott,Kelly A. Stephani,Sean Kearney,Marco Panesi*

Main category: physics.plasm-ph

TL;DR: A fully coupled multiphysics framework for simulating thermal protection system behavior in ICP wind tunnels, validated against experimental data with good accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop a predictive computational tool for simulating thermo-chemical material response in inductively coupled plasma wind tunnels used for hypersonic material testing, enabling better design and interpretation of testing campaigns.

Method: Integrated framework combining high-fidelity Navier-Stokes plasma solver, electromagnetic field solver, and discontinuous-Galerkin material response solver using partitioned coupling strategy, simulating the entire 350 kW Plasmatron X facility from plasma generation to material ablation.

Result: The framework accurately captures key ICP physics (vortex-mode recirculation, Joule heating, Lorentz forces) and predicts transition from subsonic to supersonic jet behavior. Validation shows predicted heat fluxes within experimental uncertainty, stagnation temperature histories and recession rates with errors below 12% and 10% respectively.

Conclusion: The framework demonstrates strong predictive capability for ICP wind tunnel environments and provides a foundation for improved design, interpretation, and planning of hypersonic material testing campaigns, though some discrepancies remain due to uncertainties in power-coupling efficiency and material properties.

Abstract: This work presents a fully coupled, multiphysics computational framework for predicting the thermo-chemical material response of thermal protection systems in inductively coupled plasma (ICP) wind tunnels. The framework integrates a high-fidelity Navier-Stokes plasma solver, an electromagnetic field solver, and a discontinuous-Galerkin material response solver using a partitioned coupling strategy. This enables an ab initio, end-to-end simulation of the 350 kW Plasmatron X facility at the University of Illinois Urbana-Champaign (UIUC), including plasma generation, electromagnetic heating, near-wall thermochemistry, and time-accurate material ablation. The model captures key ICP physics such as vortex-mode recirculation, Joule-heating-driven plasma formation, and Lorentz-force-induced flow confinement, and accurately predicts the transition from subsonic to supersonic jet behavior at low pressures. Validation against cold-wall calorimetry and graphite ablation experiments shows that predicted stagnation-point heat fluxes fall well within experimental uncertainty, while fully coupled simulations accurately reproduce measured stagnation temperature histories and recession rates with errors below 12% and 10%, respectively. Remaining discrepancies during early transient heating are attributed to uncertainties in power-coupling efficiency, equilibrium ablation modeling, and material property datasets. Overall, the framework demonstrates strong predictive capability for ICP wind tunnel environments and provides a foundation for improved design, interpretation, and planning of hypersonic material testing campaigns.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [27] [A Robust Truncated-Domain Approach for Cone--Jet Simulations in Electrospinning and Electrospraying](https://arxiv.org/abs/2602.15416)
*Ghanashyam K. C.,Satyavrata Samavedi,Harish N Dixit*

Main category: physics.flu-dyn

TL;DR: A truncated-domain simulation framework for electrospinning/electrospraying that uses full-domain electrostatic simulations to provide accurate boundary conditions, eliminating empirical tuning and reducing computational cost while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Full-domain simulations of electrospinning/electrospraying are computationally expensive due to large scale separation between needle and collector. Existing truncated-domain approaches rely on analytical approximations that underestimate electric fields near the needle tip and require empirical tuning, limiting predictive capability.

Method: Use inexpensive full-domain electrostatic simulations to obtain exact electric field and potential distributions near the needle, then impose these as boundary conditions in electrohydrodynamic (EHD) simulations on a truncated domain. This eliminates tunable parameters and doesn't require prior knowledge of cone-jet configuration.

Result: The approach accurately reproduces cone-jet shapes, electric currents, charge distributions, velocity fields, and Maxwell stresses compared to full-domain EHD simulations and experimental data. It converges at substantially smaller domain sizes and significantly reduces computational cost.

Conclusion: The framework provides a reliable, predictive, and computationally efficient method for studying electrohydrodynamic cone-jet flows without empirical tuning, enabling more accessible simulation of electrospinning and electrospraying processes.

Abstract: Direct numerical simulations of electrospinning and electrospraying are computationally demanding due to large-scale separation between the needle and the tip-to-collector distance. The cone-jet mode that occurs in the vicinity of the needle arises from a delicate balance between surface tension, viscous stresses, inertia, and electric stresses. This mode has a central role in determining the subsequent instabilities of the jet and the eventual outcomes on the collector. Truncated-domain simulations offer a viable alternative but depend critically on the accuracy of far-field electrostatic boundary conditions. Existing truncated-domain approaches based on analytical expressions for the electric potential systematically underestimate the electric field near the needle tip and require empirical tuning informed by prior experiments or full-domain simulations, thereby limiting their predictive capability. Here, we present a general truncated-domain framework for electrohydrodynamic (EHD) simulations of the cone-jet mode that avoids these limitations. Our approach exploits inexpensive full-domain electrostatic simulations to obtain the exact electric field and potential distributions near the needle, which are then imposed as boundary conditions in an EHD simulation carried out on a truncated domain. Comparisons with full-domain EHD simulations and experimental data demonstrate that the proposed approach accurately reproduces cone-jet shapes as well as key physical quantities, including electric currents, charge distributions, velocity fields, and Maxwell stresses, while converging at substantially smaller domain sizes. The formulation eliminates tunable parameters, does not require prior knowledge of the cone-jet configuration, and significantly reduces computational cost, providing a reliable and predictive framework for studying electrohydrodynamic cone-jet flows.

</details>


### [28] [Novel distance-based masking and adaptive alpha-shape methods for CNN-ready reconstruction of arbitrary 2D CFD flow domains](https://arxiv.org/abs/2602.15536)
*Mehran Sharifi,Gorka S. Larraona,Alejandro Rivas*

Main category: physics.flu-dyn

TL;DR: A reconstruction framework that recovers physically consistent masks from scattered CFD data using distance-based masking and adaptive alpha-shapes, achieving 500-800x speedups over classical methods with improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Interpolating scattered CFD datasets onto uniform Cartesian grids distorts true geometry, creating convex-hull envelopes and activating nonphysical regions, which is problematic for generating CNN-ready fields.

Method: Two novel strategies: 1) distance-based masking using tau set to minimum CFD grid spacing, and 2) adaptive alpha-shape formulation that normalizes alpha using local data resolution. Includes lightweight boundary inflation post-processing with minimal dilation.

Result: Distance-based method achieves 500-800x speedups over classical alpha-shapes, robust across geometries with same threshold rule. Adaptive alpha-shape is 1.7-2.6x faster than classical variant. Boundary inflation improves retention by up to 2.96% with negligible unsupported activation (<0.08%).

Conclusion: Distance-based method recommended as default due to accuracy, stability, minimal tuning, and low cost. Adaptive alpha-shape is strong alternative when grid-spacing information unavailable. Companion web application enables end-to-end workflow.

Abstract: Interpolating scattered CFD datasets onto a uniform Cartesian grid can distort the true geometry, producing a convex-hull type envelope and activating nonphysical regions. This work presents a reconstruction framework that recovers physically consistent masks before exporting CNN-ready fields. It introduces two novel strategies, distance-based masking and an adaptive alpha-shape formulation that normalizes alpha using local data resolution, and evaluates them against classical alpha-shape boundary recovery. A quantitative, topology-aware metric suite is introduced to assess retention, suppression of unsupported regions, overlap consistency, and connectivity. The novel distance-based method is robust across the geometries considered under the same threshold rule, with tau set to the minimum CFD grid spacing, and achieves 500-800 times speedups over classical alpha-shapes. The adaptive alpha-shape remains stable when its control parameter is set to 1 and is 1.7-2.6 times faster than the classical variant, which requires geometry-specific alpha tuning. A lightweight boundary inflation post-process using a minimal dilation further improves retention by up to 2.96% with negligible unsupported activation (less than 0.08%). Overall, the distance-based method is recommended as the default due to its accuracy, stability, minimal tuning, and low cost, while the adaptive alpha-shape is a strong alternative when grid-spacing information for threshold selection is unavailable. A companion web application operationalizes the workflow end to end, enabling 2D ASCII dataset upload, parameter tuning, mask and boundary generation, and export of CNN-ready outputs.

</details>


### [29] [Uni-Flow: a unified autoregressive-diffusion model for complex multiscale flows](https://arxiv.org/abs/2602.15592)
*Xiao Xue,Tianyue Yang,Mingyang Gao,Leyu Pan,Maida Wang,Kewei Zhu,Shuo Wang,Jiuling Li,Marco F. P. ten Eikelder,Peter V. Coveney*

Main category: physics.flu-dyn

TL;DR: Uni-Flow is a unified autoregressive-diffusion framework that separates temporal evolution from spatial refinement to model complex dynamical systems, enabling faster-than-real-time high-resolution flow simulations across physics and biomedical applications.


<details>
  <summary>Details</summary>
Motivation: Existing physics-informed machine learning approaches struggle to simultaneously maintain long-term temporal evolution and resolve fine-scale structure across chaotic, turbulent, and physiological flow regimes. There's a need for models that can handle multiscale dynamics while being computationally efficient for practical applications like cardiovascular simulations.

Method: Uni-Flow combines autoregressive and diffusion components: (1) an autoregressive component learns low-resolution latent dynamics to preserve large-scale structure and ensure stable long-horizon rollouts, and (2) a diffusion component reconstructs high-resolution physical fields, recovering fine-scale features in few denoising steps.

Result: Validated across canonical benchmarks including 2D Kolmogorov flow, 3D turbulent channel inflow generation with quantum-informed autoregressive prior, and patient-specific aortic coarctation simulations. In cardiovascular applications, enables faster-than-real-time inference of pulsatile hemodynamics, reconstructing high-resolution pressure fields in seconds rather than hours.

Conclusion: Uni-Flow transforms high-fidelity hemodynamic simulation from an offline, HPC-bound process into a deployable surrogate, establishing a pathway to faster-than-real-time modelling of complex multiscale flows with broad implications for scientific machine learning in flow physics.

Abstract: Spatiotemporal flows govern diverse phenomena across physics, biology, and engineering, yet modelling their multiscale dynamics remains a central challenge. Despite major advances in physics-informed machine learning, existing approaches struggle to simultaneously maintain long-term temporal evolution and resolve fine-scale structure across chaotic, turbulent, and physiological regimes. Here, we introduce Uni-Flow, a unified autoregressive-diffusion framework that explicitly separates temporal evolution from spatial refinement for modelling complex dynamical systems. The autoregressive component learns low-resolution latent dynamics that preserve large-scale structure and ensure stable long-horizon rollouts, while the diffusion component reconstructs high-resolution physical fields, recovering fine-scale features in a small number of denoising steps. We validate Uni-Flow across canonical benchmarks, including two-dimensional Kolmogorov flow, three-dimensional turbulent channel inflow generation with a quantum-informed autoregressive prior, and patient-specific simulations of aortic coarctation derived from high-fidelity lattice Boltzmann hemodynamic solvers. In the cardiovascular setting, Uni-Flow enables task-level faster than real-time inference of pulsatile hemodynamics, reconstructing high-resolution pressure fields over physiologically relevant time horizons in seconds rather than hours. By transforming high-fidelity hemodynamic simulation from an offline, HPC-bound process into a deployable surrogate, Uni-Flow establishes a pathway to faster-than-real-time modelling of complex multiscale flows, with broad implications for scientific machine learning in flow physics.

</details>


### [30] [Physics-informed data-driven inference of an interpretable equivariant LES model of incompressible fluid turbulence](https://arxiv.org/abs/2602.15743)
*Matteo Ugliotti,Brandon Choi,Mateo Reynoso,Daniel R. Gurevich,Roman O. Grigoriev*

Main category: physics.flu-dyn

TL;DR: A symbolic data-driven subgrid-scale model for turbulence that requires no phenomenological assumptions and has no adjustable parameters, outperforming leading LES models.


<details>
  <summary>Details</summary>
Motivation: Restrictive phenomenological assumptions in existing turbulence models limit their ability to accurately describe key quantities like local energy and enstrophy fluxes, especially in the presence of diverse coherent structures.

Method: Developed a symbolic data-driven subgrid-scale model using LES-style spatial coarse-graining, but with a structure more similar to RANS models that employs an additional rank-two tensor field to describe subgrid scales.

Result: The model outperforms leading LES models and produces accurate predictions of various quantities including local fluxes across a broad range of two-dimensional turbulent flows, as shown through both a priori and a posteriori benchmarks.

Conclusion: A rank-two tensor field structure is necessary to correctly represent both components of the subgrid-scale stress tensor and various fluxes, enabling a data-driven approach without phenomenological assumptions or adjustable parameters.

Abstract: Restrictive phenomenological assumptions represent a major roadblock for the development of accurate subgrid-scale models of fluid turbulence. Specifically, these assumptions limit a model's ability to describe key quantities of interest, such as local fluxes of energy and enstrophy, in the presence of diverse coherent structures. This paper introduces a symbolic data-driven subgrid-scale model that requires no phenomenological assumptions and has no adjustable parameters, yet it outperforms leading LES models. A combination of a priori and a posteriori benchmarks shows that the model produces accurate predictions of various quantities including local fluxes across a broad range of two-dimensional turbulent flows. While the model is inferred using LES-style spatial coarse-graining, its structure is more similar to RANS models, as it employs an additional field to describe subgrid scales. We find that this field must have a rank-two tensor structure in order to correctly represent both the components of the subgrid-scale stress tensor and the various fluxes.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [31] [Timelike bounce hypersurfaces in charged null dust collapse](https://arxiv.org/abs/2602.15786)
*David Bick*

Main category: gr-qc

TL;DR: Study of charged null dust beams that bounce along timelike surfaces in general relativity, with explicit constructions in spherical symmetry and solutions to free boundary problems.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of interacting charged null fluids in general relativity, particularly the bouncing continuation model where charged massless particles can change direction after losing momentum due to electrostatic repulsion.

Method: Study timelike bounce hypersurfaces in spherical symmetry, identify decoupling of equations of motion, construct explicit spacetimes by gluing Reissner-Nordström and Vaidya regions, and formulate/solve free boundary problems for bounce hypersurface formation.

Result: Every timelike curve in spherically symmetric quotient of Minkowski or Reissner-Nordström spacetimes can be realized as bounce hypersurface; constructed spacetimes with C²¹ metric regularity across bounce surface; obtained examples of timelike bounce hypersurfaces terminating in null points; solved conditional free boundary problem for bounce formation.

Conclusion: The paper establishes rigorous mathematical results for charged null dust bouncing along timelike surfaces in general relativity, providing explicit constructions and solving free boundary problems, though some results are conditional on technical regularity assumptions.

Abstract: We establish results on the dynamics of interacting charged null fluids in general relativity, specifically in the context of the bouncing continuation proposed in [Ori91]. In this model - the setting for a number of prominent case studies on black hole formation - charged massless particles may instantaneously change direction (bounce) after losing all their 4-momentum due to electrostatic repulsion. We initiate the study of timelike bounce hypersurfaces in spherical symmetry: scenarios in which an incoming beam of charged null dust changes direction along a timelike surface $\mathcal{B}$, which is the (free) boundary of an interacting 2-dust region. We identify a novel decoupling of the equations of motion in this region. First, it is shown that every timelike curve segment $γ$ in the spherically symmetric quotient of Minkowski or Reissner-Nordström spacetimes arises as the bounce hypersurface $\mathcal{B}$ of a charged null dust beam incident from past null infinity $\mathcal{I}^-$. We construct a spacetime $(\mathcal{M},g_{μν})$ describing the full trajectory of the beam, which includes gluing to Reissner-Nordström and Vaidya regions. Across $\mathcal{B}$ the metric has regularity $g_{μν}\in C^{2,1}$ and satisfies Einstein's equation classically, while $C^\infty$ gluing may be achieved across all other interfaces. We also obtain examples of timelike bounce hypersurfaces terminating in a null point. Since these constructions are teleological, we secondly consider a given charged incoming beam from past null infinity. We formulate and solve a free boundary problem which represents the formation of a timelike bounce hypersurface. The result is conditional, applying only in the exterior region of a Reissner-Nordström spacetime, and subject to a technical regularity condition.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [32] [Inviscid limit and an effective energy-enstrophy diffusion process](https://arxiv.org/abs/2602.15805)
*Alain-Sol Sznitman,Klaus Widmayer*

Main category: math.PR

TL;DR: The paper shows that the stationary diffusion in a 2D cone from a companion article serves as the inviscid limit of the enstrophy-energy process for N-dimensional Galerkin-Navier-Stokes systems with Brownian forcing, regardless of stirring strength.


<details>
  <summary>Details</summary>
Motivation: To understand the inviscid limit behavior of stochastic Navier-Stokes systems and how the stationary diffusion in a 2D cone relates to the enstrophy-energy process of higher-dimensional systems with random forcing.

Method: Analyzes stationary N-dimensional Galerkin-Navier-Stokes evolution with Brownian forcing and random stirring, connects it to the 2D cone diffusion from companion article, and uses quantitative condensation bounds to study inviscid limits.

Result: The stationary diffusion in the 2D cone is proven to be the inviscid limit of the laws of the enstrophy-energy process, independent of stirring strength. Quantitative bounds show attrition of all but lowest modes in inviscid limit for suitable forcings.

Conclusion: The inviscid limit of stochastic Navier-Stokes systems exhibits condensation behavior where only the lowest modes survive, with the 2D cone diffusion providing the limiting description regardless of random stirring intensity.

Abstract: In this article we consider a stationary $N$-dimensional Galerkin-Navier-Stokes type evolution with Brownian forcing and random stirring (of arbitrarily small strength). We show that the stationary diffusion in an open two-dimensional cone constructed in a companion article, stands as the inviscid limit of the laws of the ``enstrophy-energy'' process of the $N$-dimensional diffusion process considered here, this regardless of the strength of the stirring. With the help of the quantitative condensation bounds of the companion article, we infer quantitative inviscid condensation bounds, which for suitable forcings show an attrition of all but the lowest modes in the inviscid limit.

</details>


### [33] [Effective energy-enstrophy diffusion process and condensation bound](https://arxiv.org/abs/2602.15810)
*Alain-Sol Sznitman,Klaus Widmayer*

Main category: math.PR

TL;DR: The paper uses Gaussian measure on ℝᴺ to define coefficients for an elliptic diffusion on a 2D cone, proves existence/uniqueness of stationary distribution, and bounds energy/enstrophy ratio.


<details>
  <summary>Details</summary>
Motivation: To study inviscid condensation phenomena in fluid dynamics, specifically how certain Brownian forcings can lead to condensation where only the lowest modes survive in the inviscid limit.

Method: Uses Gaussian measure on ℝᴺ to define coefficients for an elliptic diffusion on an open cone of ℝ², then proves existence and uniqueness of stationary distribution for this diffusion.

Result: Proves existence/uniqueness of stationary distribution, bounds distance to 1 of the ratio of expected energy to expected enstrophy (ratio ≤1 with normalization), showing inviscid condensation with attrition of all but lowest modes.

Conclusion: Together with companion article, demonstrates that for suitable Brownian forcings, inviscid condensation occurs with attrition of all but the lowest modes in stationary Galerkin-Navier-Stokes type evolution.

Abstract: In this article we use Gaussian measure on $\mathbb{R}^N$ to define the coefficients of an elliptic diffusion on an open cone of $\mathbb{R}^2$. We prove the existence and uniqueness of a stationary distribution for this diffusion. In a companion article, we show that the diffusion constructed in this work is the inviscid limit of the laws of the ``enstrophy-energy'' process of a stationary $N$-dimensional Galerkin-Navier-Stokes type evolution with Brownian forcing and random stirring (the strength of which can be made to go to zero in the inviscid limit). In the present work, owing to the special properties of the coefficients constructed with the Gaussian measure, we bound the distance to $1$ of the ratio of the expected energy to the expected enstrophy (this ratio is at most $1$ with our normalization). Together with our companion article, this shows that for suitable Brownian forcings an inviscid condensation inducing an attrition of all but the lowest modes takes place.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [34] [Virtual ultrasound machine operating in a GHz to MHz frequency range for particle-based biomedical simulations](https://arxiv.org/abs/2602.15442)
*Urban Čoko,Tilen Potisk,Matej Praprotnik*

Main category: cond-mat.soft

TL;DR: A particle-based virtual ultrasound machine using smoothed dissipative particle dynamics with implicit pressure solver and negative-pressure stabilization to simulate ultrasound-matter interactions across MHz-GHz frequencies.


<details>
  <summary>Details</summary>
Motivation: Current simulation methods for ultrasound-matter interactions face limitations: continuum methods can't resolve microscale interactions, while particle-based approaches struggle with efficiency and stability at larger scales, creating a gap in simulating phenomena with large separation of viscous and sonic time scales.

Method: A novel smoothed dissipative particle dynamics variant with an implicit pressure solver and negative-pressure stabilization scheme to handle acoustic propagation across MHz-GHz frequencies, creating a particle-based virtual ultrasound machine.

Result: Successfully demonstrated by modeling acoustophoresis of encapsulated microbubbles, a key mechanism in ultrasound-mediated drug delivery, showing capability to simulate complex ultrasound-matter interactions.

Conclusion: The approach establishes a generalizable platform for simulating wave-matter interactions in soft and biological materials, opening new computational directions for studying acoustics-driven phenomena in science and engineering.

Abstract: Ultrasound-matter interactions underpin numerous biomedical and soft-matter applications, yet simulating these phenomena is challenging due to the large separation of viscous and sonic time scales. Continuum methods capture large-scale wave propagation but cannot resolve microscale interactions, while particle-based approaches offer molecular resolution but struggle with efficiency and stability at larger scales. We introduce a particle-based virtual ultrasound machine that uses a novel smoothed dissipative particle dynamics variant with an implicit pressure solver and a negative-pressure stabilization scheme, required to mimic acoustic propagation across MHz-GHz frequencies. We demonstrate its capabilities by modeling the acoustophoresis of encapsulated microbubbles, a key mechanism in ultrasound-mediated drug delivery. Beyond this application, the approach establishes a generalizable platform for simulating wave-matter interactions in soft and biological materials, opening new directions for computational studies of acoustics-driven phenomena in science and engineering.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [35] [Carleman Inequalities for the Heat Equation with Fourier Boundary Conditions: Applications to Null Controllability Problems](https://arxiv.org/abs/2602.15300)
*Jose Antonio Villa*

Main category: math.OC

TL;DR: Carleman inequality for heat equation with Fourier boundary conditions enables null controllability with boundary control on small region, solved via coupled parabolic system with iterative numerical method.


<details>
  <summary>Details</summary>
Motivation: To address null controllability problem for heat equation with Fourier boundary conditions where control acts only on a small portion of the boundary, requiring specialized mathematical tools.

Method: Establish Carleman inequality for heat equation with Fourier boundary conditions, then apply it to null controllability problem. Derive explicit solution via system of coupled parabolic equations and propose iterative numerical method.

Result: Successfully established Carleman inequality for the specified boundary conditions, achieved null controllability with boundary control on small region, and obtained explicit solution through coupled parabolic system.

Conclusion: The developed Carleman inequality enables null controllability for heat equation with Fourier boundary conditions using control on small boundary region, with practical numerical implementation via iterative method for coupled system.

Abstract: In this work, we establish a Carleman inequality for the heat equation with Fourier boundary conditions of the form $\partial_νy+by=f1_γ$, where the control acts on a small portion $γ$ of the boundary. We apply this inequality to address the null controllability problem with boundary control supported on this small region. An explicit solution to this problem is obtained via a system of coupled parabolic equations. Based on these results, we propose an iterative numerical method to solve the coupled system.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [36] [Tomography by Design: An Algebraic Approach to Low-Rank Quantum States](https://arxiv.org/abs/2602.15202)
*Shakir Showkat Sofi,Charlotte Vermeylen,Lieven De Lathauwer*

Main category: quant-ph

TL;DR: Algebraic quantum state tomography using structured measurements and matrix completion for low-rank states.


<details>
  <summary>Details</summary>
Motivation: Traditional quantum state tomography is computationally expensive. Need efficient methods for low-rank quantum states with deterministic guarantees.

Method: Algebraic algorithm using measurements of specific observables to estimate structured density matrix entries, then completing remaining entries via standard linear algebra under low-rank assumptions.

Result: Computationally efficient method applicable to broad class of low-rank mixed quantum states with deterministic recovery guarantees.

Conclusion: Proposed algebraic matrix completion framework outperforms state-of-the-art methods in efficiency while providing theoretical guarantees.

Abstract: We present an algebraic algorithm for quantum state tomography that leverages measurements of certain observables to estimate structured entries of the underlying density matrix. Under low-rank assumptions, the remaining entries can be obtained solely using standard numerical linear algebra operations. The proposed algebraic matrix completion framework applies to a broad class of generic, low-rank mixed quantum states and, compared with state-of-the-art methods, is computationally efficient while providing deterministic recovery guarantees.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [37] [SoliDualSPHysics: An extension of DualSPHysics for solid mechanics with hyperelasticity, plasticity, and fracture](https://arxiv.org/abs/2602.15149)
*Mohammad Naqib Rahimi,George Moutsanidis*

Main category: cs.CE

TL;DR: SoliDualSPHysics is an open-source GPU-accelerated SPH software that simulates hyperelastic, plastic, and brittle fracture behavior in deformable solids, featuring phase-field fracture modeling and substantial computational acceleration.


<details>
  <summary>Details</summary>
Motivation: To extend DualSPHysics capabilities to enable unified simulation of complex solid mechanics behaviors (hyperelasticity, finite-strain plasticity, brittle fracture) within a single SPH framework, addressing the need for efficient large-scale simulations with proper fracture modeling.

Method: Implements total Lagrangian SPH formulation for solid mechanics with direct load application, couples phase-field approach with SPH for brittle fracture modeling (crack initiation, propagation, branching), supports user-defined expressions for time/space-dependent quantities, and leverages DualSPHysics' CPU/GPU parallel architecture.

Result: The software achieves substantial computational acceleration for large-scale simulations, demonstrates accuracy and robustness through verification against benchmark problems and experimental data, and shows favorable scaling performance.

Conclusion: SoliDualSPHysics provides a flexible, open-source framework for unified solid mechanics and fracture simulations with GPU acceleration, verified accuracy, and comprehensive documentation for community use and further development.

Abstract: We introduce SoliDualSPHysics, a novel open-source and GPU-accelerated software that extends DualSPHysics to enable the numerical simulation of hyperelastic, finite-strain plastic, and brittle fracture behavior in deformable solids within a unified smoothed particle hydrodynamics (SPH) formulation. The software implements a total Lagrangian formulation for solid mechanics that allows direct application of external loads and boundary conditions, enabling independent solid mechanics simulations. Brittle fracture is modeled through a phase-field approach coupled with SPH, allowing crack initiation, propagation, and branching under dynamic loading without the need for additional criteria or local refinement. The framework also supports user-defined mathematical expressions to prescribe time- and space-dependent quantities, complementing the solid and fracture extensions and enhancing flexibility across existing and future DualSPHysics applications. Leveraging DualSPHysics' native CPU/GPU parallel architecture, the software achieves substantial computational acceleration for large-scale simulations, and the implementation is verified and validated against benchmark numerical problems and experimental data, demonstrating accuracy, robustness, and favorable scaling performance. Comprehensive implementation details and user documentation are provided to ensure reproducibility and to support further development by the community. The framework and source code are freely available through a public GitHub repository.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [38] [Quantitative local recovery of Kerr-de Sitter parameters from high-frequency equatorial quasinormal modes](https://arxiv.org/abs/2602.15764)
*Ruiliang Li*

Main category: math-ph

TL;DR: Inverse resonance problem for Kerr-de Sitter spacetime: finite package of high-frequency quasinormal modes determines mass, rotation, and cosmological constant parameters with stability estimates.


<details>
  <summary>Details</summary>
Motivation: To determine if and how black hole parameters (mass M, rotation a, cosmological constant Λ) can be reconstructed from observable quasinormal mode frequencies, which are characteristic oscillations of black holes that can be detected in gravitational wave signals.

Method: Uses semiclassical quantization of high-frequency quasinormal modes in slow-rotation Kerr-de Sitter spacetime, computes explicit second-order corrections to equatorial photon-orbit invariants, and analyzes finite packages of equatorial mode frequencies to solve inverse problems.

Result: 1) Finite equatorial high-frequency quasinormal mode package determines (M,a) with quantitative stability estimate. 2) Adding one damping observable (scaled imaginary part of a single equatorial mode) yields three-parameter inverse theorem determining (M,a,Λ) locally in slow-rotation regime away from a=0.

Conclusion: Black hole parameters can be uniquely determined from finite observations of quasinormal modes in the slow-rotation regime, providing theoretical foundation for parameter estimation from gravitational wave detections.

Abstract: We study an inverse resonance problem for the scalar wave equation on the Kerr-de Sitter family. In a compact subextremal slow-rotation regime and at a fixed overtone index, high-frequency quasinormal modes admit semiclassical quantization and a real-analytic labeling by angular momentum indices. Using this structure, we first prove that a finite equatorial high-frequency package of quasinormal-mode frequencies determines the mass and rotation parameter $(M,a)$ (for fixed cosmological constant $Λ>0$), with a quantitative stability estimate. As a key geometric input we compute explicit second-order (in $a$) corrections to the equatorial photon-orbit invariants which control the leading real and imaginary parts of the quasinormal modes. Finally, allowing $Λ$ to vary in a compact interval, we show that adding one damping observable (the scaled imaginary part of a single equatorial mode) yields a three-parameter inverse theorem: a finite package of three independent real observables determines $(M,a,Λ)$ locally in the slow-rotation regime away from $a=0$.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [39] [Multiplierless DFT Approximation Based on the Prime Factor Algorithm](https://arxiv.org/abs/2602.15218)
*L. Portella,F. M. Bayer,R. J. Cintra*

Main category: eess.SP

TL;DR: Proposes a multiplierless DFT approximation method using prime factor algorithm with small prime-sized DFT approximations, eliminating intermediate multiplications and error propagation.


<details>
  <summary>Details</summary>
Motivation: Existing DFT approximations either operate at small power-of-two blocklengths or large blocklengths using Cooley-Tukey-based approximations with twiddle factors that can't be approximated without error propagation. Need a method for fully multiplierless DFT approximations.

Method: Uses prime factor algorithm framework with small prime-sized DFT approximations to eliminate intermediate multiplication steps and prevent internal error propagation. Demonstrates with a 1023-point DFT approximation based on 3-, 11- and 31-point DFT approximations.

Result: Proposed approximations show significantly lower arithmetic complexity and smaller approximation error measurements compared to competing methods.

Conclusion: Prime factor algorithm provides effective framework for fully multiplierless DFT approximations that outperform existing methods in both complexity and accuracy.

Abstract: Matrix approximation methods have successfully produced efficient, low-complexity approximate transforms for the discrete cosine transforms and the discrete Fourier transforms. For the DFT case, literature archives approximations operating at small power-of-two blocklenghts, such as \{8, 16, 32\}, or at large blocklengths, such as 1024, which are obtained by means of the Cooley-Tukey-based approximation relying on the small-blocklength approximate transforms. Cooley-Tukey-based approximations inherit the intermediate multiplications by twiddled factors which are usually not approximated; otherwise the effected error propagation would prevent the overall good performance of the approximation. In this context, the prime factor algorithm can furnish the necessary framework for deriving fully multiplierless DFT approximations. We introduced an approximation method based on small prime-sized DFT approximations which entirely eliminates intermediate multiplication steps and prevents internal error propagation. To demonstrate the proposed method, we design a fully multiplierless 1023-point DFT approximation based on 3-, 11- and 31-point DFT approximations. The performance evaluation according to popular metrics showed that the proposed approximations not only presented a significantly lower arithmetic complexity but also resulted in smaller approximation error measurements when compared to competing methods.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [40] [High-throughput screening and mechanistic insights into solid acid proton conductors](https://arxiv.org/abs/2602.15268)
*Jonas Hänseroth,Max Großmann,Malte Grunert,Erich Runge,Christian Dreßler*

Main category: physics.chem-ph

TL;DR: Two-stage high-throughput screening using machine-learned potentials identifies 27 high-performing proton conductors from 6M+ materials, revealing universal O-O distance of ~2.5Å during proton transfer.


<details>
  <summary>Details</summary>
Motivation: Proton-conducting solid acids could enable water-free high-temperature fuel cells, but systematic materials screening has been computationally prohibitive due to the need for accurate proton diffusion calculations.

Method: Two-stage high-throughput screening strategy using machine-learned interatomic potentials fine-tuned to ab initio data. Screening based on structural motifs rather than empirical descriptors, starting from over 6 million materials.

Result: Identified 27 high-performing proton conductors, including over ten previously unexplored compounds, sustainable/commercially available materials, unsynthesized candidates, and organic systems outside conventional design rules. Revealed universal oxygen-oxygen distance of ~2.5Å at proton transfer moment across diverse chemistries.

Conclusion: The approach enables systematic discovery of proton conductors and provides mechanistic insight: macroscopic proton conductivity emerges from interplay between anion rotational dynamics, hydrogen-bond network connectivity, and proton-transfer probability.

Abstract: Proton-conducting solid acids could enable water-free operation of high-temperature fuel cells. However, systematic materials screening has, hitherto, been computationally prohibitive. Here, we introduce a two-stage high-throughput screening strategy that directly computes proton diffusion coefficients, enabled by machine-learned interatomic potentials fine-tuned to ab initio data. Starting from more than six million materials, our screening -- based on structural motifs rather than empirical descriptors -- identifies $27$ high-performing proton conductors, including over ten previously unexplored compounds. These include sustainable and commercially available materials, candidates that have not yet been synthesized, organic systems that fall outside conventional design rules, and known proton conductors that validate our approach. Importantly, our findings reveal a universal oxygen--oxygen distance of approximately $2.5$~Å at the moment of proton transfer across diverse chemistries, providing mechanistic insight and showing that macroscopic proton conductivity emerges from the interplay between anion rotational dynamics, hydrogen-bond network connectivity, and proton-transfer probability.

</details>


### [41] [Machine learning electronic structure and atomistic properties from the external potential](https://arxiv.org/abs/2602.15345)
*Jigyasa Nigam,Tess Smidt,Geneviève Dusson*

Main category: physics.chem-ph

TL;DR: The paper proposes an operator-centered ML framework that uses the external nuclear potential expressed in an atomic orbital basis as input, enabling efficient modeling of molecular properties and electronic structure mappings.


<details>
  <summary>Details</summary>
Motivation: Current ML approaches for electronic structure calculations either learn direct geometry-to-property mappings or use ML as surrogates for electronic structure theory. These methods face limitations in capturing long-range effects and efficiently representing atomic configurations.

Method: Proposes an operator-centered framework where the external (nuclear) potential in AO basis serves as input. Constructs hierarchical, body-ordered representations of atomic configurations that mirror atom-centered descriptors. Uses matrix-valued nature of external potential to connect with equivariant message-passing neural networks, with successive products of external potential providing scalable route to equivariant message passing.

Result: The approach enables modeling of molecular properties (energies, dipole moments) directly from external potential, and learning effective operator-to-operator maps including mappings to Fock matrix and reduced density matrix, from which multiple molecular observables can be simultaneously derived.

Conclusion: The operator-centered framework provides a principled connection between electronic structure theory and ML, offering scalable equivariant message passing, efficient long-range effect description, and unified approach for learning both molecular properties and electronic structure operators.

Abstract: Electronic structure calculations remain a major bottleneck in atomistic simulations and, not surprisingly, have attracted significant attention in machine learning (ML). Most existing approaches learn a direct map from molecular geometries, typically represented as graphs or encoded local environments, to molecular properties or use ML as a surrogate for electronic structure theory by targeting quantities such as Fock or density matrices expressed in an atomic orbital (AO) basis.
  Inspired by the Hohenberg-Kohn theorem, in this work, we propose an operator-centered framework in which the external (nuclear) potential, expressed in an AO basis, serves as the model input. From this operator, we construct hierarchical, body-ordered representations of atomic configurations that closely mirror the principles underlying several popular atom-centered descriptors. At the same time, the matrix-valued nature of the external potential provides a natural connection to equivariant message-passing neural networks. In particular, we show that successive products of the external potential provide a scalable route to equivariant message passing and enable an efficient description of long-range effects. We demonstrate that this approach can be used to model molecular properties, such as energies and dipole moments, from the external potential, or learn effective operator-to-operator maps, including mappings to the Fock matrix and the reduced density matrix from which multiple molecular observables can be simultaneously derived.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [42] [Space-filling lattice designs for computer experiments](https://arxiv.org/abs/2602.15390)
*Naoki Sakai,Takashi Goda*

Main category: stat.ME

TL;DR: The paper proposes two algorithms for constructing quasi-uniform space-filling designs using QMC lattice point sets, with theoretical guarantees and empirical validation in Gaussian process regression.


<details>
  <summary>Details</summary>
Motivation: To develop efficient space-filling designs for computer experiments by focusing on quasi-uniformity, which integrates covering and separation properties through a unified criterion.

Method: Two construction algorithms: 1) rank-1 lattice point sets approximating quasi-uniform Kronecker sequences with explicit generating vectors, 2) Korobov lattice point sets using LLL basis reduction to find generating vectors ensuring quasi-uniformity.

Result: Theoretical proof that explicit point sets achieve O(N^{-1/d}) isotropic discrepancy, numerical validation of quasi-uniformity claims, and empirical comparisons showing efficacy in Gaussian process regression.

Conclusion: The proposed QMC lattice point sets provide effective space-filling designs for computer experiments with theoretical guarantees and practical performance in Gaussian process regression applications.

Abstract: This paper investigates the construction of space-filling designs for computer experiments. The space-filling property is characterized by the covering and separation radii of a design, which are integrated through the unified criterion of quasi-uniformity. We focus on a special class of designs, known as quasi-Monte Carlo (QMC) lattice point sets, and propose two construction algorithms. The first algorithm generates rank-1 lattice point sets as an approximation of quasi-uniform Kronecker sequences, where the generating vector is determined explicitly. As a byproduct of our analysis, we prove that this explicit point set achieves an isotropic discrepancy of $O(N^{-1/d})$. The second algorithm utilizes Korobov lattice point sets, employing the Lenstra--Lenstra--Lovász (LLL) basis reduction algorithm to identify the generating vector that ensures quasi-uniformity. Numerical experiments are provided to validate our theoretical claims regarding quasi-uniformity. Furthermore, we conduct empirical comparisons between various QMC point sets in the context of Gaussian process regression, showcasing the efficacy of the proposed designs for computer experiments.

</details>
